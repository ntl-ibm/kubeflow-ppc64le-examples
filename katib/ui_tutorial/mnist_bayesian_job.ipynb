{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eebbdcc-5ac4-43be-90a0-2bc1a73538e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141507e7-ea34-4cc9-acd5-2fd7a324166c",
   "metadata": {},
   "source": [
    "# Katib Experiment using the UI\n",
    "\n",
    "This tutorial shows how to run an experiment using Kubeflow's Katib UI. The example finds the best hyperparameters for a MNIST model implemented in PyTorch.\n",
    "\n",
    "A Katib Experiment runs trials in parallel; each trial is a run of the training algorithm with a specific set of hyperparameters.\n",
    "\n",
    "In practice, each trial (training run) could be distributed across multiple GPUs and there could also be multiple trials running in parallel. In this example trials do not distribute the training across GPUS. However, multiple trials run in parallel. This is a simple way to use multiple GPUs within the experiment, with needing distributed training for each trial.\n",
    "\n",
    "We will place our training data and training script on a shared PVC so that it can be shared across trials running in parallel.\n",
    "\n",
    "The example shows how to interact with the UI and does NOT make use of Kubeflow pipelines. (The Katib SDK does enable pipeline integration, which may be covered in other examples).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01e7284-78c5-417e-bf27-f5f03fde6fbe",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "The notebook needs a data volume to load the training data and scripts into. The volume needs to be mounted read-write-many.\n",
    "\n",
    "You will have to recreate the notebook server and mount a volume if this volume is not already mounted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "742b4fc7-1cfe-4596-8728-251958bf0c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOLUME_NAME = \"my-notebook-datavol-1\"\n",
    "VOLUME_MOUNT_POINT = \"/home/jovyan/my-notebook-datavol-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e0d27a-5ade-4b3a-9f93-637ea3789b66",
   "metadata": {},
   "source": [
    "# Download training data & Clone repo with training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e4e8920-b24a-40ad-9f90-412c5843dc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "\n",
    "_ = MNIST(VOLUME_MOUNT_POINT, download=True, train=True)\n",
    "_ = MNIST(VOLUME_MOUNT_POINT, download=True, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee6d681d-9413-41b1-83f9-528b18e52cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ${VOLUME_MOUNT_POINT}/code || true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7310e98-6142-43e2-91da-aed4e40f51d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path '/home/jovyan/my-notebook-datavol-1/code' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ntl-ibm/kubeflow-ppc64le-examples.git $VOLUME_MOUNT_POINT/code -b pytorch_example_improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16c3352-4ea5-4fbe-8b53-feec877b8a90",
   "metadata": {},
   "source": [
    "# Define a trial template\n",
    "\n",
    "The first step of any experiment is to define a trial template that will accept the hyperparameters that need to be searched. We'll create a template for Kubernetes Job that runs the training using PyTorch. The job will define a pod that mounts the pvc that contains the training data and python scripts. The Pod will invoke the script in its command.\n",
    "\n",
    "Katib supports frameworks other than Job such as TFJob, PyTorchJob, MPIJob, etc. Using these frame works add additional features for machine learning platforms, but can be more difficult to setup.\n",
    "\n",
    "Several input parameters to the script are defined as references to Katib search parameters. Katib will replace these with values provided by the suggestion during the experiment when the trial is created.\n",
    "\n",
    "Take care that any data written to common storage (/workspace) does not conflict with other trails running in parallel. Our program uses '/tmp' as the root directory, and disables checkpointing to avoid conficts.\n",
    "\n",
    "The python SDK is used here to create the Job definition, although it is also common to define a YAML declaration string for the Job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b57a8760-e8a3-48ff-9fab-bf2f0722c72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubernetes import client, config\n",
    "from kubernetes.client import (\n",
    "    V1ObjectMeta,\n",
    "    V1ConfigMap,\n",
    "    V1Job,\n",
    "    V1JobSpec,\n",
    "    V1PodTemplateSpec,\n",
    "    V1PodSpec,\n",
    "    V1Container,\n",
    "    V1Volume,\n",
    "    V1VolumeMount,\n",
    "    V1PersistentVolumeClaimVolumeSource,\n",
    "    V1EmptyDirVolumeSource,\n",
    "    V1ResourceRequirements,\n",
    ")\n",
    "\n",
    "job = V1Job(\n",
    "    api_version=\"batch/v1\",\n",
    "    kind=\"Job\",\n",
    "    spec=V1JobSpec(\n",
    "        template=V1PodTemplateSpec(\n",
    "            metadata=V1ObjectMeta(\n",
    "                # https://github.com/kubeflow/website/issues/2011\n",
    "                annotations={\"sidecar.istio.io/inject\": \"false\"},\n",
    "            ),\n",
    "            spec=V1PodSpec(\n",
    "                restart_policy=\"Never\",\n",
    "                containers=[\n",
    "                    V1Container(\n",
    "                        name=\"pytorch\",\n",
    "                        image=\"quay.io/ntlawrence/pytorch@sha256:75b0d6804e480a864ce2cc0e14be9b0bd993cc341dee346d29031defdba2f06d\",\n",
    "                        command=[\n",
    "                            \"python\",\n",
    "                            \"./train.py\",\n",
    "                            \"--root_dir=/tmp/workspace\",\n",
    "                            \"--data_dir=/workspace\",\n",
    "                            \"--model=/tmp/mnist_model.pt\",\n",
    "                            \"--batch_size=672\",\n",
    "                            \"--max_epochs=${trialParameters.epochs}\",\n",
    "                            \"--lr=${trialParameters.lr}\",\n",
    "                            \"--no-checkpoint\",\n",
    "                            \"--katib_log_file=/var/log/katib/metrics.log\",\n",
    "                        ],\n",
    "                        working_dir=\"/workspace/code/distributed_training/pytorch/mnist/src\",\n",
    "                        resources=V1ResourceRequirements(limits={\"nvidia.com/gpu\": 1}),\n",
    "                        volume_mounts=[\n",
    "                            V1VolumeMount(mount_path=\"/workspace\", name=\"workspace\"),\n",
    "                            V1VolumeMount(mount_path=\"/dev/shm\", name=\"dshm\"),\n",
    "                        ],\n",
    "                    )\n",
    "                ],\n",
    "                volumes=[\n",
    "                    V1Volume(\n",
    "                        name=\"workspace\",\n",
    "                        persistent_volume_claim=V1PersistentVolumeClaimVolumeSource(\n",
    "                            claim_name=VOLUME_NAME\n",
    "                        ),\n",
    "                    ),\n",
    "                    V1Volume(\n",
    "                        name=\"dshm\", empty_dir=V1EmptyDirVolumeSource(medium=\"Memory\")\n",
    "                    ),\n",
    "                ],\n",
    "            ),\n",
    "        )\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa332f4-999f-4de9-b243-77075abe68f4",
   "metadata": {},
   "source": [
    "## Convert to YAML\n",
    "\n",
    "The python code in the previous cell is a progammatic way of creating the declaration for the job. This next cell shows what the description looks like in YAML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16aa21ad-90a6-4354-807c-ce46a8a45a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apiVersion: batch/v1\n",
      "kind: Job\n",
      "spec:\n",
      "  template:\n",
      "    metadata:\n",
      "      annotations:\n",
      "        sidecar.istio.io/inject: 'false'\n",
      "    spec:\n",
      "      containers:\n",
      "      - command:\n",
      "        - python\n",
      "        - ./train.py\n",
      "        - --root_dir=/tmp/workspace\n",
      "        - --data_dir=/workspace\n",
      "        - --model=/tmp/mnist_model.pt\n",
      "        - --batch_size=672\n",
      "        - --max_epochs=${trialParameters.epochs}\n",
      "        - --lr=${trialParameters.lr}\n",
      "        - --no-checkpoint\n",
      "        - --katib_log_file=/var/log/katib/metrics.log\n",
      "        image: quay.io/ntlawrence/pytorch@sha256:75b0d6804e480a864ce2cc0e14be9b0bd993cc341dee346d29031defdba2f06d\n",
      "        name: pytorch\n",
      "        resources:\n",
      "          limits:\n",
      "            nvidia.com/gpu: 1\n",
      "        volumeMounts:\n",
      "        - mountPath: /workspace\n",
      "          name: workspace\n",
      "        - mountPath: /dev/shm\n",
      "          name: dshm\n",
      "        workingDir: /workspace/code/distributed_training/pytorch/mnist/src\n",
      "      restartPolicy: Never\n",
      "      volumes:\n",
      "      - name: workspace\n",
      "        persistentVolumeClaim:\n",
      "          claimName: my-notebook-datavol-1\n",
      "      - emptyDir:\n",
      "          medium: Memory\n",
      "        name: dshm\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ruamel.yaml import YAML\n",
    "from ruamel.yaml.compat import StringIO\n",
    "\n",
    "yaml = YAML()\n",
    "s = StringIO()\n",
    "client_api = client.ApiClient()\n",
    "yaml.dump(client_api.sanitize_for_serialization(job), s)\n",
    "print(s.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25a02f9-4336-4899-b58f-435544b20202",
   "metadata": {},
   "source": [
    "# Create a configmap with the template\n",
    "If many experiments will be created using the same trial template, the template's YAML can be saved in a configmap and referenced in each experiment. This avoids needing to copy the YAML for each new experiment.\n",
    "\n",
    "This next cell creates a config map with the trial template as the value for pytorch-mnist-job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a298213a-cf95-4b6a-8a3e-d5c770d5be9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials_config_map = client.V1ConfigMap(\n",
    "    metadata=V1ObjectMeta(\n",
    "        name=\"katib-example-trial-defs\",\n",
    "        labels={\"katib.kubeflow.org/component\": \"trial-templates\"},\n",
    "    ),\n",
    "    data={\"pytorch-mnist-job\": s.getvalue()},\n",
    ")\n",
    "\n",
    "with open(\"/var/run/secrets/kubernetes.io/serviceaccount/namespace\", \"r\") as f:\n",
    "    NAMESPACE = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecf76708-c336-4926-9d18-e53c130b2f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubernetes.client import ApiException\n",
    "\n",
    "config.load_incluster_config()\n",
    "k8s_api = client.CoreV1Api()\n",
    "\n",
    "try:\n",
    "    rsp = k8s_api.create_namespaced_config_map(\n",
    "        namespace=NAMESPACE, body=trials_config_map\n",
    "    )\n",
    "except ApiException as e:\n",
    "    rsp = k8s_api.patch_namespaced_config_map(\n",
    "        namespace=NAMESPACE, name=\"katib-example-trial-defs\", body=trials_config_map\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f454002-d21b-45ed-9011-1c029bc85b88",
   "metadata": {},
   "source": [
    "# Create Experiment\n",
    "\n",
    "Now that the trial has been defined, we are ready to use Katib to search for the best hyperparameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccea53d-7889-49a0-b4b2-209a8cd6513b",
   "metadata": {},
   "source": [
    "Navigate to the Experiments (AutoML) panel and Click the new Experiment Button\n",
    "\n",
    "<img src=\"./images/Katib_AutoML.jpeg\" alt=\"Create Experiment Button\" width=\"700\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a28a8d4-757f-43fc-944b-fdfb7382dd45",
   "metadata": {},
   "source": [
    "Give the experiment a name such as 'my-experiment'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9731753-33fd-4f68-9164-b58f324c82c8",
   "metadata": {},
   "source": [
    "The trial thresholds controls the maximum number of trials that can run in parallel and the maximum number of trials that will be explored. For this experiment, leave these at the default values. \n",
    "\n",
    "**Caution**: During time periods of heavy workloads, or on small clusters, it may be necessary to reduce the number of parallel trials to 2 or even 1.\n",
    "\n",
    "<img src=\"./images/Trial_Thresholds.jpeg\" alt=\"Set Thresholds\" width=\"700\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b6594d-2dc9-4335-9529-6a2533025d9d",
   "metadata": {},
   "source": [
    "The objective controls what metric should be used to measure the quality of the hyperparameters.\n",
    "\n",
    "For this tutorial, we want to maximize the f1 score. \n",
    "\n",
    "We'll set this to a very high .999 so that the experiment runs to completion. In practice, we'd want to set this to a value that is a \"good enough\" value of the objective function for the model.\n",
    "\n",
    "Add the additional metric of acc. This means that accuracy will be tracked and reported in the experiment. (The experiment will not try to maximize accuracy, the experiment will maximize the f1 score).\n",
    "\n",
    "<img src=\"./images/Trial_Objective.jpeg\" alt=\"Objective Function\" width=\"700\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78358cd-80aa-4f19-89fe-e6232ccea22a",
   "metadata": {},
   "source": [
    "Use Basesian Optimization as a search algorithm for this example. \n",
    "\n",
    "Evaluating the objective function of is expensive (since it requires training with the suggested hyperparameters); Basesian Optimization is designed for problems where the objective functinon is very expensive. The algorithm also balances exploration vs exploitation of the search space and is data efficient. It is considered a state of the art algorithm. A different search algorithm might be required if it is necessary to scale to a very high level of parallelism. \n",
    "\n",
    "In the algorithm settings, the \"Random State\" should be set to a value. This makes the experiment more repeatable. This example used 42.\n",
    "\n",
    "<img src=\"./images/Search_Alg.jpeg\" alt=\"Search Algorithm\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b66d9ca-25a3-4076-889c-7ca4e882d578",
   "metadata": {},
   "source": [
    "For this tutorial, leave the Early Stopping as \"None\".\n",
    "\n",
    "<img src=\"./images/Early_Stop.jpeg\" alt=\"Early Stopping\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579a9f73-5488-4e73-880d-3204135e489c",
   "metadata": {},
   "source": [
    "Set the search space of the hyper parameters. This tells Katib the space of values that it should consider for each suggestion.\n",
    "This example optimizes two hyperparameters, lr and epochs.\n",
    "* Use the existing values for lr\n",
    "* Delete the other parameters\n",
    "* Add an epochs parameter, type int with a range of 5 to 25 and step 5\n",
    "\n",
    "<img src=\"./images/Hyper_Parameters.jpeg\" alt=\"Hyper Parameters\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ab751e-a6f4-4595-bad6-adf0a673d945",
   "metadata": {},
   "source": [
    "The python script is designed to log metrics to a file. We'll use a File metrics collector, with the default file name. Metrics can be collected from stdandard out as well, however the file metrics collector worked better in this example. The reason was there were additional log messages in stdout that confused the collector.\n",
    "\n",
    "<img src=\"./images/Metrics_Collector.jpeg\" alt=\"Metrics Collector\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e20563a-9208-4ac9-912b-7c13d6217a36",
   "metadata": {},
   "source": [
    "Fill in the trial template. Because we saved the Job description in a config map, we can reference that here.\n",
    "\n",
    "The primary container name should be set to 'pytorch', since this is the name of the container our training code runs under.\n",
    "The Yaml for the Job is filled in automatically after you change the configMap name and namespace to the config map created in this script.\n",
    "\n",
    "<img src=\"./images/Trial_Template_1.jpeg\" alt=\"Trial Template\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b9a66b",
   "metadata": {},
   "source": [
    "Fill in the reference variables under the template. This is how variables in the template are matched to the parameters from the search space.\n",
    "\n",
    "<img src=\"./images/Trial_Template_2.jpeg\" alt=\"Trial Template references\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fed1ae",
   "metadata": {},
   "source": [
    "Press Create to create the experiment and start it running.\n",
    "\n",
    "The running experiment will appear in the experiment's list.\n",
    "\n",
    "<img src=\"./images/Running_Experiment.jpeg\" alt=\"Experiment\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafd9eaf",
   "metadata": {},
   "source": [
    "The best parameters are shown in the overview of the experiment.\n",
    "\n",
    "<img src=\"./images/Best_Trial.jpeg\" alt=\"Optimal Hyperparameters\" width=\"700\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
