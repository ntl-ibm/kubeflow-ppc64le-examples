{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e373bc98-4651-464c-81c2-4017c73c9601",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ff1b7d-83b6-4fcd-95cc-537e52df68d4",
   "metadata": {},
   "source": [
    "## Defines a component to deploy an inference service using NVIDIA Triton\n",
    "\n",
    "The component is intended to be reusable across multiple pipelines. Our NVIDIA Triton builds support ONNX models.\n",
    "\n",
    "The component builds the InferenceService yaml, and waits for the inference service to deploy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81816b35-ffc3-4798-b7c3-643317205029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.components import create_component_from_func, InputPath\n",
    "from typing import Dict, Union\n",
    "\n",
    "BASE_IMAGE = (\n",
    "    \"quay.io/ibm/kubeflow-notebook-image-ppc64le:elyra3.13.0-py3.8-tf2.9.2-pt1.12.1-v0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "009a7699-4124-4713-b3e9-a581ff986c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_inference_service(\n",
    "    name: str,\n",
    "    storage_uri: str,\n",
    "    minio_url: str,\n",
    "    rm_existing: bool = False,\n",
    "    minio_credential_secret=\"mlpipeline-minio-artifact\",\n",
    "    concurrency_target: int = None,\n",
    "    predictor_min_replicas: int = None,\n",
    "    predictor_max_replicas: int = None,\n",
    "    predictor_gpu_allocation: int = 0,\n",
    "    predictor_protocol: str = \"v2\",  # or grpc-v2\n",
    "    triton_runtime_version: str = \"22.03-py3\",\n",
    "    predictor_node_selector: str = \"\",  # Requires admin to enable the capability\n",
    "    transformer_specification: Dict[str, Union[str, int]] = None,\n",
    "):\n",
    "    import os\n",
    "    import subprocess\n",
    "    import yaml\n",
    "    import base64\n",
    "\n",
    "    # https://kserve.github.io/website/modelserving/storage/s3/s3/#predict-on-a-inferenceservice-with-a-saved-model-on-s3\n",
    "    # https://kserve.github.io/website/reference/api/\n",
    "    # https://kserve.github.io/website/modelserving/autoscaling/autoscaling/\n",
    "    # https://kserve.github.io/website/modelserving/v1beta1/triton/torchscript/#inference-with-http-endpoint\n",
    "    # https://github.com/kserve/kserve/blob/master/docs/samples/multimodelserving/triton/README.md\n",
    "    # https://kserve.github.io/website/modelserving/v1beta1/triton/torchscript/#inference-with-grpc-endpoint\n",
    "\n",
    "    # Caution: If using nodeSelector, the nodeSelector capability must be enabled for knative by an admin\n",
    "    # https://knative.dev/docs/serving/configuration/feature-flags/#kubernetes-node-selector\n",
    "    # The default for our installs is FALSE\n",
    "    # once enabled, the value should be 'label: \"value\"', to force the predictor/transformer to\n",
    "    # run on specific hardware\n",
    "\n",
    "    # It happens that the credentials for the minio user name and password are already in a secret\n",
    "    # This just loads them so that we can create our own secret to store the S3 connection information\n",
    "    # for the Inference service\n",
    "    r = subprocess.run(\n",
    "        [\"kubectl\", \"get\", \"secret\", minio_credential_secret, \"-oyaml\"],\n",
    "        stdout=subprocess.PIPE,\n",
    "        check=True,\n",
    "        text=True,\n",
    "    )\n",
    "    secret = yaml.safe_load(r.stdout)\n",
    "\n",
    "    s3_credentials_spec = f\"\"\"\n",
    "    apiVersion: v1\n",
    "    kind: Secret\n",
    "    metadata:\n",
    "      name: minio-credentials\n",
    "      annotations:\n",
    "        serving.kserve.io/s3-endpoint: {minio_url} \n",
    "        serving.kserve.io/s3-usehttps: \"0\"\n",
    "        serving.kserve.io/s3-region: \"us-west1\"\n",
    "        serving.kserve.io/s3-useanoncredential: \"false\"\n",
    "    type: Opaque\n",
    "    data:\n",
    "      AWS_ACCESS_KEY_ID: {secret['data']['accesskey']}\n",
    "      AWS_SECRET_ACCESS_KEY: {secret['data']['secretkey']}\n",
    "    \"\"\"\n",
    "\n",
    "    print(s3_credentials_spec)\n",
    "    subprocess.run(\n",
    "        [\"kubectl\", \"apply\", \"-f\", \"-\"],\n",
    "        input=s3_credentials_spec,\n",
    "        check=True,\n",
    "        text=True,\n",
    "    )\n",
    "\n",
    "    sa_spec = \"\"\"\n",
    "    apiVersion: v1\n",
    "    kind: ServiceAccount\n",
    "    metadata:\n",
    "      name: kserve-inference-sa\n",
    "    secrets:\n",
    "    - name: minio-credentials\n",
    "    \"\"\"\n",
    "\n",
    "    print(sa_spec)\n",
    "    subprocess.run(\n",
    "        [\"kubectl\", \"apply\", \"-f\", \"-\"], input=sa_spec, check=True, text=True\n",
    "    )\n",
    "\n",
    "    ### Remove Existing Inferenceservice, if requested\n",
    "    ### Ignores errrors if service does not already exist\n",
    "    if rm_existing:\n",
    "        subprocess.run([\"kubectl\", \"delete\", \"inferenceservice\", name], check=False)\n",
    "\n",
    "    ####### Transformer Spec ######\n",
    "    if transformer_specification:\n",
    "        min_t_replicas = (\n",
    "            (\"minReplicas: \" + transformer_specification[\"min_replicas\"])\n",
    "            if \"min_replicas\" in transformer_specification\n",
    "            else \"\"\n",
    "        )\n",
    "        max_t_replicas = (\n",
    "            (\"maxReplicas: \" + transformer_specification[\"max_replicas\"])\n",
    "            if \"min_replicas\" in transformer_specification\n",
    "            else \"\"\n",
    "        )\n",
    "\n",
    "        # EnvFrom allows all vars to be read from a config map\n",
    "        # If a variable is defined by both env and envFrom,\n",
    "        # env takes precedance. If a variable is defined twice\n",
    "        # in env from, then the last one wins.\n",
    "        if \"env_configmap\" in transformer_specification:\n",
    "            envFrom = f\"\"\"\n",
    "          envFrom:\n",
    "            - configMapRef:\n",
    "                name: {transformer_specification[\"env_configmap\"]}\n",
    "          \"\"\"\n",
    "        else:\n",
    "            envFrom = \"\"\n",
    "\n",
    "        # Node selector\n",
    "        # https://kserve.github.io/website/0.9/modelserving/nodescheduling/inferenceservicenodescheduling/#usage\n",
    "        if \"node_selector\" in transformer_specification:\n",
    "            t_node_selector = (\n",
    "                f'nodeSelector:\\n          {transformer_specification[\"node_selector\"]}'\n",
    "            )\n",
    "        else:\n",
    "            t_node_selector = \"\"\n",
    "\n",
    "        #### Transformer specification ####\n",
    "        transform_spec = f\"\"\"\n",
    "      transformer:\n",
    "        {min_t_replicas}\n",
    "        {max_t_replicas}\n",
    "        serviceAccountName: kserve-inference-sa\n",
    "        {t_node_selector}\n",
    "        containers:\n",
    "        - image: \"{transformer_specification[\"image\"]}\"\n",
    "          name: \"{name}-transformer\"\n",
    "          command: {transformer_specification.get(\"command\", '[\"python\", \"transform.py\"]')}\n",
    "          args: [\"--protocol={predictor_protocol}\"]\n",
    "          env:\n",
    "            - name: STORAGE_URI\n",
    "              value: {storage_uri}\n",
    "          {envFrom}\n",
    "          \"\"\"\n",
    "    else:\n",
    "        transform_spec = \"\"\n",
    "\n",
    "    gpu_resources = (\n",
    "        f\"nvidia.com/gpu: {predictor_gpu_allocation}\"\n",
    "        if predictor_gpu_allocation\n",
    "        else \"\"\n",
    "    )\n",
    "\n",
    "    min_p_replicas = (\n",
    "        f\"minReplicas: {predictor_min_replicas}\"\n",
    "        if predictor_min_replicas is not None\n",
    "        else \"\"\n",
    "    )\n",
    "    max_p_replicas = (\n",
    "        f\"maxReplicas: {predictor_max_replicas}\"\n",
    "        if predictor_max_replicas is not None\n",
    "        else \"\"\n",
    "    )\n",
    "\n",
    "    predictor_port_spec = (\n",
    "        '[{\"containerPort\": 9000, \"name\": \"h2c\", \"protocol\": \"TCP\"}]'\n",
    "        if predictor_protocol == \"grpc-v2\"\n",
    "        else \"\"\n",
    "    )\n",
    "\n",
    "    if concurrency_target:\n",
    "        autoscaling_target = f\"\"\"\n",
    "        autoscaling.knative.dev/target: \"{concurrency_target}\"\n",
    "        autoscaling.knative.dev/metric: \"concurrency\"\n",
    "        \"\"\"\n",
    "    else:\n",
    "        autoscaling_target = \"\"\n",
    "\n",
    "    # Node selector\n",
    "    # https://kserve.github.io/website/0.9/modelserving/nodescheduling/inferenceservicenodescheduling/#usage\n",
    "    if predictor_node_selector:\n",
    "        p_node_selector = f\"nodeSelector:\\n          {predictor_node_selector}\"\n",
    "    else:\n",
    "        p_node_selector = \"\"\n",
    "\n",
    "    ##### Inference Service Spec ####\n",
    "    service_spec = f\"\"\"\n",
    "    apiVersion: serving.kserve.io/v1beta1\n",
    "    kind: InferenceService\n",
    "    metadata:\n",
    "      name: {name}\n",
    "      annotations:\n",
    "        sidecar.istio.io/inject: \"false\"\n",
    "        # https://knative.dev/docs/serving/autoscaling/concurrency/#soft-limit\n",
    "        {autoscaling_target}\n",
    "    spec:\n",
    "      {transform_spec}\n",
    "      predictor:\n",
    "        {min_p_replicas}\n",
    "        {max_p_replicas}\n",
    "        serviceAccountName: kserve-inference-sa\n",
    "        {p_node_selector}\n",
    "        triton:\n",
    "          runtimeVersion: {triton_runtime_version}\n",
    "          args: [ \"--strict-model-config=false\"]\n",
    "          storageUri: {storage_uri}\n",
    "          ports: {predictor_port_spec}\n",
    "          env:\n",
    "          - name: OMP_NUM_THREADS\n",
    "            value: \"1\"\n",
    "          resources:\n",
    "            limits:\n",
    "               {gpu_resources}\n",
    "    \"\"\"\n",
    "\n",
    "    print(service_spec)\n",
    "    subprocess.run(\n",
    "        [\"kubectl\", \"apply\", \"-f\", \"-\"], input=service_spec, check=True, text=True\n",
    "    )\n",
    "\n",
    "    print(\"Waiting for inference service to become available\")\n",
    "    subprocess.run(\n",
    "        [\n",
    "            \"kubectl\",\n",
    "            \"wait\",\n",
    "            \"--for=condition=Ready\",\n",
    "            f\"inferenceservice/{name}\",\n",
    "            \"--timeout=600s\",\n",
    "        ],\n",
    "        check=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89a5f0a2-a06f-4e8e-8a5a-5ea396b00cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_inference_service_comp = create_component_from_func(\n",
    "    func=deploy_inference_service,\n",
    "    output_component_file=\"deploy_triton_inference_service_component.yaml\",\n",
    "    base_image=BASE_IMAGE,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
