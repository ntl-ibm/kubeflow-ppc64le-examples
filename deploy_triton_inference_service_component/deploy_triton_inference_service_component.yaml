name: Deploy inference service
inputs:
- {name: name, type: String}
- {name: storage_uri, type: String}
- {name: minio_url, type: String}
- name: rm_existing
  type: Boolean
  default: "False"
  optional: true
- {name: minio_credential_secret, default: mlpipeline-minio-artifact, optional: true}
- {name: concurrency_target, type: Integer, optional: true}
- {name: predictor_min_replicas, type: Integer, optional: true}
- {name: predictor_max_replicas, type: Integer, optional: true}
- {name: predictor_gpu_allocation, type: Integer, default: '0', optional: true}
- {name: predictor_protocol, type: String, default: v2, optional: true}
- {name: triton_runtime_version, type: String, default: 22.03-py3, optional: true}
- {name: transformer_specification, type: 'typing.Dict[str, typing.Union[str, int]]',
  optional: true}
implementation:
  container:
    image: quay.io/ibm/kubeflow-notebook-image-ppc64le:elyra3.13.0-py3.8-tf2.9.2-pt1.12.1-v0
    command:
    - sh
    - -ec
    - |
      program_path=$(mktemp)
      printf "%s" "$0" > "$program_path"
      python3 -u "$program_path" "$@"
    - "def deploy_inference_service(\n    name,\n    storage_uri,\n    minio_url,\n\
      \    rm_existing = False,\n    minio_credential_secret=\"mlpipeline-minio-artifact\"\
      ,\n    concurrency_target = None,\n    predictor_min_replicas = None,\n    predictor_max_replicas\
      \ = None,\n    predictor_gpu_allocation = 0,\n    predictor_protocol = \"v2\"\
      ,  # or grpc-v2\n    triton_runtime_version = \"22.03-py3\",\n    transformer_specification\
      \ = None,\n):\n    import os\n    import subprocess\n    import yaml\n    import\
      \ base64\n\n    # https://kserve.github.io/website/modelserving/storage/s3/s3/#predict-on-a-inferenceservice-with-a-saved-model-on-s3\n\
      \    # https://kserve.github.io/website/reference/api/\n    # https://kserve.github.io/website/modelserving/autoscaling/autoscaling/\n\
      \    # https://kserve.github.io/website/modelserving/v1beta1/triton/torchscript/#inference-with-http-endpoint\n\
      \    # https://github.com/kserve/kserve/blob/master/docs/samples/multimodelserving/triton/README.md\n\
      \    # https://kserve.github.io/website/modelserving/v1beta1/triton/torchscript/#inference-with-grpc-endpoint\n\
      \n    # It happens that the credentials for the minio user name and password\
      \ are already in a secret\n    # This just loads them so that we can create\
      \ our own secret to store the S3 connection information\n    # for the Inference\
      \ service\n    r = subprocess.run(\n        [\"kubectl\", \"get\", \"secret\"\
      , minio_credential_secret, \"-oyaml\"],\n        stdout=subprocess.PIPE,\n \
      \       check=True,\n        text=True,\n    )\n    secret = yaml.safe_load(r.stdout)\n\
      \n    s3_credentials_spec = f\"\"\"\n    apiVersion: v1\n    kind: Secret\n\
      \    metadata:\n      name: minio-credentials\n      annotations:\n        serving.kserve.io/s3-endpoint:\
      \ {minio_url} \n        serving.kserve.io/s3-usehttps: \"0\"\n        serving.kserve.io/s3-region:\
      \ \"us-west1\"\n        serving.kserve.io/s3-useanoncredential: \"false\"\n\
      \    type: Opaque\n    data:\n      AWS_ACCESS_KEY_ID: {secret['data']['accesskey']}\n\
      \      AWS_SECRET_ACCESS_KEY: {secret['data']['secretkey']}\n    \"\"\"\n\n\
      \    print(s3_credentials_spec)\n    subprocess.run(\n        [\"kubectl\",\
      \ \"apply\", \"-f\", \"-\"],\n        input=s3_credentials_spec,\n        check=True,\n\
      \        text=True,\n    )\n\n    sa_spec = \"\"\"\n    apiVersion: v1\n   \
      \ kind: ServiceAccount\n    metadata:\n      name: kserve-inference-sa\n   \
      \ secrets:\n    - name: minio-credentials\n    \"\"\"\n\n    print(sa_spec)\n\
      \    subprocess.run(\n        [\"kubectl\", \"apply\", \"-f\", \"-\"], input=sa_spec,\
      \ check=True, text=True\n    )\n\n    ### Remove Existing\n    if rm_existing:\n\
      \        subprocess.run([\"kubectl\", \"delete\", \"inferenceservice\", name],\
      \ check=False)\n\n    ####### Inference Service template #######\n    min_t_replicas\
      \ = (\n        (\"minReplicas: \" + transformer_specification[\"min_replicas\"\
      ])\n        if \"min_replicas\" in transformer_specification\n        else \"\
      \"\n    )\n    max_t_replicas = (\n        (\"maxReplicas: \" + transformer_specification[\"\
      max_replicas\"])\n        if \"min_replicas\" in transformer_specification\n\
      \        else \"\"\n    )\n\n    # There is a bug in the pipeline that prevents\
      \ passing\n    # json serialized strings in some instances, workaround\n   \
      \ # was to use base64 encoded json strings\n    if \"labels\" in transformer_specification:\n\
      \        labels = base64.b64decode(transformer_specification[\"labels\"]).decode(\"\
      utf-8\")\n    else:\n        labels = \"\"\n\n    if transformer_specification:\n\
      \        transform_spec = f\"\"\"\n      transformer:\n        {min_t_replicas}\n\
      \        {max_t_replicas}\n        serviceAccountName: kserve-inference-sa\n\
      \        containers:\n        - image: \"{transformer_specification[\"image\"\
      ]}\n          name: \"{name}-transformer\"\n          command: {transformer_specification.get(\"\
      command\", '[\"python\", \"transform.py\"]')}\n          args: [\"--protocol={predictor_protocol}\"\
      ]\n          env:\n            - name: STORAGE_URI\n              value: {storage_uri}\n\
      \            - name: CLASS_LABELS\n              value: |\n                \
      \    {labels}\n          \"\"\"\n    else:\n        transform_spec = \"\"\n\n\
      \    gpu_resources = (\n        f\"nvidia.com/gpu: {predictor_gpu_allocation}\"\
      \n        if predictor_gpu_allocation\n        else \"\"\n    )\n\n    min_p_replicas\
      \ = (\n        f\"minReplicas: {predictor_min_replicas}\"\n        if predictor_min_replicas\
      \ is not None\n        else \"\"\n    )\n    max_p_replicas = (\n        f\"\
      maxReplicas: {predictor_max_replicas}\"\n        if predictor_max_replicas is\
      \ not None\n        else \"\"\n    )\n\n    predictor_port_spec = (\n      \
      \  '[{\"containerPort\": 9000, \"name\": \"h2c\", \"protocol\": \"TCP\"}]'\n\
      \        if predictor_protocol == \"grpc-v2\"\n        else \"\"\n    )\n\n\
      \    if concurrency_target:\n        autoscaling_target = f\"\"\"\n        autoscaling.knative.dev/target:\
      \ \"{concurrency_target}\"\n        autoscaling.knative.dev/metric: \"concurrency\"\
      \n        \"\"\"\n    else:\n        autoscaling_target = \"\"\n\n    service_spec\
      \ = f\"\"\"\n    apiVersion: serving.kserve.io/v1beta1\n    kind: InferenceService\n\
      \    metadata:\n      name: {name}\n      annotations:\n        sidecar.istio.io/inject:\
      \ \"false\"\n        # https://knative.dev/docs/serving/autoscaling/concurrency/#soft-limit\n\
      \        {autoscaling_target}\n    spec:\n      {transform_spec}\n\n      predictor:\n\
      \        {min_p_replicas}\n        {max_p_replicas}\n        serviceAccountName:\
      \ kserve-inference-sa\n        triton:\n          runtimeVersion: {triton_runtime_version}\n\
      \          args: [ \"--strict-model-config=false\"]\n          storageUri: {storage_uri}\n\
      \          ports: {predictor_port_spec}\n          env:\n          - name: OMP_NUM_THREADS\n\
      \            value: \"1\"\n          resources:\n            limits:\n     \
      \          {gpu_resources}\n    \"\"\"\n\n    print(service_spec)\n    subprocess.run(\n\
      \        [\"kubectl\", \"apply\", \"-f\", \"-\"], input=service_spec, check=True,\
      \ text=True\n    )\n\n    print(\"Waiting for inference service to become available\"\
      )\n    subprocess.run(\n        [\n            \"kubectl\",\n            \"\
      wait\",\n            \"--for=condition=Ready\",\n            f\"inferenceservice/{name}\"\
      ,\n            \"--timeout=600s\",\n        ],\n        check=True,\n    )\n\
      \ndef _deserialize_bool(s) -> bool:\n    from distutils.util import strtobool\n\
      \    return strtobool(s) == 1\n\nimport json\nimport argparse\n_parser = argparse.ArgumentParser(prog='Deploy\
      \ inference service', description='')\n_parser.add_argument(\"--name\", dest=\"\
      name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
      --storage-uri\", dest=\"storage_uri\", type=str, required=True, default=argparse.SUPPRESS)\n\
      _parser.add_argument(\"--minio-url\", dest=\"minio_url\", type=str, required=True,\
      \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--rm-existing\", dest=\"\
      rm_existing\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n\
      _parser.add_argument(\"--minio-credential-secret\", dest=\"minio_credential_secret\"\
      , type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
      --concurrency-target\", dest=\"concurrency_target\", type=int, required=False,\
      \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictor-min-replicas\"\
      , dest=\"predictor_min_replicas\", type=int, required=False, default=argparse.SUPPRESS)\n\
      _parser.add_argument(\"--predictor-max-replicas\", dest=\"predictor_max_replicas\"\
      , type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
      --predictor-gpu-allocation\", dest=\"predictor_gpu_allocation\", type=int, required=False,\
      \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictor-protocol\"\
      , dest=\"predictor_protocol\", type=str, required=False, default=argparse.SUPPRESS)\n\
      _parser.add_argument(\"--triton-runtime-version\", dest=\"triton_runtime_version\"\
      , type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
      --transformer-specification\", dest=\"transformer_specification\", type=json.loads,\
      \ required=False, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
      \n_outputs = deploy_inference_service(**_parsed_args)\n"
    args:
    - --name
    - {inputValue: name}
    - --storage-uri
    - {inputValue: storage_uri}
    - --minio-url
    - {inputValue: minio_url}
    - if:
        cond: {isPresent: rm_existing}
        then:
        - --rm-existing
        - {inputValue: rm_existing}
    - if:
        cond: {isPresent: minio_credential_secret}
        then:
        - --minio-credential-secret
        - {inputValue: minio_credential_secret}
    - if:
        cond: {isPresent: concurrency_target}
        then:
        - --concurrency-target
        - {inputValue: concurrency_target}
    - if:
        cond: {isPresent: predictor_min_replicas}
        then:
        - --predictor-min-replicas
        - {inputValue: predictor_min_replicas}
    - if:
        cond: {isPresent: predictor_max_replicas}
        then:
        - --predictor-max-replicas
        - {inputValue: predictor_max_replicas}
    - if:
        cond: {isPresent: predictor_gpu_allocation}
        then:
        - --predictor-gpu-allocation
        - {inputValue: predictor_gpu_allocation}
    - if:
        cond: {isPresent: predictor_protocol}
        then:
        - --predictor-protocol
        - {inputValue: predictor_protocol}
    - if:
        cond: {isPresent: triton_runtime_version}
        then:
        - --triton-runtime-version
        - {inputValue: triton_runtime_version}
    - if:
        cond: {isPresent: transformer_specification}
        then:
        - --transformer-specification
        - {inputValue: transformer_specification}
