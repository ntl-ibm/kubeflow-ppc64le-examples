{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ba72d66b-7e23-44c5-8fa3-e93d23f2088c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lab_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext lab_black\n"
     ]
    }
   ],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694bbf2f-551d-435a-90c9-e810c64971a3",
   "metadata": {},
   "source": [
    "# Honey Bee Computer Vision Example\n",
    "\n",
    "This example trains a yolov5 model to detect Honey Bees using the V7 version of the https://github.com/ultralytics/yolov5. The purpose of the example is to explain how to train a model using Kubeflow, YOLOV5 and sample data. \n",
    "\n",
    "Accessed classes are:\n",
    "* Bees (workers or foragers)\n",
    "* Bees carrying pollen\n",
    "* Drones\n",
    "* Queens\n",
    "\n",
    "\n",
    "For our purposes, we've included a sample of training data to use for this exercise. \n",
    "We also included initial weights calculated from a much larger version of this dataset with 500 epochs (This takes many hours to train).\n",
    "This should allow us to experiment with the pipeline design without needing to wait hours for training to complete.\n",
    "\n",
    "The dataset was sampled from here: https://universe.roboflow.com/matt-nudi/honey-bee-detection-model-zgjnb (creative commons license).\n",
    "\n",
    "\n",
    "The assumption is that this notebook has a data volume mounted (you can set this up when you create the notebook). The PVC should have been created with an access mode of ReadWriteMany. \n",
    "This allows other pods to mount the volume.\n",
    "\n",
    "The data set will be extracted to the data volume in these next few cells. The data will be loaded into the pipeline via a volume, rather than a download. This simulates a use case where the training data has been pre loaded onto SCOUT, and is large enough where a download is expensive.\n",
    "\n",
    "The volume name is defined in this next cell, as is the path to the extracted Roboflow data set for the bees. To keep things simple, the mount point is the same for both the notebook server, and also for the containers that mount the volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3b76914e-c3d1-496e-8a52-764eab5cd409",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOLUME_CLAIM_NAME = \"yolov5-work\"\n",
    "MOUNT_POINT = \"/home/jovyan/vol-1\"\n",
    "BEE_DATA_SET_PATH = f\"{MOUNT_POINT}/bee_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b4f8c147-ef4d-4cad-9613-f52395b14fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {BEE_DATA_SET_PATH}\n",
    "!tar -xf data/dataset.tar.gz -C {BEE_DATA_SET_PATH} --strip-components 1\n",
    "!cp data/model.pt {MOUNT_POINT}/pre_trained_initial_weights.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf03b6e-d8cd-4900-896d-353f332a27c7",
   "metadata": {},
   "source": [
    "## Imports and constants\n",
    "\n",
    "The base image is an image that has been built to include the libraries for yolov5. The docker file is included in the \"Notebook Container Image Source\" directory. You can build this from the command line on SCOUT, but not from within a Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b555cbe3-c15a-4361-aa42-153fa814e9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.components import InputPath, OutputPath\n",
    "from kubernetes.client.models import (\n",
    "    V1Volume,\n",
    "    V1VolumeMount,\n",
    "    V1PersistentVolumeClaimVolumeSource,\n",
    ")\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "BASE_IMAGE = \"quay.io/ntlawrence/yolov5:pt1.12.1-yolo7.0-v1.1\"\n",
    "COMPONENT_CATALOG_FOLDER = f\"{os.getenv('HOME')}/components\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f6c0e3-98ed-462c-8746-3adc4ecdf930",
   "metadata": {},
   "source": [
    "## Load Data component\n",
    "The first component in the pipeline copies the data from an input path to an output parameter.\n",
    "\n",
    "Essentially this moves the data from the volume into the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f800195c-1c9c-4a92-8106-d884a0975288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_url(\n",
    "    source: str,\n",
    "    dest: OutputPath(str),\n",
    "):\n",
    "    import os\n",
    "    import shutil\n",
    "    from urllib.request import urlretrieve\n",
    "    from urllib.parse import urlparse\n",
    "\n",
    "    # Make target directories if needed\n",
    "    parent_dirs = os.path.basename(dest)\n",
    "    if not os.path.exists(parent_dirs):\n",
    "        os.makedirs(parent_dirs)\n",
    "\n",
    "    # Option to use an empty file to indicate no weights\n",
    "    if not source:\n",
    "        with open(dest, \"w\") as _:\n",
    "            pass\n",
    "\n",
    "    source_details = urlparse(source)\n",
    "\n",
    "    if source_details.scheme == \"file\":\n",
    "        if os.path.isdir(source_details.path):\n",
    "            shutil.copytree(source_details.path, dest)\n",
    "        else:\n",
    "            shutil.copyfile(source_details.path, dest)\n",
    "    elif source_details.scheme in (\"http\", \"https\", \"ftp\", \"ftps\"):\n",
    "        urlretrieve(source, filename=dest)\n",
    "    else:\n",
    "        raise ValueError(f\"source does not use a supported url scheme\")\n",
    "\n",
    "\n",
    "load_from_url_comp = kfp.components.create_component_from_func(\n",
    "    load_from_url, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3ec2ab-9d32-44af-b126-57afbe8e9c39",
   "metadata": {},
   "source": [
    "## Train Model component\n",
    "\n",
    "The training component has several steps to it:\n",
    "\n",
    "* Update the data.yaml with the paths to the train, test, and validation data sets.\n",
    "* Run the python train.py CLI to train the model\n",
    "* Convert the trained model to ONNX\n",
    "\n",
    "When the model is converted to ONNX, it is quantized from FP32 to int8. A subseet of the training data is used in the quantization.\n",
    "\n",
    "The training is initialized with the weights from yolov5s.pt.\n",
    "\n",
    "Performance could be improved by using distributed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "96e35b5a-ad5d-4510-9574-c41c910382f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    onnx_model: OutputPath(str),\n",
    "    pt_model: OutputPath(str),\n",
    "    results: OutputPath(str),\n",
    "    model_cfg: InputPath(str),\n",
    "    initial_weights: InputPath(str),\n",
    "    epochs: int,\n",
    "):\n",
    "    import subprocess\n",
    "    import pathlib\n",
    "    from ruamel.yaml import YAML\n",
    "    import os\n",
    "    import shutil\n",
    "\n",
    "    # data_dir is from an input path, so we have to\n",
    "    # update the data.yaml with the path names\n",
    "    # yaml = YAML()\n",
    "    # dataf = pathlib.Path(f\"{data_dir}/data.yaml\")\n",
    "    # d = yaml.load(dataf)\n",
    "    # d[\"train\"] = f\"{data_dir}/train\"\n",
    "    # d[\"test\"] = f\"{data_dir}/test\"\n",
    "    # d[\"val\"] = f\"{data_dir}/valid\"\n",
    "    # yaml.dump(d, dataf)\n",
    "\n",
    "    # Option to pass an empty file to train from scratch\n",
    "    weights = initial_weights if os.path.getsize(initial_weights) > 0 else \"\"\n",
    "\n",
    "    subprocess.run(\n",
    "        f\"python train.py --img 640 --batch -1 --noplots --epochs {epochs} --cache ram --cfg={model_cfg} \"\n",
    "        f\"--data /dataset/data.yaml --weights {weights} --workers=0 --device=0 --optimizer=Adam\",\n",
    "        check=True,\n",
    "        cwd=\"/yolov5\",\n",
    "        shell=True,\n",
    "    )\n",
    "\n",
    "    subprocess.run(\n",
    "        f\"python export.py --img 640 --include=onnx --int8 \"\n",
    "        f\"--data /dataset/data.yaml --weights /yolov5/runs/train/exp/weights/best.pt --device=0 \",\n",
    "        check=True,\n",
    "        cwd=\"/yolov5\",\n",
    "        shell=True,\n",
    "    )\n",
    "\n",
    "    os.makedirs(os.path.dirname(pt_model), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(onnx_model), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(results), exist_ok=True)\n",
    "\n",
    "    shutil.copyfile(\"/yolov5/runs/train/exp/weights/best.pt\", pt_model)\n",
    "    shutil.copyfile(\"/yolov5/runs/train/exp/weights/best.onnx\", onnx_model)\n",
    "    shutil.copyfile(\"/yolov5/runs/train/exp/results.csv\", results)\n",
    "\n",
    "\n",
    "train_model_comp = kfp.components.create_component_from_func(\n",
    "    train_model, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3693a4de-9443-49a0-9222-466ce0996ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    model: InputPath(str),\n",
    "    model_format: str = \"onnx\",  # onnx, pt, tf ....\n",
    "    conf_thres: float = 0.001,\n",
    "    iou_thres: float = 0.6,\n",
    "    max_det: int = 300,\n",
    "):\n",
    "    import subprocess\n",
    "    import os\n",
    "    import torch\n",
    "    from ruamel.yaml import YAML\n",
    "    import pathlib\n",
    "\n",
    "    print(f\"The size of the model is {os.path.getsize(model)}\")\n",
    "\n",
    "    if model_format == \"onnx\" and not torch.cuda.is_available():\n",
    "        # the base image is built with an onnxruntime for GPU\n",
    "        # This should work for both CPU and GPU, but val.py\n",
    "        # does it's own checking for CPU onnxruntime only\n",
    "        # Since that's not installed and not on pypl for ppc64le,\n",
    "        # The script won't work unless we change up the version\n",
    "        subprocess.run(\n",
    "            \"mamba install -c rocketce onnxruntime=1.13.1=hea80eff_cpu_py39_pb3.19_1 -y\",\n",
    "            check=True,\n",
    "            shell=True,\n",
    "        )\n",
    "\n",
    "    # valy.py uses the file name to determine the type of model\n",
    "    # mode is an input path where the name is generated by kubeflow\n",
    "    # We need to control the name that is used...\n",
    "    named_model = f\"/tmp/{os.path.basename(model)}.{model_format}\"\n",
    "    os.symlink(model, named_model)\n",
    "\n",
    "    # Update data.yaml so that the directories point to where the\n",
    "    # data is\n",
    "    # yaml = YAML()\n",
    "    # dataf = pathlib.Path(f\"{data_dir}/data.yaml\")\n",
    "    # d = yaml.load(dataf)\n",
    "    # d[\"train\"] = f\"{data_dir}/train\"\n",
    "    # d[\"test\"] = f\"{data_dir}/test\"\n",
    "    # d[\"val\"] = f\"{data_dir}/valid\"\n",
    "    # yaml.dump(d, dataf)\n",
    "\n",
    "    subprocess.run(\n",
    "        f\"python val.py --weights {named_model} --data /dataset/data.yaml --img 640 \"\n",
    "        f\"--conf-thres {conf_thres} --iou-thres {iou_thres} --max-det {max_det} --workers=0 \",\n",
    "        check=True,\n",
    "        shell=True,\n",
    "        cwd=\"/yolov5\",\n",
    "    )\n",
    "\n",
    "    subprocess.run(\"find . -print\", cwd=\"/yolov5\", shell=True, check=True)\n",
    "\n",
    "\n",
    "evaluate_model_comp = kfp.components.create_component_from_func(\n",
    "    evaluate_model, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139dd24b-4266-4672-81e8-715de84a23cd",
   "metadata": {},
   "source": [
    "## Upload the ONNX model, using a previously defined component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3e184495-f063-4a9d-b488-dd494ae3000a",
   "metadata": {},
   "outputs": [],
   "source": [
    "UPLOAD_MODEL_COMPONENT = (\n",
    "    f\"{COMPONENT_CATALOG_FOLDER}/model-building/upload-model/component.yaml\"\n",
    ")\n",
    "\n",
    "upload_model_comp = kfp.components.load_component_from_file(UPLOAD_MODEL_COMPONENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8784575-c885-48dc-a78f-2805964f08a8",
   "metadata": {},
   "source": [
    "## Deploy Model Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "343e326d-27bd-4e56-9d86-a136e8030c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPLOY_MODEL_COMPONENT = f\"./deploy_inference_service_component.yaml\"\n",
    "deploy_model_comp = kfp.components.load_component_from_file(DEPLOY_MODEL_COMPONENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a50e359-defd-469d-a22b-61bfb1d7ce69",
   "metadata": {},
   "source": [
    "## Pipeline Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "bded8ffe-e62c-4b52-9d04-2f765ed6fcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"bee-yolov5\")\n",
    "def bee_yolov5(\n",
    "    model_config_url: str,\n",
    "    initial_weights_url: str,\n",
    "    data_vol_pvc_name: str,\n",
    "    data_vol_subpath: str,\n",
    "    blackboard: str = \"mlpipeline-artefacts\",\n",
    "    epochs: int = 750,\n",
    "    minio_url=\"minio-service.kubeflow:9000\",\n",
    "    model_version: int = 1,\n",
    "):\n",
    "    def mount_volume(task, pvc_name, mount_path, volume_subpath):\n",
    "        task.add_volume(\n",
    "            V1Volume(\n",
    "                name=\"vol\",\n",
    "                persistent_volume_claim=V1PersistentVolumeClaimVolumeSource(pvc_name),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        task.add_volume_mount(\n",
    "            V1VolumeMount(name=\"vol\", mount_path=mount_path, sub_path=volume_subpath)\n",
    "        )\n",
    "\n",
    "    create_blackboard = dsl.VolumeOp(\n",
    "        name=\"Create Artefacts Blackboard\",\n",
    "        resource_name=blackboard,\n",
    "        modes=dsl.VOLUME_MODE_RWO,\n",
    "        size=\"4Gi\",\n",
    "        set_owner_reference=True,\n",
    "    )\n",
    "\n",
    "    # Load config Tasks\n",
    "    model_config_task = load_from_url_comp(model_config_url)\n",
    "    model_config_task.after(create_blackboard)\n",
    "    initial_weights_task = load_from_url_comp(initial_weights_url)\n",
    "    initial_weights_task.after(create_blackboard)\n",
    "\n",
    "    # Train Model, also converts to ONNX\n",
    "    train_model_task = train_model_comp(\n",
    "        model_cfg=model_config_task.outputs[\"dest\"],\n",
    "        initial_weights=initial_weights_task.outputs[\"dest\"],\n",
    "        epochs=epochs,\n",
    "    )\n",
    "    train_model_task.set_gpu_limit(1)\n",
    "    train_model_task.set_memory_limit(\"30G\")\n",
    "    mount_volume(train_model_task, data_vol_pvc_name, \"/dataset\", data_vol_subpath)\n",
    "\n",
    "    # Evaluate\n",
    "    evaluate_onnx_model_task = evaluate_model_comp(\n",
    "        model=train_model_task.outputs[\"onnx_model\"],\n",
    "    )\n",
    "    evaluate_onnx_model_task.set_display_name(\"Evaluate ONNX\")\n",
    "    mount_volume(\n",
    "        evaluate_onnx_model_task, data_vol_pvc_name, \"/dataset\", data_vol_subpath\n",
    "    )\n",
    "\n",
    "    evaluate_pt_model_task = evaluate_model_comp(\n",
    "        model=train_model_task.outputs[\"pt_model\"],\n",
    "        model_format=\"pt\",\n",
    "    )\n",
    "    evaluate_pt_model_task.set_display_name(\"Evaluate with best weights\")\n",
    "    mount_volume(\n",
    "        evaluate_pt_model_task, data_vol_pvc_name, \"/dataset\", data_vol_subpath\n",
    "    )\n",
    "\n",
    "    # Upload ONNX model\n",
    "    upload_model_task = upload_model_comp(\n",
    "        train_model_task.outputs[\"onnx_model\"],\n",
    "        minio_url=minio_url,\n",
    "        export_bucket=\"{{workflow.namespace}}-bee\",\n",
    "        model_format=\"onnx\",\n",
    "        model_name=\"bee\",\n",
    "        model_version=model_version,\n",
    "    )\n",
    "\n",
    "    # Deploy Inference Service\n",
    "    deploy_model_task = deploy_model_comp(\n",
    "        name=\"bee\",\n",
    "        rm_existing=True,\n",
    "        storage_uri=\"s3://{{workflow.namespace}}-bee/onnx\",\n",
    "        minio_url=minio_url,\n",
    "        predictor_protocol=\"v2\",\n",
    "    )\n",
    "    deploy_model_task.after(upload_model_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f2156e60-c033-4e1c-bde1-23d1a85fded9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_NAME = \"Bee detector pipeline\"\n",
    "\n",
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=bee_yolov5,\n",
    "    package_path=f\"{PIPELINE_NAME}.yaml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "3bf9f3c5-9a0c-4389-a802-c3a22ff40731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pipeline(pipeline_name: str):\n",
    "    \"\"\"Delete's a pipeline with the specified name\"\"\"\n",
    "\n",
    "    client = kfp.Client()\n",
    "    existing_pipelines = client.list_pipelines(page_size=999).pipelines\n",
    "    matches = (\n",
    "        [ep.id for ep in existing_pipelines if ep.name == pipeline_name]\n",
    "        if existing_pipelines\n",
    "        else []\n",
    "    )\n",
    "    for id in matches:\n",
    "        client.delete_pipeline(id)\n",
    "\n",
    "\n",
    "def get_experiment_id(experiment_name: str) -> str:\n",
    "    \"\"\"Returns the id for the experiment, creating the experiment if needed\"\"\"\n",
    "    client = kfp.Client()\n",
    "    existing_experiments = client.list_experiments(page_size=999).experiments\n",
    "    matches = (\n",
    "        [ex.id for ex in existing_experiments if ex.name == experiment_name]\n",
    "        if existing_experiments\n",
    "        else []\n",
    "    )\n",
    "\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "\n",
    "    exp = client.create_experiment(experiment_name)\n",
    "    return exp.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "ddf37a64-9df0-4b00-a4ee-5b28a6c765d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=/pipeline/#/pipelines/details/2258fc3c-d925-4564-a56e-ad0c81f91ef1>Pipeline details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pipeline names need to be unique, so before we upload,\n",
    "# check for and delete any pipeline with the same name\n",
    "delete_pipeline(PIPELINE_NAME)\n",
    "\n",
    "# upload\n",
    "client = kfp.Client()\n",
    "uploaded_pipeline = client.upload_pipeline(f\"{PIPELINE_NAME}.yaml\", PIPELINE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b5f4733f-90fc-4dd3-842e-d37e5b9710d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/32f43a3f-0f7f-4720-bbf0-6bb4a0111ada\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline_params = {\n",
    "    \"data_vol_pvc_name\": VOLUME_CLAIM_NAME,\n",
    "    \"data_vol_subpath\": \"bee_dataset\",\n",
    "    \"model_config_url\": \"https://github.com/ultralytics/yolov5/raw/v7.0/models/yolov5s.yaml\",\n",
    "    \"initial_weights_url\": \"https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt\",\n",
    "    \"blackboard\": \"mlpipeline-artefacts\",\n",
    "    \"epochs\": \"1\",\n",
    "    \"model_version\": \"1\",\n",
    "}\n",
    "\n",
    "run = client.run_pipeline(\n",
    "    experiment_id=get_experiment_id(\"bee-exp\"),\n",
    "    job_name=\"bees\",\n",
    "    pipeline_id=uploaded_pipeline.id,\n",
    "    params=pipeline_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a774585c-f882-44ab-afc2-7e885fece3b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'Failed', 'error': None, 'time': '0:03:49', 'metrics': None}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TWENTY_MIN = 20 * 60\n",
    "result = client.wait_for_run_completion(run.id, timeout=TWENTY_MIN)\n",
    "{\n",
    "    \"status\": result.run.status,\n",
    "    \"error\": result.run.error,\n",
    "    \"time\": str(result.run.finished_at - result.run.created_at),\n",
    "    \"metrics\": result.run.metrics,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fadb0d2-c85e-4c86-bce5-21509765aa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE = \"/home/jovyan/vol-1/bee_data/test/images/DLQueenIMG_8012-680x538_jpg.rf.aa539ec13ba2b9c5bf7b4de6107f23cd.jpg\"\n",
    "# image = mpimg.imread(IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec10bc8-d2b6-4864-8c4d-cd9022946436",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /home/jovyan/vol-1/yolov5/detect.py --weights=http://bee.kubeflow-ntl.svc.cluster.local/v2/models/bee/infer --data=/home/jovyan/vol-1/bee_data/data.yaml --source=$IMAGE --conf-thres=.7 --iou-thres=.2 --max-det=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19103d8-e0ce-4aae-b10c-4f9b558325eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "\n",
    "img = cv2.imread(\n",
    "    \"/home/jovyan/vol-1/yolov5/runs/detect/exp21/DLQueenIMG_8012-680x538_jpg.rf.aa539ec13ba2b9c5bf7b4de6107f23cd.jpg\"\n",
    ")\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3318ac4-ec13-4c9a-9c9c-02c81b911611",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
