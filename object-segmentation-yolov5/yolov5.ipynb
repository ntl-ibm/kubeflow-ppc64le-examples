{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba72d66b-7e23-44c5-8fa3-e93d23f2088c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694bbf2f-551d-435a-90c9-e810c64971a3",
   "metadata": {},
   "source": [
    "# Honey Bee Computer Vision Example\n",
    "\n",
    "This example uses a dataset from the Roboflow Universe to train a model to detect Honey Bees.\n",
    "\n",
    "Accessed classes are:\n",
    "* Bees (workers or foragers)\n",
    "* Bees carrying pollen\n",
    "* Drones\n",
    "* Queens\n",
    "\n",
    "The dataset can be found here: https://universe.roboflow.com/matt-nudi/honey-bee-detection-model-zgjnb\n",
    "\n",
    "The dataset should be downloaded and extracted to a volume. You can create a volume in Kubeflow from the volumes pannel on the central dashboard. The access mode must be created readWriteMany. The volume can be attached to the notebook server when the server is created.\n",
    "\n",
    "This allows the dataset to be loaded into the pipeline from PVC, rather than needing to wait for expensive download.\n",
    "\n",
    "The volume name is defined in this next cell, as is the path to the extracted Roboflow data set for the bees. To keep things simple, the mount point is the same for both the notebook server, and also for the containers that mount the volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b76914e-c3d1-496e-8a52-764eab5cd409",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOLUME_CLAIM_NAME = \"yolov5-work\"\n",
    "MOUNT_POINT = \"/vol-1\"\n",
    "BEE_DATA_SET_PATH = f\"{MOUNT_POINT}/bee_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf03b6e-d8cd-4900-896d-353f332a27c7",
   "metadata": {},
   "source": [
    "## Imports and constants\n",
    "\n",
    "The base image is an image that has been built to include the libraries for yolov5. The docker file is included in the \"Notebook Container Image Source\" directory. You can build this from the command line on SCOUT, but not from within a Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b555cbe3-c15a-4361-aa42-153fa814e9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.components import InputPath, OutputPath\n",
    "from kubernetes.client.models import (\n",
    "    V1Volume,\n",
    "    V1VolumeMount,\n",
    "    V1PersistentVolumeClaimVolumeSource,\n",
    ")\n",
    "import os\n",
    "\n",
    "\n",
    "BASE_IMAGE = \"quay.io/ntlawrence/yolov5:pt1.12.1-yolo7.0-v1.1\"\n",
    "COMPONENT_CATALOG_FOLDER = f\"{os.getenv('HOME')}/components\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f6c0e3-98ed-462c-8746-3adc4ecdf930",
   "metadata": {},
   "source": [
    "## Load Data component\n",
    "The first component in the pipeline copies the data from an input path to an output parameter.\n",
    "\n",
    "Essentially this moves the data from the volume into the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f800195c-1c9c-4a92-8106-d884a0975288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(source_dataset_dir: str, pipeline_dataset_dir: OutputPath(str)):\n",
    "    import os\n",
    "    import shutil\n",
    "\n",
    "    if not os.path.exists(pipeline_dataset_dir):\n",
    "        os.makedirs(pipeline_dataset_dir)\n",
    "\n",
    "    shutil.copytree(source_dataset_dir, f\"{pipeline_dataset_dir}/data\")\n",
    "\n",
    "\n",
    "load_data_comp = kfp.components.create_component_from_func(\n",
    "    load_data, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3ec2ab-9d32-44af-b126-57afbe8e9c39",
   "metadata": {},
   "source": [
    "## Train Model component\n",
    "\n",
    "The training component has several steps to it:\n",
    "\n",
    "* Update the data.yaml with the paths to the train, test, and validation data sets.\n",
    "* Run the python train.py CLI to train the model\n",
    "* Convert the trained model to ONNX\n",
    "\n",
    "When the model is converted to ONNX, it is quantized from FP32 to int8. A subseet of the training data is used in the quantization.\n",
    "\n",
    "The training is initialized with the weights from yolov5s.pt.\n",
    "\n",
    "Performance could be improved by using distributed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e35b5a-ad5d-4510-9574-c41c910382f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_dir: InputPath(str), model: OutputPath(str), epochs: int = 300):\n",
    "    import subprocess\n",
    "    import pathlib\n",
    "    from ruamel.yaml import YAML\n",
    "    import os\n",
    "    import shutil\n",
    "\n",
    "    yaml = YAML()\n",
    "    dataf = pathlib.Path(f\"{data_dir}/data/data.yaml\")\n",
    "    d = yaml.load(dataf)\n",
    "    d[\"train\"] = f\"{data_dir}/data/train\"\n",
    "    d[\"test\"] = f\"{data_dir}/data/test\"\n",
    "    d[\"val\"] = f\"{data_dir}/data/valid\"\n",
    "    yaml.dump(d, dataf)\n",
    "\n",
    "    subprocess.run(\n",
    "        f\"python train.py --img 640 --batch -1 --noplots --epochs {epochs} --cache ram \"\n",
    "        f\"--data {data_dir}/data/data.yaml --weights yolov5s.pt --workers=0 --device=0 --optimizer=Adam\",\n",
    "        check=True,\n",
    "        cwd=\"/yolov5\",\n",
    "        shell=True,\n",
    "    )\n",
    "\n",
    "    subprocess.run(\n",
    "        f\"python export.py --img 640 --include=onnx --int8 \"\n",
    "        f\"--data {data_dir}/data/data.yaml --weights /yolov5/runs/train/exp/weights/best.pt --device=0 \",\n",
    "        check=True,\n",
    "        cwd=\"/yolov5\",\n",
    "        shell=True,\n",
    "    )\n",
    "\n",
    "    target_path = os.path.basename(model)\n",
    "    if not os.path.exists(target_path):\n",
    "        os.makedirs(target_path)\n",
    "\n",
    "    shutil.copyfile(\"/yolov5/runs/train/exp/weights/best.onnx\", model)\n",
    "\n",
    "\n",
    "train_model_comp = kfp.components.create_component_from_func(\n",
    "    train_model, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139dd24b-4266-4672-81e8-715de84a23cd",
   "metadata": {},
   "source": [
    "## Upload the ONNX model, using a previously defined component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e184495-f063-4a9d-b488-dd494ae3000a",
   "metadata": {},
   "outputs": [],
   "source": [
    "UPLOAD_MODEL_COMPONENT = (\n",
    "    f\"{COMPONENT_CATALOG_FOLDER}/model-building/upload-model/component.yaml\"\n",
    ")\n",
    "\n",
    "upload_model_comp = kfp.components.load_component_from_file(UPLOAD_MODEL_COMPONENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8784575-c885-48dc-a78f-2805964f08a8",
   "metadata": {},
   "source": [
    "## Deploy Model Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343e326d-27bd-4e56-9d86-a136e8030c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPLOY_MODEL_COMPONENT = f\"./deploy_inference_service_component.yaml\"\n",
    "deploy_model_comp = kfp.components.load_component_from_file(DEPLOY_MODEL_COMPONENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a50e359-defd-469d-a22b-61bfb1d7ce69",
   "metadata": {},
   "source": [
    "## Pipeline Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bded8ffe-e62c-4b52-9d04-2f765ed6fcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"bee-yolov5\")\n",
    "def bee_yolov5(epochs: int = 300):\n",
    "    load_data_task = load_data_comp(BEE_DATA_SET_PATH)\n",
    "    load_data_task.add_volume(\n",
    "        V1Volume(\n",
    "            name=\"vol-1\",\n",
    "            persistent_volume_claim=V1PersistentVolumeClaimVolumeSource(\n",
    "                VOLUME_CLAIM_NAME\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    load_data_task.add_volume_mount(V1VolumeMount(name=\"vol-1\", mount_path=MOUNT_POINT))\n",
    "    train_model_task = train_model_comp(\n",
    "        data_dir=load_data_task.outputs[\"pipeline_dataset_dir\"], epochs=1\n",
    "    )\n",
    "    train_model_task.set_gpu_limit(1)\n",
    "    train_model_task.set_memory_limit(\"30G\")\n",
    "    upload_model_task = upload_model_comp(\n",
    "        train_model_task.outputs[\"model\"],\n",
    "        minio_url=\"minio-service.kubeflow:9000\",\n",
    "        export_bucket=\"{{workflow.namespace}}-bee-yolov5\",\n",
    "        model_format=\"onnx\",\n",
    "        model_name=\"bee\",\n",
    "        model_version=1,\n",
    "    )\n",
    "    deploy_model_task = deploy_model_comp(\n",
    "        name=\"bee\",\n",
    "        rm_existing=True,\n",
    "        storage_uri=\"s3://{{workflow.namespace}}-bee-yolov5/onnx\",\n",
    "        minio_url=\"minio-service.kubeflow:9000\",\n",
    "        predictor_protocol=\"v2\",\n",
    "    )\n",
    "    deploy_model_task.after(upload_model_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2156e60-c033-4e1c-bde1-23d1a85fded9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_NAME = \"Bee detector pipeline\"\n",
    "\n",
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=bee_yolov5,\n",
    "    package_path=f\"{PIPELINE_NAME}.yaml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf9f3c5-9a0c-4389-a802-c3a22ff40731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pipeline(pipeline_name: str):\n",
    "    \"\"\"Delete's a pipeline with the specified name\"\"\"\n",
    "\n",
    "    client = kfp.Client()\n",
    "    existing_pipelines = client.list_pipelines(page_size=999).pipelines\n",
    "    matches = (\n",
    "        [ep.id for ep in existing_pipelines if ep.name == pipeline_name]\n",
    "        if existing_pipelines\n",
    "        else []\n",
    "    )\n",
    "    for id in matches:\n",
    "        client.delete_pipeline(id)\n",
    "\n",
    "\n",
    "def get_experiment_id(experiment_name: str) -> str:\n",
    "    \"\"\"Returns the id for the experiment, creating the experiment if needed\"\"\"\n",
    "    client = kfp.Client()\n",
    "    existing_experiments = client.list_experiments(page_size=999).experiments\n",
    "    matches = (\n",
    "        [ex.id for ex in existing_experiments if ex.name == experiment_name]\n",
    "        if existing_experiments\n",
    "        else []\n",
    "    )\n",
    "\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "\n",
    "    exp = client.create_experiment(experiment_name)\n",
    "    return exp.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf37a64-9df0-4b00-a4ee-5b28a6c765d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline names need to be unique, so before we upload,\n",
    "# check for and delete any pipeline with the same name\n",
    "delete_pipeline(PIPELINE_NAME)\n",
    "\n",
    "# upload\n",
    "client = kfp.Client()\n",
    "uploaded_pipeline = client.upload_pipeline(f\"{PIPELINE_NAME}.yaml\", PIPELINE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f4733f-90fc-4dd3-842e-d37e5b9710d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = client.run_pipeline(\n",
    "    experiment_id=get_experiment_id(\"bee-exp\"),\n",
    "    job_name=\"bees\",\n",
    "    pipeline_id=uploaded_pipeline.id,\n",
    "    params={\"epochs\": 1},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a774585c-f882-44ab-afc2-7e885fece3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TWENTY_MIN = 20 * 60\n",
    "result = client.wait_for_run_completion(run.id, timeout=TWENTY_MIN)\n",
    "{\n",
    "    \"status\": result.run.status,\n",
    "    \"error\": result.run.error,\n",
    "    \"time\": str(result.run.finished_at - result.run.created_at),\n",
    "    \"metrics\": result.run.metrics,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fadb0d2-c85e-4c86-bce5-21509765aa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE = \"/home/jovyan/vol-1/bee_data/test/images/DLQueenIMG_8012-680x538_jpg.rf.aa539ec13ba2b9c5bf7b4de6107f23cd.jpg\"\n",
    "# image = mpimg.imread(IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec10bc8-d2b6-4864-8c4d-cd9022946436",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /home/jovyan/vol-1/yolov5/detect.py --weights=http://bee.kubeflow-ntl.svc.cluster.local/v2/models/bee/infer --data=/home/jovyan/vol-1/bee_data/data.yaml --source=$IMAGE --conf-thres=.7 --iou-thres=.2 --max-det=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19103d8-e0ce-4aae-b10c-4f9b558325eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "\n",
    "img = cv2.imread(\n",
    "    \"/home/jovyan/vol-1/yolov5/runs/detect/exp21/DLQueenIMG_8012-680x538_jpg.rf.aa539ec13ba2b9c5bf7b4de6107f23cd.jpg\"\n",
    ")\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3318ac4-ec13-4c9a-9c9c-02c81b911611",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
