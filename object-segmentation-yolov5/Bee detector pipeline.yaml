apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: bee-yolov5-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.18, pipelines.kubeflow.org/pipeline_compilation_time: '2023-01-25T22:33:32.011898',
    pipelines.kubeflow.org/pipeline_spec: '{"inputs": [{"name": "dataset_path", "type":
      "String"}, {"name": "initial_weights", "optional": true, "type": "String"},
      {"default": "300", "name": "epochs", "optional": true, "type": "Integer"}],
      "name": "bee-yolov5"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.18}
spec:
  entrypoint: bee-yolov5
  templates:
  - name: bee-yolov5
    inputs:
      parameters:
      - {name: dataset_path}
      - {name: initial_weights}
      artifacts:
      - {name: initial_weights}
    dag:
      tasks:
      - name: deploy-inference-service
        template: deploy-inference-service
        dependencies: [upload-model]
      - name: load-data
        template: load-data
        arguments:
          parameters:
          - {name: dataset_path, value: '{{inputs.parameters.dataset_path}}'}
          - {name: initial_weights, value: '{{inputs.parameters.initial_weights}}'}
      - name: train-model
        template: train-model
        dependencies: [load-data]
        arguments:
          artifacts:
          - {name: initial_weights, from: '{{inputs.artifacts.initial_weights}}'}
          - {name: load-data-pipeline_dataset_dir, from: '{{tasks.load-data.outputs.artifacts.load-data-pipeline_dataset_dir}}'}
      - name: upload-model
        template: upload-model
        dependencies: [train-model]
        arguments:
          artifacts:
          - {name: train-model-model, from: '{{tasks.train-model.outputs.artifacts.train-model-model}}'}
  - name: deploy-inference-service
    container:
      args:
      - --name
      - bee
      - --storage-uri
      - s3://{{workflow.namespace}}-bee-yolov5/onnx
      - --minio-url
      - minio-service.kubeflow:9000
      - --rm-existing
      - "True"
      - --minio-credential-secret
      - mlpipeline-minio-artifact
      - --predictor-gpu-allocation
      - '0'
      - --predictor-protocol
      - v2
      - --triton-runtime-version
      - 22.03-py3
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def deploy_inference_service(\n    name,\n    storage_uri,\n    minio_url,\n\
        \    rm_existing = False,\n    minio_credential_secret=\"mlpipeline-minio-artifact\"\
        ,\n    concurrency_target = None,\n    predictor_min_replicas = None,\n  \
        \  predictor_max_replicas = None,\n    predictor_gpu_allocation = 0,\n   \
        \ predictor_protocol = \"v2\",  # or grpc-v2\n    triton_runtime_version =\
        \ \"22.03-py3\",\n    transformer_specification = None,\n):\n    import os\n\
        \    import subprocess\n    import yaml\n\n    # https://kserve.github.io/website/modelserving/storage/s3/s3/#predict-on-a-inferenceservice-with-a-saved-model-on-s3\n\
        \    # https://kserve.github.io/website/reference/api/\n    # https://kserve.github.io/website/modelserving/autoscaling/autoscaling/\n\
        \    # https://kserve.github.io/website/modelserving/v1beta1/triton/torchscript/#inference-with-http-endpoint\n\
        \    # https://github.com/kserve/kserve/blob/master/docs/samples/multimodelserving/triton/README.md\n\
        \    # https://kserve.github.io/website/modelserving/v1beta1/triton/torchscript/#inference-with-grpc-endpoint\n\
        \n    # It happens that the credentials for the minio user name and password\
        \ are already in a secret\n    # This just loads them so that we can create\
        \ our own secret to store the S3 connection information\n    # for the Inference\
        \ service\n    r = subprocess.run(\n        [\"kubectl\", \"get\", \"secret\"\
        , minio_credential_secret, \"-oyaml\"],\n        stdout=subprocess.PIPE,\n\
        \        check=True,\n        text=True,\n    )\n    secret = yaml.safe_load(r.stdout)\n\
        \n    s3_credentials_spec = f\"\"\"\n    apiVersion: v1\n    kind: Secret\n\
        \    metadata:\n      name: minio-credentials\n      annotations:\n      \
        \  serving.kserve.io/s3-endpoint: {minio_url} \n        serving.kserve.io/s3-usehttps:\
        \ \"0\"\n        serving.kserve.io/s3-region: \"us-west1\"\n        serving.kserve.io/s3-useanoncredential:\
        \ \"false\"\n    type: Opaque\n    data:\n      AWS_ACCESS_KEY_ID: {secret['data']['accesskey']}\n\
        \      AWS_SECRET_ACCESS_KEY: {secret['data']['secretkey']}\n    \"\"\"\n\n\
        \    print(s3_credentials_spec)\n    subprocess.run(\n        [\"kubectl\"\
        , \"apply\", \"-f\", \"-\"],\n        input=s3_credentials_spec,\n       \
        \ check=True,\n        text=True,\n    )\n\n    sa_spec = \"\"\"\n    apiVersion:\
        \ v1\n    kind: ServiceAccount\n    metadata:\n      name: kserve-inference-sa\n\
        \    secrets:\n    - name: minio-credentials\n    \"\"\"\n\n    print(sa_spec)\n\
        \    subprocess.run(\n        [\"kubectl\", \"apply\", \"-f\", \"-\"], input=sa_spec,\
        \ check=True, text=True\n    )\n\n    ### Remove Existing\n    if rm_existing:\n\
        \        subprocess.run([\"kubectl\", \"delete\", \"inferenceservice\", name],\
        \ check=False)\n\n    ####### Inference Service template #######\n    if transformer_specification:\n\
        \        transform_spec = f\"\"\"\n      transformer:\n        {t_min_replicas}\n\
        \        {t_max_replicas}\n        serviceAccountName: kserve-inference-sa\n\
        \        containers:\n        - image: \"{transformer_specification[\"image\"\
        ]}\n          name: \"{name}-transformer\"\n          command: {transformer_specification.get(\"\
        command\", '[\"python\", \"transform.py\"]')}\n          args: [\"--protocol={predictor_protocol}\"\
        ]\n          env:\n            - name: STORAGE_URI\n              value: {storage_uri}\n\
        \            - name: CLASS_LABELS\n              value: |\n              \
        \      {transformer_specification.get(\"labels\")}\n          \"\"\"\n   \
        \ else:\n        transform_spec = \"\"\n\n    gpu_resources = (\n        f\"\
        nvidia.com/gpu: {predictor_gpu_allocation}\"\n        if predictor_gpu_allocation\n\
        \        else \"\"\n    )\n\n    min_p_replicas = (\n        f\"minReplicas:\
        \ {predictor_min_replicas}\"\n        if predictor_min_replicas is not None\n\
        \        else \"\"\n    )\n    max_p_replicas = (\n        f\"maxReplicas:\
        \ {predictor_max_replicas}\"\n        if predictor_max_replicas is not None\n\
        \        else \"\"\n    )\n\n    predictor_port_spec = (\n        '[{\"containerPort\"\
        : 9000, \"name\": \"h2c\", \"protocol\": \"TCP\"}]'\n        if predictor_protocol\
        \ == \"grpc-v2\"\n        else \"\"\n    )\n\n    if concurrency_target:\n\
        \        autoscaling_target=f'''\n        autoscaling.knative.dev/target:\
        \ \"{concurrency_target}\"\n        autoscaling.knative.dev/metric: \"concurrency\"\
        \n        '''\n    else:\n        autoscaling_target=''\n\n    service_spec\
        \ = f\"\"\"\n    apiVersion: serving.kserve.io/v1beta1\n    kind: InferenceService\n\
        \    metadata:\n      name: {name}\n      annotations:\n        sidecar.istio.io/inject:\
        \ \"false\"\n        # https://knative.dev/docs/serving/autoscaling/concurrency/#soft-limit\n\
        \        {autoscaling_target}\n    spec:\n      {transform_spec}\n\n     \
        \ predictor:\n        {min_p_replicas}\n        {max_p_replicas}\n       \
        \ serviceAccountName: kserve-inference-sa\n        triton:\n          runtimeVersion:\
        \ {triton_runtime_version}\n          args: [ \"--strict-model-config=false\"\
        ]\n          storageUri: {storage_uri}\n          ports: {predictor_port_spec}\n\
        \          env:\n          - name: OMP_NUM_THREADS\n            value: \"\
        1\"\n          resources:\n            limits:\n               {gpu_resources}\n\
        \    \"\"\"\n\n    print(service_spec)\n    subprocess.run(\n        [\"kubectl\"\
        , \"apply\", \"-f\", \"-\"], input=service_spec, check=True, text=True\n \
        \   )\n\n    print(\"Waiting for inference service to become available\")\n\
        \    subprocess.run(\n        [\n            \"kubectl\",\n            \"\
        wait\",\n            \"--for=condition=Ready\",\n            f\"inferenceservice/{name}\"\
        ,\n            \"--timeout=600s\",\n        ],\n        check=True,\n    )\n\
        \ndef _deserialize_bool(s) -> bool:\n    from distutils.util import strtobool\n\
        \    return strtobool(s) == 1\n\nimport json\nimport argparse\n_parser = argparse.ArgumentParser(prog='Deploy\
        \ inference service', description='')\n_parser.add_argument(\"--name\", dest=\"\
        name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --storage-uri\", dest=\"storage_uri\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--minio-url\", dest=\"minio_url\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--rm-existing\", dest=\"\
        rm_existing\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--minio-credential-secret\", dest=\"minio_credential_secret\"\
        , type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --concurrency-target\", dest=\"concurrency_target\", type=int, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictor-min-replicas\"\
        , dest=\"predictor_min_replicas\", type=int, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--predictor-max-replicas\", dest=\"predictor_max_replicas\"\
        , type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --predictor-gpu-allocation\", dest=\"predictor_gpu_allocation\", type=int,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictor-protocol\"\
        , dest=\"predictor_protocol\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--triton-runtime-version\", dest=\"triton_runtime_version\"\
        , type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --transformer-specification\", dest=\"transformer_specification\", type=json.loads,\
        \ required=False, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
        \n_outputs = deploy_inference_service(**_parsed_args)\n"
      image: quay.io/ibm/kubeflow-notebook-image-ppc64le:elyra3.13.0-py3.8-tf2.9.2-pt1.12.1-v0
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--name", {"inputValue": "name"}, "--storage-uri", {"inputValue":
          "storage_uri"}, "--minio-url", {"inputValue": "minio_url"}, {"if": {"cond":
          {"isPresent": "rm_existing"}, "then": ["--rm-existing", {"inputValue": "rm_existing"}]}},
          {"if": {"cond": {"isPresent": "minio_credential_secret"}, "then": ["--minio-credential-secret",
          {"inputValue": "minio_credential_secret"}]}}, {"if": {"cond": {"isPresent":
          "concurrency_target"}, "then": ["--concurrency-target", {"inputValue": "concurrency_target"}]}},
          {"if": {"cond": {"isPresent": "predictor_min_replicas"}, "then": ["--predictor-min-replicas",
          {"inputValue": "predictor_min_replicas"}]}}, {"if": {"cond": {"isPresent":
          "predictor_max_replicas"}, "then": ["--predictor-max-replicas", {"inputValue":
          "predictor_max_replicas"}]}}, {"if": {"cond": {"isPresent": "predictor_gpu_allocation"},
          "then": ["--predictor-gpu-allocation", {"inputValue": "predictor_gpu_allocation"}]}},
          {"if": {"cond": {"isPresent": "predictor_protocol"}, "then": ["--predictor-protocol",
          {"inputValue": "predictor_protocol"}]}}, {"if": {"cond": {"isPresent": "triton_runtime_version"},
          "then": ["--triton-runtime-version", {"inputValue": "triton_runtime_version"}]}},
          {"if": {"cond": {"isPresent": "transformer_specification"}, "then": ["--transformer-specification",
          {"inputValue": "transformer_specification"}]}}], "command": ["sh", "-ec",
          "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def deploy_inference_service(\n    name,\n    storage_uri,\n    minio_url,\n    rm_existing
          = False,\n    minio_credential_secret=\"mlpipeline-minio-artifact\",\n    concurrency_target
          = None,\n    predictor_min_replicas = None,\n    predictor_max_replicas
          = None,\n    predictor_gpu_allocation = 0,\n    predictor_protocol = \"v2\",  #
          or grpc-v2\n    triton_runtime_version = \"22.03-py3\",\n    transformer_specification
          = None,\n):\n    import os\n    import subprocess\n    import yaml\n\n    #
          https://kserve.github.io/website/modelserving/storage/s3/s3/#predict-on-a-inferenceservice-with-a-saved-model-on-s3\n    #
          https://kserve.github.io/website/reference/api/\n    # https://kserve.github.io/website/modelserving/autoscaling/autoscaling/\n    #
          https://kserve.github.io/website/modelserving/v1beta1/triton/torchscript/#inference-with-http-endpoint\n    #
          https://github.com/kserve/kserve/blob/master/docs/samples/multimodelserving/triton/README.md\n    #
          https://kserve.github.io/website/modelserving/v1beta1/triton/torchscript/#inference-with-grpc-endpoint\n\n    #
          It happens that the credentials for the minio user name and password are
          already in a secret\n    # This just loads them so that we can create our
          own secret to store the S3 connection information\n    # for the Inference
          service\n    r = subprocess.run(\n        [\"kubectl\", \"get\", \"secret\",
          minio_credential_secret, \"-oyaml\"],\n        stdout=subprocess.PIPE,\n        check=True,\n        text=True,\n    )\n    secret
          = yaml.safe_load(r.stdout)\n\n    s3_credentials_spec = f\"\"\"\n    apiVersion:
          v1\n    kind: Secret\n    metadata:\n      name: minio-credentials\n      annotations:\n        serving.kserve.io/s3-endpoint:
          {minio_url} \n        serving.kserve.io/s3-usehttps: \"0\"\n        serving.kserve.io/s3-region:
          \"us-west1\"\n        serving.kserve.io/s3-useanoncredential: \"false\"\n    type:
          Opaque\n    data:\n      AWS_ACCESS_KEY_ID: {secret[''data''][''accesskey'']}\n      AWS_SECRET_ACCESS_KEY:
          {secret[''data''][''secretkey'']}\n    \"\"\"\n\n    print(s3_credentials_spec)\n    subprocess.run(\n        [\"kubectl\",
          \"apply\", \"-f\", \"-\"],\n        input=s3_credentials_spec,\n        check=True,\n        text=True,\n    )\n\n    sa_spec
          = \"\"\"\n    apiVersion: v1\n    kind: ServiceAccount\n    metadata:\n      name:
          kserve-inference-sa\n    secrets:\n    - name: minio-credentials\n    \"\"\"\n\n    print(sa_spec)\n    subprocess.run(\n        [\"kubectl\",
          \"apply\", \"-f\", \"-\"], input=sa_spec, check=True, text=True\n    )\n\n    ###
          Remove Existing\n    if rm_existing:\n        subprocess.run([\"kubectl\",
          \"delete\", \"inferenceservice\", name], check=False)\n\n    ####### Inference
          Service template #######\n    if transformer_specification:\n        transform_spec
          = f\"\"\"\n      transformer:\n        {t_min_replicas}\n        {t_max_replicas}\n        serviceAccountName:
          kserve-inference-sa\n        containers:\n        - image: \"{transformer_specification[\"image\"]}\n          name:
          \"{name}-transformer\"\n          command: {transformer_specification.get(\"command\",
          ''[\"python\", \"transform.py\"]'')}\n          args: [\"--protocol={predictor_protocol}\"]\n          env:\n            -
          name: STORAGE_URI\n              value: {storage_uri}\n            - name:
          CLASS_LABELS\n              value: |\n                    {transformer_specification.get(\"labels\")}\n          \"\"\"\n    else:\n        transform_spec
          = \"\"\n\n    gpu_resources = (\n        f\"nvidia.com/gpu: {predictor_gpu_allocation}\"\n        if
          predictor_gpu_allocation\n        else \"\"\n    )\n\n    min_p_replicas
          = (\n        f\"minReplicas: {predictor_min_replicas}\"\n        if predictor_min_replicas
          is not None\n        else \"\"\n    )\n    max_p_replicas = (\n        f\"maxReplicas:
          {predictor_max_replicas}\"\n        if predictor_max_replicas is not None\n        else
          \"\"\n    )\n\n    predictor_port_spec = (\n        ''[{\"containerPort\":
          9000, \"name\": \"h2c\", \"protocol\": \"TCP\"}]''\n        if predictor_protocol
          == \"grpc-v2\"\n        else \"\"\n    )\n\n    if concurrency_target:\n        autoscaling_target=f''''''\n        autoscaling.knative.dev/target:
          \"{concurrency_target}\"\n        autoscaling.knative.dev/metric: \"concurrency\"\n        ''''''\n    else:\n        autoscaling_target=''''\n\n    service_spec
          = f\"\"\"\n    apiVersion: serving.kserve.io/v1beta1\n    kind: InferenceService\n    metadata:\n      name:
          {name}\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n        #
          https://knative.dev/docs/serving/autoscaling/concurrency/#soft-limit\n        {autoscaling_target}\n    spec:\n      {transform_spec}\n\n      predictor:\n        {min_p_replicas}\n        {max_p_replicas}\n        serviceAccountName:
          kserve-inference-sa\n        triton:\n          runtimeVersion: {triton_runtime_version}\n          args:
          [ \"--strict-model-config=false\"]\n          storageUri: {storage_uri}\n          ports:
          {predictor_port_spec}\n          env:\n          - name: OMP_NUM_THREADS\n            value:
          \"1\"\n          resources:\n            limits:\n               {gpu_resources}\n    \"\"\"\n\n    print(service_spec)\n    subprocess.run(\n        [\"kubectl\",
          \"apply\", \"-f\", \"-\"], input=service_spec, check=True, text=True\n    )\n\n    print(\"Waiting
          for inference service to become available\")\n    subprocess.run(\n        [\n            \"kubectl\",\n            \"wait\",\n            \"--for=condition=Ready\",\n            f\"inferenceservice/{name}\",\n            \"--timeout=600s\",\n        ],\n        check=True,\n    )\n\ndef
          _deserialize_bool(s) -> bool:\n    from distutils.util import strtobool\n    return
          strtobool(s) == 1\n\nimport json\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Deploy
          inference service'', description='''')\n_parser.add_argument(\"--name\",
          dest=\"name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--storage-uri\",
          dest=\"storage_uri\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--minio-url\",
          dest=\"minio_url\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--rm-existing\",
          dest=\"rm_existing\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--minio-credential-secret\",
          dest=\"minio_credential_secret\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--concurrency-target\",
          dest=\"concurrency_target\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictor-min-replicas\",
          dest=\"predictor_min_replicas\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictor-max-replicas\",
          dest=\"predictor_max_replicas\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictor-gpu-allocation\",
          dest=\"predictor_gpu_allocation\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictor-protocol\",
          dest=\"predictor_protocol\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--triton-runtime-version\",
          dest=\"triton_runtime_version\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--transformer-specification\",
          dest=\"transformer_specification\", type=json.loads, required=False, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = deploy_inference_service(**_parsed_args)\n"],
          "image": "quay.io/ibm/kubeflow-notebook-image-ppc64le:elyra3.13.0-py3.8-tf2.9.2-pt1.12.1-v0"}},
          "inputs": [{"name": "name", "type": "String"}, {"name": "storage_uri", "type":
          "String"}, {"name": "minio_url", "type": "String"}, {"default": "False",
          "name": "rm_existing", "optional": true, "type": "Boolean"}, {"default":
          "mlpipeline-minio-artifact", "name": "minio_credential_secret", "optional":
          true}, {"name": "concurrency_target", "optional": true, "type": "Integer"},
          {"name": "predictor_min_replicas", "optional": true, "type": "Integer"},
          {"name": "predictor_max_replicas", "optional": true, "type": "Integer"},
          {"default": "0", "name": "predictor_gpu_allocation", "optional": true, "type":
          "Integer"}, {"default": "v2", "name": "predictor_protocol", "optional":
          true, "type": "String"}, {"default": "22.03-py3", "name": "triton_runtime_version",
          "optional": true, "type": "String"}, {"name": "transformer_specification",
          "optional": true, "type": "typing.Dict[str, typing.Union[str, int]]"}],
          "name": "Deploy inference service"}', pipelines.kubeflow.org/component_ref: '{"digest":
          "8b89e82d18a951f64d5b1a66187e34e7705a4e857d96b85700b676fd59375d93", "url":
          "./deploy_inference_service_component.yaml"}', pipelines.kubeflow.org/arguments.parameters: '{"minio_credential_secret":
          "mlpipeline-minio-artifact", "minio_url": "minio-service.kubeflow:9000",
          "name": "bee", "predictor_gpu_allocation": "0", "predictor_protocol": "v2",
          "rm_existing": "True", "storage_uri": "s3://{{workflow.namespace}}-bee-yolov5/onnx",
          "triton_runtime_version": "22.03-py3"}'}
  - name: load-data
    container:
      args: [--source-dataset-dir, '{{inputs.parameters.dataset_path}}', --initial-weights,
        '{{inputs.parameters.initial_weights}}', --pipeline-dataset-dir, /tmp/outputs/pipeline_dataset_dir/data,
        --pipeline-initial-weights, /tmp/outputs/pipeline_initial_weights/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def load_data(source_dataset_dir,
                      initial_weights,
                      pipeline_dataset_dir,
                      pipeline_initial_weights):
            import os
            import shutil

            if not os.path.exists(pipeline_dataset_dir):
                os.makedirs(pipeline_dataset_dir)

            if not os.path.exists(os.path.dirname(pipeline_initial_weights)):
                os.makedirs(os.path.dirname(pipeline_initial_weights))

            shutil.copytree(source_dataset_dir, f"{pipeline_dataset_dir}/data")
            shutil.copyfile(initial_weights, pipeline_initial_weights)

        import argparse
        _parser = argparse.ArgumentParser(prog='Load data', description='')
        _parser.add_argument("--source-dataset-dir", dest="source_dataset_dir", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--initial-weights", dest="initial_weights", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--pipeline-dataset-dir", dest="pipeline_dataset_dir", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--pipeline-initial-weights", dest="pipeline_initial_weights", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = load_data(**_parsed_args)
      image: quay.io/ntlawrence/yolov5:pt1.12.1-yolo7.0-v1.1
      volumeMounts:
      - {mountPath: /home/jovyan/vol-1, name: vol-1}
    inputs:
      parameters:
      - {name: dataset_path}
      - {name: initial_weights}
    outputs:
      artifacts:
      - {name: load-data-pipeline_dataset_dir, path: /tmp/outputs/pipeline_dataset_dir/data}
      - {name: load-data-pipeline_initial_weights, path: /tmp/outputs/pipeline_initial_weights/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--source-dataset-dir", {"inputValue": "source_dataset_dir"},
          "--initial-weights", {"inputValue": "initial_weights"}, "--pipeline-dataset-dir",
          {"outputPath": "pipeline_dataset_dir"}, "--pipeline-initial-weights", {"outputPath":
          "pipeline_initial_weights"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef load_data(source_dataset_dir,\n              initial_weights,\n              pipeline_dataset_dir,\n              pipeline_initial_weights):\n    import
          os\n    import shutil\n\n    if not os.path.exists(pipeline_dataset_dir):\n        os.makedirs(pipeline_dataset_dir)\n\n    if
          not os.path.exists(os.path.dirname(pipeline_initial_weights)):\n        os.makedirs(os.path.dirname(pipeline_initial_weights))\n\n    shutil.copytree(source_dataset_dir,
          f\"{pipeline_dataset_dir}/data\")\n    shutil.copyfile(initial_weights,
          pipeline_initial_weights)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Load
          data'', description='''')\n_parser.add_argument(\"--source-dataset-dir\",
          dest=\"source_dataset_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--initial-weights\",
          dest=\"initial_weights\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--pipeline-dataset-dir\",
          dest=\"pipeline_dataset_dir\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--pipeline-initial-weights\",
          dest=\"pipeline_initial_weights\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = load_data(**_parsed_args)\n"], "image": "quay.io/ntlawrence/yolov5:pt1.12.1-yolo7.0-v1.1"}},
          "inputs": [{"name": "source_dataset_dir", "type": "String"}, {"name": "initial_weights",
          "type": "String"}], "name": "Load data", "outputs": [{"name": "pipeline_dataset_dir",
          "type": "String"}, {"name": "pipeline_initial_weights", "type": "String"}]}',
        pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"initial_weights":
          "{{inputs.parameters.initial_weights}}", "source_dataset_dir": "{{inputs.parameters.dataset_path}}"}'}
    volumes:
    - name: vol-1
      persistentVolumeClaim: {claimName: yolov5-work}
  - name: train-model
    container:
      args: [--data-dir, /tmp/inputs/data_dir/data, --initial-weights, /tmp/inputs/initial_weights/data,
        --epochs, '1', --model, /tmp/outputs/model/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def train_model(
            data_dir,
            model,
            initial_weights = None,
            epochs = 500,
        ):
            import subprocess
            import pathlib
            from ruamel.yaml import YAML
            import os
            import shutil

            yaml = YAML()
            dataf = pathlib.Path(f"{data_dir}/data/data.yaml")
            d = yaml.load(dataf)
            d["train"] = f"{data_dir}/data/train"
            d["test"] = f"{data_dir}/data/test"
            d["val"] = f"{data_dir}/data/valid"
            yaml.dump(d, dataf)

            weights = initial_weights if initial_weights else "yolov5s.pt"

            subprocess.run(
                f"python train.py --img 640 --batch -1 --noplots --epochs {epochs} --cache ram "
                f"--data {data_dir}/data/data.yaml --weights {weights} --workers=0 --device=0 --optimizer=Adam",
                check=True,
                cwd="/yolov5",
                shell=True,
            )

            subprocess.run(
                f"python export.py --img 640 --include=onnx --int8 "
                f"--data {data_dir}/data/data.yaml --weights /yolov5/runs/train/exp/weights/best.pt --device=0 ",
                check=True,
                cwd="/yolov5",
                shell=True,
            )

            subprocess.run("find /yolov5 -print")

            target_path = os.path.basename(model)
            if not os.path.exists(target_path):
                os.makedirs(target_path)

            shutil.copyfile("/yolov5/runs/train/exp/weights/best.onnx", model)

        import argparse
        _parser = argparse.ArgumentParser(prog='Train model', description='')
        _parser.add_argument("--data-dir", dest="data_dir", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--initial-weights", dest="initial_weights", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--epochs", dest="epochs", type=int, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--model", dest="model", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = train_model(**_parsed_args)
      image: quay.io/ntlawrence/yolov5:pt1.12.1-yolo7.0-v1.1
      resources:
        limits: {nvidia.com/gpu: 1, memory: 30G}
    inputs:
      artifacts:
      - {name: load-data-pipeline_dataset_dir, path: /tmp/inputs/data_dir/data}
      - {name: initial_weights, path: /tmp/inputs/initial_weights/data}
    outputs:
      artifacts:
      - {name: train-model-model, path: /tmp/outputs/model/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--data-dir", {"inputPath": "data_dir"}, {"if": {"cond": {"isPresent":
          "initial_weights"}, "then": ["--initial-weights", {"inputPath": "initial_weights"}]}},
          {"if": {"cond": {"isPresent": "epochs"}, "then": ["--epochs", {"inputValue":
          "epochs"}]}}, "--model", {"outputPath": "model"}], "command": ["sh", "-ec",
          "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef train_model(\n    data_dir,\n    model,\n    initial_weights
          = None,\n    epochs = 500,\n):\n    import subprocess\n    import pathlib\n    from
          ruamel.yaml import YAML\n    import os\n    import shutil\n\n    yaml =
          YAML()\n    dataf = pathlib.Path(f\"{data_dir}/data/data.yaml\")\n    d
          = yaml.load(dataf)\n    d[\"train\"] = f\"{data_dir}/data/train\"\n    d[\"test\"]
          = f\"{data_dir}/data/test\"\n    d[\"val\"] = f\"{data_dir}/data/valid\"\n    yaml.dump(d,
          dataf)\n\n    weights = initial_weights if initial_weights else \"yolov5s.pt\"\n\n    subprocess.run(\n        f\"python
          train.py --img 640 --batch -1 --noplots --epochs {epochs} --cache ram \"\n        f\"--data
          {data_dir}/data/data.yaml --weights {weights} --workers=0 --device=0 --optimizer=Adam\",\n        check=True,\n        cwd=\"/yolov5\",\n        shell=True,\n    )\n\n    subprocess.run(\n        f\"python
          export.py --img 640 --include=onnx --int8 \"\n        f\"--data {data_dir}/data/data.yaml
          --weights /yolov5/runs/train/exp/weights/best.pt --device=0 \",\n        check=True,\n        cwd=\"/yolov5\",\n        shell=True,\n    )\n\n    subprocess.run(\"find
          /yolov5 -print\")\n\n    target_path = os.path.basename(model)\n    if not
          os.path.exists(target_path):\n        os.makedirs(target_path)\n\n    shutil.copyfile(\"/yolov5/runs/train/exp/weights/best.onnx\",
          model)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Train
          model'', description='''')\n_parser.add_argument(\"--data-dir\", dest=\"data_dir\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--initial-weights\",
          dest=\"initial_weights\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--epochs\",
          dest=\"epochs\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model\",
          dest=\"model\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = train_model(**_parsed_args)\n"],
          "image": "quay.io/ntlawrence/yolov5:pt1.12.1-yolo7.0-v1.1"}}, "inputs":
          [{"name": "data_dir", "type": "String"}, {"name": "initial_weights", "optional":
          true, "type": "String"}, {"default": "500", "name": "epochs", "optional":
          true, "type": "Integer"}], "name": "Train model", "outputs": [{"name": "model",
          "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"epochs":
          "1"}'}
  - name: upload-model
    container:
      args: [--model, /tmp/inputs/model/data, --minio-url, 'minio-service.kubeflow:9000',
        --minio-secret, mlpipeline-minio-artifact, --export-bucket, '{{workflow.namespace}}-bee-yolov5',
        --model-name, bee, --model-version, '1', --model-format, onnx, '----output-paths',
        /tmp/outputs/s3_address/data, /tmp/outputs/triton_s3_address/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def upload_model(
            model,
            model_config = None,
            minio_url = "minio-service.kubeflow:9000",
            minio_secret = "mlpipeline-minio-artifact",
            export_bucket = "models",
            model_name = "my-model",
            model_version = 1,
            model_format = "onnx",
        ):
            """Uploads a model file to MinIO artifact store."""

            from collections import namedtuple
            from kubernetes import client, config
            import logging
            from minio import Minio
            import sys

            logging.basicConfig(
                stream=sys.stdout,
                level=logging.INFO,
                format="%(levelname)s %(asctime)s: %(message)s",
            )
            logger = logging.getLogger()

            def get_minio_client(minio_secret):
                import base64
                from kubernetes.client.rest import ApiException

                def get_current_namespace():
                    SA_NAMESPACE = "/var/run/secrets/kubernetes.io/serviceaccount/namespace"
                    with open(SA_NAMESPACE) as f:
                        return f.read()

                def decode(text):
                    return base64.b64decode(text).decode("utf-8")

                config.load_incluster_config()
                api_client = client.ApiClient()

                try:
                    secret = client.CoreV1Api(api_client).read_namespaced_secret(
                        minio_secret, get_current_namespace()
                    )

                    minio_user = decode(secret.data["accesskey"])
                    minio_pass = decode(secret.data["secretkey"])

                    return Minio(
                        minio_url, access_key=minio_user, secret_key=minio_pass, secure=False
                    )
                except ApiException as e:
                    if e.status == 404:
                        logger.error(
                            "Failed to get secret 'mlpipeline-minio-artifact', which is needed for communicating with MinIO!"
                        )
                    raise Exception(e)

            logger.info(f"Establishing MinIO connection to '{minio_url}'...")
            minio_client = get_minio_client(minio_secret)

            # Create export bucket if it does not yet exist
            response = minio_client.list_buckets()
            export_bucket_exists = False
            for bucket in response:
                if bucket.name == export_bucket:
                    export_bucket_exists = True

            if not export_bucket_exists:
                logger.info(f"Creating bucket '{export_bucket}'...")
                minio_client.make_bucket(bucket_name=export_bucket)

            model_path = f"{model_name}/{model_version}/model.{model_format}"
            s3_address = f"s3://{minio_url}/{export_bucket}/{model_format}"
            triton_s3_address = f"{s3_address}/{model_path}"

            logger.info(f"Saving onnx file to MinIO (s3 address: {s3_address})...")
            minio_client.fput_object(
                bucket_name=export_bucket,  # bucket name in Minio
                object_name=f"{model_format}/{model_path}",  # file name in bucket of Minio / for Triton name MUST be model.onnx!
                file_path=model,  # file path / name in local system
            )

            if model_config:
                logger.info("Saving model config for triton to MinIO")
                # The config is above the version in the directory tree
                # https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_repository.md#repository-layout
                model_config_path = f"{model_name}/config.pbtxt"
                minio_client.fput_object(
                    bucket_name=export_bucket,  # bucket name in Minio
                    object_name=f"{model_format}/{model_config_path}",
                    file_path=model_config,  # file path / name in local system
                )

            logger.info("Finished.")
            out_tuple = namedtuple("UploadOutput", ["s3_address", "triton_s3_address"])
            return out_tuple(s3_address, triton_s3_address)

        def _serialize_str(str_value: str) -> str:
            if not isinstance(str_value, str):
                raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                    str(str_value), str(type(str_value))))
            return str_value

        import argparse
        _parser = argparse.ArgumentParser(prog='Upload model', description='Uploads a model file to MinIO artifact store.')
        _parser.add_argument("--model", dest="model", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--model-config", dest="model_config", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--minio-url", dest="minio_url", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--minio-secret", dest="minio_secret", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--export-bucket", dest="export_bucket", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--model-name", dest="model_name", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--model-version", dest="model_version", type=int, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--model-format", dest="model_format", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=2)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = upload_model(**_parsed_args)

        _output_serializers = [
            _serialize_str,
            _serialize_str,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      image: quay.io/ibm/kubeflow-notebook-image-ppc64le:elyra3.13.0-py3.8-tf2.9.2-pt1.12.1-v0
    inputs:
      artifacts:
      - {name: train-model-model, path: /tmp/inputs/model/data}
    outputs:
      artifacts:
      - {name: upload-model-s3_address, path: /tmp/outputs/s3_address/data}
      - {name: upload-model-triton_s3_address, path: /tmp/outputs/triton_s3_address/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Uploads
          a model file to MinIO artifact store.", "implementation": {"container":
          {"args": ["--model", {"inputPath": "model"}, {"if": {"cond": {"isPresent":
          "model_config"}, "then": ["--model-config", {"inputPath": "model_config"}]}},
          {"if": {"cond": {"isPresent": "minio_url"}, "then": ["--minio-url", {"inputValue":
          "minio_url"}]}}, {"if": {"cond": {"isPresent": "minio_secret"}, "then":
          ["--minio-secret", {"inputValue": "minio_secret"}]}}, {"if": {"cond": {"isPresent":
          "export_bucket"}, "then": ["--export-bucket", {"inputValue": "export_bucket"}]}},
          {"if": {"cond": {"isPresent": "model_name"}, "then": ["--model-name", {"inputValue":
          "model_name"}]}}, {"if": {"cond": {"isPresent": "model_version"}, "then":
          ["--model-version", {"inputValue": "model_version"}]}}, {"if": {"cond":
          {"isPresent": "model_format"}, "then": ["--model-format", {"inputValue":
          "model_format"}]}}, "----output-paths", {"outputPath": "s3_address"}, {"outputPath":
          "triton_s3_address"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def upload_model(\n    model,\n    model_config = None,\n    minio_url
          = \"minio-service.kubeflow:9000\",\n    minio_secret = \"mlpipeline-minio-artifact\",\n    export_bucket
          = \"models\",\n    model_name = \"my-model\",\n    model_version = 1,\n    model_format
          = \"onnx\",\n):\n    \"\"\"Uploads a model file to MinIO artifact store.\"\"\"\n\n    from
          collections import namedtuple\n    from kubernetes import client, config\n    import
          logging\n    from minio import Minio\n    import sys\n\n    logging.basicConfig(\n        stream=sys.stdout,\n        level=logging.INFO,\n        format=\"%(levelname)s
          %(asctime)s: %(message)s\",\n    )\n    logger = logging.getLogger()\n\n    def
          get_minio_client(minio_secret):\n        import base64\n        from kubernetes.client.rest
          import ApiException\n\n        def get_current_namespace():\n            SA_NAMESPACE
          = \"/var/run/secrets/kubernetes.io/serviceaccount/namespace\"\n            with
          open(SA_NAMESPACE) as f:\n                return f.read()\n\n        def
          decode(text):\n            return base64.b64decode(text).decode(\"utf-8\")\n\n        config.load_incluster_config()\n        api_client
          = client.ApiClient()\n\n        try:\n            secret = client.CoreV1Api(api_client).read_namespaced_secret(\n                minio_secret,
          get_current_namespace()\n            )\n\n            minio_user = decode(secret.data[\"accesskey\"])\n            minio_pass
          = decode(secret.data[\"secretkey\"])\n\n            return Minio(\n                minio_url,
          access_key=minio_user, secret_key=minio_pass, secure=False\n            )\n        except
          ApiException as e:\n            if e.status == 404:\n                logger.error(\n                    \"Failed
          to get secret ''mlpipeline-minio-artifact'', which is needed for communicating
          with MinIO!\"\n                )\n            raise Exception(e)\n\n    logger.info(f\"Establishing
          MinIO connection to ''{minio_url}''...\")\n    minio_client = get_minio_client(minio_secret)\n\n    #
          Create export bucket if it does not yet exist\n    response = minio_client.list_buckets()\n    export_bucket_exists
          = False\n    for bucket in response:\n        if bucket.name == export_bucket:\n            export_bucket_exists
          = True\n\n    if not export_bucket_exists:\n        logger.info(f\"Creating
          bucket ''{export_bucket}''...\")\n        minio_client.make_bucket(bucket_name=export_bucket)\n\n    model_path
          = f\"{model_name}/{model_version}/model.{model_format}\"\n    s3_address
          = f\"s3://{minio_url}/{export_bucket}/{model_format}\"\n    triton_s3_address
          = f\"{s3_address}/{model_path}\"\n\n    logger.info(f\"Saving onnx file
          to MinIO (s3 address: {s3_address})...\")\n    minio_client.fput_object(\n        bucket_name=export_bucket,  #
          bucket name in Minio\n        object_name=f\"{model_format}/{model_path}\",  #
          file name in bucket of Minio / for Triton name MUST be model.onnx!\n        file_path=model,  #
          file path / name in local system\n    )\n\n    if model_config:\n        logger.info(\"Saving
          model config for triton to MinIO\")\n        # The config is above the version
          in the directory tree\n        # https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_repository.md#repository-layout\n        model_config_path
          = f\"{model_name}/config.pbtxt\"\n        minio_client.fput_object(\n            bucket_name=export_bucket,  #
          bucket name in Minio\n            object_name=f\"{model_format}/{model_config_path}\",\n            file_path=model_config,  #
          file path / name in local system\n        )\n\n    logger.info(\"Finished.\")\n    out_tuple
          = namedtuple(\"UploadOutput\", [\"s3_address\", \"triton_s3_address\"])\n    return
          out_tuple(s3_address, triton_s3_address)\n\ndef _serialize_str(str_value:
          str) -> str:\n    if not isinstance(str_value, str):\n        raise TypeError(''Value
          \"{}\" has type \"{}\" instead of str.''.format(\n            str(str_value),
          str(type(str_value))))\n    return str_value\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Upload model'', description=''Uploads a
          model file to MinIO artifact store.'')\n_parser.add_argument(\"--model\",
          dest=\"model\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-config\",
          dest=\"model_config\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--minio-url\",
          dest=\"minio_url\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--minio-secret\",
          dest=\"minio_secret\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--export-bucket\",
          dest=\"export_bucket\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-name\",
          dest=\"model_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-version\",
          dest=\"model_version\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-format\",
          dest=\"model_format\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=2)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = upload_model(**_parsed_args)\n\n_output_serializers
          = [\n    _serialize_str,\n    _serialize_str,\n\n]\n\nimport os\nfor idx,
          output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "quay.io/ibm/kubeflow-notebook-image-ppc64le:elyra3.13.0-py3.8-tf2.9.2-pt1.12.1-v0"}},
          "inputs": [{"name": "model", "type": "String"}, {"name": "model_config",
          "optional": true, "type": "String"}, {"default": "minio-service.kubeflow:9000",
          "name": "minio_url", "optional": true, "type": "String"}, {"default": "mlpipeline-minio-artifact",
          "name": "minio_secret", "optional": true, "type": "String"}, {"default":
          "models", "name": "export_bucket", "optional": true, "type": "String"},
          {"default": "my-model", "name": "model_name", "optional": true, "type":
          "String"}, {"default": "1", "name": "model_version", "optional": true, "type":
          "Integer"}, {"default": "onnx", "name": "model_format", "optional": true,
          "type": "String"}], "name": "Upload model", "outputs": [{"name": "s3_address",
          "type": "String"}, {"name": "triton_s3_address", "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "6da225cd49bc4c67005599df5513903975e1aa6837d1a1e41c503d7b793253c6", "url":
          "/home/jovyan/components/model-building/upload-model/component.yaml"}',
        pipelines.kubeflow.org/arguments.parameters: '{"export_bucket": "{{workflow.namespace}}-bee-yolov5",
          "minio_secret": "mlpipeline-minio-artifact", "minio_url": "minio-service.kubeflow:9000",
          "model_format": "onnx", "model_name": "bee", "model_version": "1"}'}
  arguments:
    parameters:
    - {name: dataset_path}
    - {name: initial_weights}
    - {name: epochs, value: '300'}
    artifacts:
    - name: initial_weights
      raw: {data: '{{workflow.parameters.initial_weights}}'}
  serviceAccountName: pipeline-runner
