apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: end-to-end-monkey-species-classification-pipeline-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.18, pipelines.kubeflow.org/pipeline_compilation_time: '2023-02-06T19:44:19.571981',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "An example pipeline that
      performs an image classification and determines different monkey species", "inputs":
      [{"default": "Lehrig/Monkey-Species-Collection", "name": "dataset_url", "optional":
      true, "type": "String"}, {"default": "downsized", "name": "dataset_configuration",
      "optional": true, "type": "String"}, {"default": "[\"label\"]", "name": "dataset_label_columns",
      "optional": true, "type": "typing.List[str]"}, {"default": "monkey-classification",
      "name": "model_name", "optional": true, "type": "String"}, {"default": "", "name":
      "cluster_configuration_secret", "optional": true, "type": "String"}, {"default":
      "1", "name": "training_gpus", "optional": true, "type": "Integer"}, {"default":
      "2", "name": "number_of_workers", "optional": true, "type": "Integer"}, {"default":
      "", "name": "training_node_selector", "optional": true, "type": "String"}, {"default":
      "100", "name": "epochs", "optional": true, "type": "Integer"}, {"default": "minio-service.kubeflow:9000",
      "name": "minio_url", "optional": true}], "name": "End-to-end monkey species
      classification pipeline"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.18}
spec:
  entrypoint: end-to-end-monkey-species-classification-pipeline
  templates:
  - name: convert-model-to-onnx
    container:
      args: [--model-dir, /tmp/inputs/model_dir/data, --onnx-model-dir, /tmp/outputs/onnx_model_dir/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def convert_model_to_onnx(model_dir, onnx_model_dir):
            """Converts a model to ONNX format. Supported input formats: Keras."""

            import logging
            import onnx
            import sys
            import tensorflow as tf
            import tf2onnx

            logging.basicConfig(
                stream=sys.stdout,
                level=logging.INFO,
                format="%(levelname)s %(asctime)s: %(message)s",
            )
            logger = logging.getLogger()

            logger.info(f"Loading model from '{model_dir}'...")
            keras_model = tf.keras.models.load_model(model_dir)

            logger.info("Converting model to ONNX...")
            converted_model, _ = tf2onnx.convert.from_keras(keras_model)

            logger.info(f"Saving ONNX model to '{onnx_model_dir}'...")
            onnx.save_model(converted_model, onnx_model_dir)

            logger.info("Finished.")

        import argparse
        _parser = argparse.ArgumentParser(prog='Convert model to onnx', description='Converts a model to ONNX format. Supported input formats: Keras.')
        _parser.add_argument("--model-dir", dest="model_dir", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--onnx-model-dir", dest="onnx_model_dir", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = convert_model_to_onnx(**_parsed_args)
      image: quay.io/ibm/kubeflow-notebook-image-ppc64le:elyra3.14.1-py3.9-tf2.10.1-pt1.10.2-v0
      volumeMounts:
      - {mountPath: /tmp/inputs/model_dir, name: data-storage, subPath: '{{inputs.parameters.train-model-job-model_dir-subpath}}',
        readOnly: true}
      - {mountPath: /tmp/outputs/onnx_model_dir, name: data-storage, subPath: 'artefacts/{{workflow.uid}}_{{pod.name}}/convert-model-to-onnx-onnx_model_dir'}
    inputs:
      parameters:
      - {name: train-model-job-model_dir-subpath}
    outputs:
      parameters:
      - {name: convert-model-to-onnx-onnx_model_dir-subpath, value: 'artefacts/{{workflow.uid}}_{{pod.name}}/convert-model-to-onnx-onnx_model_dir'}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Converts
          a model to ONNX format. Supported input formats: Keras.", "implementation":
          {"container": {"args": ["--model-dir", {"inputPath": "model_dir"}, "--onnx-model-dir",
          {"outputPath": "onnx_model_dir"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef convert_model_to_onnx(model_dir,
          onnx_model_dir):\n    \"\"\"Converts a model to ONNX format. Supported input
          formats: Keras.\"\"\"\n\n    import logging\n    import onnx\n    import
          sys\n    import tensorflow as tf\n    import tf2onnx\n\n    logging.basicConfig(\n        stream=sys.stdout,\n        level=logging.INFO,\n        format=\"%(levelname)s
          %(asctime)s: %(message)s\",\n    )\n    logger = logging.getLogger()\n\n    logger.info(f\"Loading
          model from ''{model_dir}''...\")\n    keras_model = tf.keras.models.load_model(model_dir)\n\n    logger.info(\"Converting
          model to ONNX...\")\n    converted_model, _ = tf2onnx.convert.from_keras(keras_model)\n\n    logger.info(f\"Saving
          ONNX model to ''{onnx_model_dir}''...\")\n    onnx.save_model(converted_model,
          onnx_model_dir)\n\n    logger.info(\"Finished.\")\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Convert model to onnx'', description=''Converts
          a model to ONNX format. Supported input formats: Keras.'')\n_parser.add_argument(\"--model-dir\",
          dest=\"model_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--onnx-model-dir\",
          dest=\"onnx_model_dir\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = convert_model_to_onnx(**_parsed_args)\n"], "image": "quay.io/ibm/kubeflow-notebook-image-ppc64le:elyra3.14.1-py3.9-tf2.10.1-pt1.10.2-v0"}},
          "inputs": [{"name": "model_dir", "type": "String"}], "name": "Convert model
          to onnx", "outputs": [{"name": "onnx_model_dir", "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "c05e8c3ed1e38d36f48b1418ce4f68c28b9a0ccb67e69ab8c595253d8841979f", "url":
          "/home/jovyan/components/model-building/convert-to-onnx/component.yaml"}',
        pipelines.kubeflow.org/max_cache_staleness: P0D}
  - name: create-artefacts-blackboard
    resource:
      action: create
      setOwnerReference: true
      manifest: |
        apiVersion: v1
        kind: PersistentVolumeClaim
        metadata:
          name: '{{workflow.name}}-artefacts'
        spec:
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: 4Gi
    outputs:
      parameters:
      - name: create-artefacts-blackboard-manifest
        valueFrom: {jsonPath: '{}'}
      - name: create-artefacts-blackboard-name
        valueFrom: {jsonPath: '{.metadata.name}'}
      - name: create-artefacts-blackboard-size
        valueFrom: {jsonPath: '{.status.capacity.storage}'}
    metadata:
      annotations: {pipelines.kubeflow.org/max_cache_staleness: P0D}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: deploy-inference-service
    container:
      args:
      - --name
      - '{{inputs.parameters.model_name}}'
      - --storage-uri
      - s3://{{workflow.namespace}}-{{inputs.parameters.model_name}}/onnx
      - --minio-url
      - '{{inputs.parameters.minio_url}}'
      - --rm-existing
      - "True"
      - --minio-credential-secret
      - mlpipeline-minio-artifact
      - --predictor-gpu-allocation
      - '0'
      - --predictor-protocol
      - v2
      - --triton-runtime-version
      - 22.03-py3
      - --predictor-node-selector
      - ''
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def deploy_inference_service(\n    name,\n    storage_uri,\n    minio_url,\n\
        \    rm_existing = False,\n    minio_credential_secret=\"mlpipeline-minio-artifact\"\
        ,\n    concurrency_target = None,\n    predictor_min_replicas = None,\n  \
        \  predictor_max_replicas = None,\n    predictor_gpu_allocation = 0,\n   \
        \ predictor_protocol = \"v2\",  # or grpc-v2\n    triton_runtime_version =\
        \ \"22.03-py3\",\n    predictor_node_selector = \"\",  # Requires admin to\
        \ enable the capability\n    transformer_specification = None,\n):\n    import\
        \ os\n    import subprocess\n    import yaml\n    import base64\n\n    # https://kserve.github.io/website/modelserving/storage/s3/s3/#predict-on-a-inferenceservice-with-a-saved-model-on-s3\n\
        \    # https://kserve.github.io/website/reference/api/\n    # https://kserve.github.io/website/modelserving/autoscaling/autoscaling/\n\
        \    # https://kserve.github.io/website/modelserving/v1beta1/triton/torchscript/#inference-with-http-endpoint\n\
        \    # https://github.com/kserve/kserve/blob/master/docs/samples/multimodelserving/triton/README.md\n\
        \    # https://kserve.github.io/website/modelserving/v1beta1/triton/torchscript/#inference-with-grpc-endpoint\n\
        \n    # Caution: If using nodeSelector, this capability must be enabled for\
        \ knative by an admin\n    # https://knative.dev/docs/serving/configuration/feature-flags/#kubernetes-node-selector\n\
        \    # The default for our installs is FALSE\n    # once enabled, the value\
        \ should be 'label: \"value\"', to force the predictor/transformer to\n  \
        \  # run on specific hardware\n\n    # It happens that the credentials for\
        \ the minio user name and password are already in a secret\n    # This just\
        \ loads them so that we can create our own secret to store the S3 connection\
        \ information\n    # for the Inference service\n    r = subprocess.run(\n\
        \        [\"kubectl\", \"get\", \"secret\", minio_credential_secret, \"-oyaml\"\
        ],\n        stdout=subprocess.PIPE,\n        check=True,\n        text=True,\n\
        \    )\n    secret = yaml.safe_load(r.stdout)\n\n    s3_credentials_spec =\
        \ f\"\"\"\n    apiVersion: v1\n    kind: Secret\n    metadata:\n      name:\
        \ minio-credentials\n      annotations:\n        serving.kserve.io/s3-endpoint:\
        \ {minio_url} \n        serving.kserve.io/s3-usehttps: \"0\"\n        serving.kserve.io/s3-region:\
        \ \"us-west1\"\n        serving.kserve.io/s3-useanoncredential: \"false\"\n\
        \    type: Opaque\n    data:\n      AWS_ACCESS_KEY_ID: {secret['data']['accesskey']}\n\
        \      AWS_SECRET_ACCESS_KEY: {secret['data']['secretkey']}\n    \"\"\"\n\n\
        \    print(s3_credentials_spec)\n    subprocess.run(\n        [\"kubectl\"\
        , \"apply\", \"-f\", \"-\"],\n        input=s3_credentials_spec,\n       \
        \ check=True,\n        text=True,\n    )\n\n    sa_spec = \"\"\"\n    apiVersion:\
        \ v1\n    kind: ServiceAccount\n    metadata:\n      name: kserve-inference-sa\n\
        \    secrets:\n    - name: minio-credentials\n    \"\"\"\n\n    print(sa_spec)\n\
        \    subprocess.run(\n        [\"kubectl\", \"apply\", \"-f\", \"-\"], input=sa_spec,\
        \ check=True, text=True\n    )\n\n    ### Remove Existing Inferenceservice,\
        \ if requested\n    ### Ignores errrors if service does not already exist\n\
        \    if rm_existing:\n        subprocess.run([\"kubectl\", \"delete\", \"\
        inferenceservice\", name], check=False)\n\n    ####### Transformer Spec ######\n\
        \    if transformer_specification:\n        min_t_replicas = (\n         \
        \   (\"minReplicas: \" + transformer_specification[\"min_replicas\"])\n  \
        \          if \"min_replicas\" in transformer_specification\n            else\
        \ \"\"\n        )\n        max_t_replicas = (\n            (\"maxReplicas:\
        \ \" + transformer_specification[\"max_replicas\"])\n            if \"min_replicas\"\
        \ in transformer_specification\n            else \"\"\n        )\n\n     \
        \   # EnvFrom allows all vars to be read from a config map\n        # If a\
        \ variable is defined by both env and envFrom,\n        # env takes precedance.\
        \ If a variable is defined twice\n        # in env from, then the last one\
        \ wins.\n        if \"env_configmap\" in transformer_specification:\n    \
        \        envFrom = f\"\"\"\n          envFrom:\n            - configMapRef:\n\
        \                name: {transformer_specification[\"env_configmap\"]}\n  \
        \        \"\"\"\n        else:\n            envFrom = \"\"\n\n        # Node\
        \ selector\n        # https://kserve.github.io/website/0.9/modelserving/nodescheduling/inferenceservicenodescheduling/#usage\n\
        \        if \"node_selector\" in transformer_specification:\n            t_node_selector\
        \ = (\n                f'nodeSelector:\\n          {transformer_specification[\"\
        node_selector\"]}'\n            )\n        else:\n            t_node_selector\
        \ = \"\"\n\n        #### Transformer specification ####\n        transform_spec\
        \ = f\"\"\"\n      transformer:\n        {min_t_replicas}\n        {max_t_replicas}\n\
        \        serviceAccountName: kserve-inference-sa\n        {t_node_selector}\n\
        \        containers:\n        - image: \"{transformer_specification[\"image\"\
        ]}\"\n          name: \"{name}-transformer\"\n          command: {transformer_specification.get(\"\
        command\", '[\"python\", \"transform.py\"]')}\n          args: [\"--protocol={predictor_protocol}\"\
        ]\n          env:\n            - name: STORAGE_URI\n              value: {storage_uri}\n\
        \          {envFrom}\n          \"\"\"\n    else:\n        transform_spec\
        \ = \"\"\n\n    gpu_resources = (\n        f\"nvidia.com/gpu: {predictor_gpu_allocation}\"\
        \n        if predictor_gpu_allocation\n        else \"\"\n    )\n\n    min_p_replicas\
        \ = (\n        f\"minReplicas: {predictor_min_replicas}\"\n        if predictor_min_replicas\
        \ is not None\n        else \"\"\n    )\n    max_p_replicas = (\n        f\"\
        maxReplicas: {predictor_max_replicas}\"\n        if predictor_max_replicas\
        \ is not None\n        else \"\"\n    )\n\n    predictor_port_spec = (\n \
        \       '[{\"containerPort\": 9000, \"name\": \"h2c\", \"protocol\": \"TCP\"\
        }]'\n        if predictor_protocol == \"grpc-v2\"\n        else \"\"\n   \
        \ )\n\n    if concurrency_target:\n        autoscaling_target = f\"\"\"\n\
        \        autoscaling.knative.dev/target: \"{concurrency_target}\"\n      \
        \  autoscaling.knative.dev/metric: \"concurrency\"\n        \"\"\"\n    else:\n\
        \        autoscaling_target = \"\"\n\n    # Node selector\n    # https://kserve.github.io/website/0.9/modelserving/nodescheduling/inferenceservicenodescheduling/#usage\n\
        \    if predictor_node_selector:\n        p_node_selector = f\"nodeSelector:\\\
        n          {predictor_node_selector}\"\n    else:\n        p_node_selector\
        \ = \"\"\n\n    ##### Inference Service Spec ####\n    service_spec = f\"\"\
        \"\n    apiVersion: serving.kserve.io/v1beta1\n    kind: InferenceService\n\
        \    metadata:\n      name: {name}\n      annotations:\n        sidecar.istio.io/inject:\
        \ \"false\"\n        # https://knative.dev/docs/serving/autoscaling/concurrency/#soft-limit\n\
        \        {autoscaling_target}\n    spec:\n      {transform_spec}\n      predictor:\n\
        \        {min_p_replicas}\n        {max_p_replicas}\n        serviceAccountName:\
        \ kserve-inference-sa\n        {p_node_selector}\n        triton:\n      \
        \    runtimeVersion: {triton_runtime_version}\n          args: [ \"--strict-model-config=false\"\
        ]\n          storageUri: {storage_uri}\n          ports: {predictor_port_spec}\n\
        \          env:\n          - name: OMP_NUM_THREADS\n            value: \"\
        1\"\n          resources:\n            limits:\n               {gpu_resources}\n\
        \    \"\"\"\n\n    print(service_spec)\n    subprocess.run(\n        [\"kubectl\"\
        , \"apply\", \"-f\", \"-\"], input=service_spec, check=True, text=True\n \
        \   )\n\n    print(\"Waiting for inference service to become available\")\n\
        \    subprocess.run(\n        [\n            \"kubectl\",\n            \"\
        wait\",\n            \"--for=condition=Ready\",\n            f\"inferenceservice/{name}\"\
        ,\n            \"--timeout=600s\",\n        ],\n        check=True,\n    )\n\
        \ndef _deserialize_bool(s) -> bool:\n    from distutils.util import strtobool\n\
        \    return strtobool(s) == 1\n\nimport json\nimport argparse\n_parser = argparse.ArgumentParser(prog='Deploy\
        \ inference service', description='')\n_parser.add_argument(\"--name\", dest=\"\
        name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --storage-uri\", dest=\"storage_uri\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--minio-url\", dest=\"minio_url\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--rm-existing\", dest=\"\
        rm_existing\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--minio-credential-secret\", dest=\"minio_credential_secret\"\
        , type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --concurrency-target\", dest=\"concurrency_target\", type=int, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictor-min-replicas\"\
        , dest=\"predictor_min_replicas\", type=int, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--predictor-max-replicas\", dest=\"predictor_max_replicas\"\
        , type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --predictor-gpu-allocation\", dest=\"predictor_gpu_allocation\", type=int,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictor-protocol\"\
        , dest=\"predictor_protocol\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--triton-runtime-version\", dest=\"triton_runtime_version\"\
        , type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --predictor-node-selector\", dest=\"predictor_node_selector\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--transformer-specification\"\
        , dest=\"transformer_specification\", type=json.loads, required=False, default=argparse.SUPPRESS)\n\
        _parsed_args = vars(_parser.parse_args())\n\n_outputs = deploy_inference_service(**_parsed_args)\n"
      image: quay.io/ibm/kubeflow-notebook-image-ppc64le:elyra3.13.0-py3.8-tf2.9.2-pt1.12.1-v0
    inputs:
      parameters:
      - {name: minio_url}
      - {name: model_name}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--name", {"inputValue": "name"}, "--storage-uri", {"inputValue":
          "storage_uri"}, "--minio-url", {"inputValue": "minio_url"}, {"if": {"cond":
          {"isPresent": "rm_existing"}, "then": ["--rm-existing", {"inputValue": "rm_existing"}]}},
          {"if": {"cond": {"isPresent": "minio_credential_secret"}, "then": ["--minio-credential-secret",
          {"inputValue": "minio_credential_secret"}]}}, {"if": {"cond": {"isPresent":
          "concurrency_target"}, "then": ["--concurrency-target", {"inputValue": "concurrency_target"}]}},
          {"if": {"cond": {"isPresent": "predictor_min_replicas"}, "then": ["--predictor-min-replicas",
          {"inputValue": "predictor_min_replicas"}]}}, {"if": {"cond": {"isPresent":
          "predictor_max_replicas"}, "then": ["--predictor-max-replicas", {"inputValue":
          "predictor_max_replicas"}]}}, {"if": {"cond": {"isPresent": "predictor_gpu_allocation"},
          "then": ["--predictor-gpu-allocation", {"inputValue": "predictor_gpu_allocation"}]}},
          {"if": {"cond": {"isPresent": "predictor_protocol"}, "then": ["--predictor-protocol",
          {"inputValue": "predictor_protocol"}]}}, {"if": {"cond": {"isPresent": "triton_runtime_version"},
          "then": ["--triton-runtime-version", {"inputValue": "triton_runtime_version"}]}},
          {"if": {"cond": {"isPresent": "predictor_node_selector"}, "then": ["--predictor-node-selector",
          {"inputValue": "predictor_node_selector"}]}}, {"if": {"cond": {"isPresent":
          "transformer_specification"}, "then": ["--transformer-specification", {"inputValue":
          "transformer_specification"}]}}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def deploy_inference_service(\n    name,\n    storage_uri,\n    minio_url,\n    rm_existing
          = False,\n    minio_credential_secret=\"mlpipeline-minio-artifact\",\n    concurrency_target
          = None,\n    predictor_min_replicas = None,\n    predictor_max_replicas
          = None,\n    predictor_gpu_allocation = 0,\n    predictor_protocol = \"v2\",  #
          or grpc-v2\n    triton_runtime_version = \"22.03-py3\",\n    predictor_node_selector
          = \"\",  # Requires admin to enable the capability\n    transformer_specification
          = None,\n):\n    import os\n    import subprocess\n    import yaml\n    import
          base64\n\n    # https://kserve.github.io/website/modelserving/storage/s3/s3/#predict-on-a-inferenceservice-with-a-saved-model-on-s3\n    #
          https://kserve.github.io/website/reference/api/\n    # https://kserve.github.io/website/modelserving/autoscaling/autoscaling/\n    #
          https://kserve.github.io/website/modelserving/v1beta1/triton/torchscript/#inference-with-http-endpoint\n    #
          https://github.com/kserve/kserve/blob/master/docs/samples/multimodelserving/triton/README.md\n    #
          https://kserve.github.io/website/modelserving/v1beta1/triton/torchscript/#inference-with-grpc-endpoint\n\n    #
          Caution: If using nodeSelector, this capability must be enabled for knative
          by an admin\n    # https://knative.dev/docs/serving/configuration/feature-flags/#kubernetes-node-selector\n    #
          The default for our installs is FALSE\n    # once enabled, the value should
          be ''label: \"value\"'', to force the predictor/transformer to\n    # run
          on specific hardware\n\n    # It happens that the credentials for the minio
          user name and password are already in a secret\n    # This just loads them
          so that we can create our own secret to store the S3 connection information\n    #
          for the Inference service\n    r = subprocess.run(\n        [\"kubectl\",
          \"get\", \"secret\", minio_credential_secret, \"-oyaml\"],\n        stdout=subprocess.PIPE,\n        check=True,\n        text=True,\n    )\n    secret
          = yaml.safe_load(r.stdout)\n\n    s3_credentials_spec = f\"\"\"\n    apiVersion:
          v1\n    kind: Secret\n    metadata:\n      name: minio-credentials\n      annotations:\n        serving.kserve.io/s3-endpoint:
          {minio_url} \n        serving.kserve.io/s3-usehttps: \"0\"\n        serving.kserve.io/s3-region:
          \"us-west1\"\n        serving.kserve.io/s3-useanoncredential: \"false\"\n    type:
          Opaque\n    data:\n      AWS_ACCESS_KEY_ID: {secret[''data''][''accesskey'']}\n      AWS_SECRET_ACCESS_KEY:
          {secret[''data''][''secretkey'']}\n    \"\"\"\n\n    print(s3_credentials_spec)\n    subprocess.run(\n        [\"kubectl\",
          \"apply\", \"-f\", \"-\"],\n        input=s3_credentials_spec,\n        check=True,\n        text=True,\n    )\n\n    sa_spec
          = \"\"\"\n    apiVersion: v1\n    kind: ServiceAccount\n    metadata:\n      name:
          kserve-inference-sa\n    secrets:\n    - name: minio-credentials\n    \"\"\"\n\n    print(sa_spec)\n    subprocess.run(\n        [\"kubectl\",
          \"apply\", \"-f\", \"-\"], input=sa_spec, check=True, text=True\n    )\n\n    ###
          Remove Existing Inferenceservice, if requested\n    ### Ignores errrors
          if service does not already exist\n    if rm_existing:\n        subprocess.run([\"kubectl\",
          \"delete\", \"inferenceservice\", name], check=False)\n\n    ####### Transformer
          Spec ######\n    if transformer_specification:\n        min_t_replicas =
          (\n            (\"minReplicas: \" + transformer_specification[\"min_replicas\"])\n            if
          \"min_replicas\" in transformer_specification\n            else \"\"\n        )\n        max_t_replicas
          = (\n            (\"maxReplicas: \" + transformer_specification[\"max_replicas\"])\n            if
          \"min_replicas\" in transformer_specification\n            else \"\"\n        )\n\n        #
          EnvFrom allows all vars to be read from a config map\n        # If a variable
          is defined by both env and envFrom,\n        # env takes precedance. If
          a variable is defined twice\n        # in env from, then the last one wins.\n        if
          \"env_configmap\" in transformer_specification:\n            envFrom = f\"\"\"\n          envFrom:\n            -
          configMapRef:\n                name: {transformer_specification[\"env_configmap\"]}\n          \"\"\"\n        else:\n            envFrom
          = \"\"\n\n        # Node selector\n        # https://kserve.github.io/website/0.9/modelserving/nodescheduling/inferenceservicenodescheduling/#usage\n        if
          \"node_selector\" in transformer_specification:\n            t_node_selector
          = (\n                f''nodeSelector:\\n          {transformer_specification[\"node_selector\"]}''\n            )\n        else:\n            t_node_selector
          = \"\"\n\n        #### Transformer specification ####\n        transform_spec
          = f\"\"\"\n      transformer:\n        {min_t_replicas}\n        {max_t_replicas}\n        serviceAccountName:
          kserve-inference-sa\n        {t_node_selector}\n        containers:\n        -
          image: \"{transformer_specification[\"image\"]}\"\n          name: \"{name}-transformer\"\n          command:
          {transformer_specification.get(\"command\", ''[\"python\", \"transform.py\"]'')}\n          args:
          [\"--protocol={predictor_protocol}\"]\n          env:\n            - name:
          STORAGE_URI\n              value: {storage_uri}\n          {envFrom}\n          \"\"\"\n    else:\n        transform_spec
          = \"\"\n\n    gpu_resources = (\n        f\"nvidia.com/gpu: {predictor_gpu_allocation}\"\n        if
          predictor_gpu_allocation\n        else \"\"\n    )\n\n    min_p_replicas
          = (\n        f\"minReplicas: {predictor_min_replicas}\"\n        if predictor_min_replicas
          is not None\n        else \"\"\n    )\n    max_p_replicas = (\n        f\"maxReplicas:
          {predictor_max_replicas}\"\n        if predictor_max_replicas is not None\n        else
          \"\"\n    )\n\n    predictor_port_spec = (\n        ''[{\"containerPort\":
          9000, \"name\": \"h2c\", \"protocol\": \"TCP\"}]''\n        if predictor_protocol
          == \"grpc-v2\"\n        else \"\"\n    )\n\n    if concurrency_target:\n        autoscaling_target
          = f\"\"\"\n        autoscaling.knative.dev/target: \"{concurrency_target}\"\n        autoscaling.knative.dev/metric:
          \"concurrency\"\n        \"\"\"\n    else:\n        autoscaling_target =
          \"\"\n\n    # Node selector\n    # https://kserve.github.io/website/0.9/modelserving/nodescheduling/inferenceservicenodescheduling/#usage\n    if
          predictor_node_selector:\n        p_node_selector = f\"nodeSelector:\\n          {predictor_node_selector}\"\n    else:\n        p_node_selector
          = \"\"\n\n    ##### Inference Service Spec ####\n    service_spec = f\"\"\"\n    apiVersion:
          serving.kserve.io/v1beta1\n    kind: InferenceService\n    metadata:\n      name:
          {name}\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n        #
          https://knative.dev/docs/serving/autoscaling/concurrency/#soft-limit\n        {autoscaling_target}\n    spec:\n      {transform_spec}\n      predictor:\n        {min_p_replicas}\n        {max_p_replicas}\n        serviceAccountName:
          kserve-inference-sa\n        {p_node_selector}\n        triton:\n          runtimeVersion:
          {triton_runtime_version}\n          args: [ \"--strict-model-config=false\"]\n          storageUri:
          {storage_uri}\n          ports: {predictor_port_spec}\n          env:\n          -
          name: OMP_NUM_THREADS\n            value: \"1\"\n          resources:\n            limits:\n               {gpu_resources}\n    \"\"\"\n\n    print(service_spec)\n    subprocess.run(\n        [\"kubectl\",
          \"apply\", \"-f\", \"-\"], input=service_spec, check=True, text=True\n    )\n\n    print(\"Waiting
          for inference service to become available\")\n    subprocess.run(\n        [\n            \"kubectl\",\n            \"wait\",\n            \"--for=condition=Ready\",\n            f\"inferenceservice/{name}\",\n            \"--timeout=600s\",\n        ],\n        check=True,\n    )\n\ndef
          _deserialize_bool(s) -> bool:\n    from distutils.util import strtobool\n    return
          strtobool(s) == 1\n\nimport json\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Deploy
          inference service'', description='''')\n_parser.add_argument(\"--name\",
          dest=\"name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--storage-uri\",
          dest=\"storage_uri\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--minio-url\",
          dest=\"minio_url\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--rm-existing\",
          dest=\"rm_existing\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--minio-credential-secret\",
          dest=\"minio_credential_secret\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--concurrency-target\",
          dest=\"concurrency_target\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictor-min-replicas\",
          dest=\"predictor_min_replicas\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictor-max-replicas\",
          dest=\"predictor_max_replicas\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictor-gpu-allocation\",
          dest=\"predictor_gpu_allocation\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictor-protocol\",
          dest=\"predictor_protocol\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--triton-runtime-version\",
          dest=\"triton_runtime_version\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictor-node-selector\",
          dest=\"predictor_node_selector\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--transformer-specification\",
          dest=\"transformer_specification\", type=json.loads, required=False, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = deploy_inference_service(**_parsed_args)\n"],
          "image": "quay.io/ibm/kubeflow-notebook-image-ppc64le:elyra3.13.0-py3.8-tf2.9.2-pt1.12.1-v0"}},
          "inputs": [{"name": "name", "type": "String"}, {"name": "storage_uri", "type":
          "String"}, {"name": "minio_url", "type": "String"}, {"default": "False",
          "name": "rm_existing", "optional": true, "type": "Boolean"}, {"default":
          "mlpipeline-minio-artifact", "name": "minio_credential_secret", "optional":
          true}, {"name": "concurrency_target", "optional": true, "type": "Integer"},
          {"name": "predictor_min_replicas", "optional": true, "type": "Integer"},
          {"name": "predictor_max_replicas", "optional": true, "type": "Integer"},
          {"default": "0", "name": "predictor_gpu_allocation", "optional": true, "type":
          "Integer"}, {"default": "v2", "name": "predictor_protocol", "optional":
          true, "type": "String"}, {"default": "22.03-py3", "name": "triton_runtime_version",
          "optional": true, "type": "String"}, {"default": "", "name": "predictor_node_selector",
          "optional": true, "type": "String"}, {"name": "transformer_specification",
          "optional": true, "type": "typing.Dict[str, typing.Union[str, int]]"}],
          "name": "Deploy inference service"}', pipelines.kubeflow.org/component_ref: '{"digest":
          "606d302117e348d3844087381c423bd6f531e7ba6f4cb24569c1de0e935ee137", "url":
          "/home/jovyan/kubeflow-ppc64le-examples/deploy_triton_inference_service_component/deploy_triton_inference_service_component.yaml"}',
        pipelines.kubeflow.org/arguments.parameters: '{"minio_credential_secret":
          "mlpipeline-minio-artifact", "minio_url": "{{inputs.parameters.minio_url}}",
          "name": "{{inputs.parameters.model_name}}", "predictor_gpu_allocation":
          "0", "predictor_node_selector": "", "predictor_protocol": "v2", "rm_existing":
          "True", "storage_uri": "s3://{{workflow.namespace}}-{{inputs.parameters.model_name}}/onnx",
          "triton_runtime_version": "22.03-py3"}', pipelines.kubeflow.org/max_cache_staleness: P0D}
  - name: end-to-end-monkey-species-classification-pipeline
    inputs:
      parameters:
      - {name: cluster_configuration_secret}
      - {name: dataset_configuration}
      - {name: dataset_label_columns}
      - {name: dataset_url}
      - {name: epochs}
      - {name: minio_url}
      - {name: model_name}
      - {name: number_of_workers}
      - {name: training_gpus}
      - {name: training_node_selector}
    dag:
      tasks:
      - name: convert-model-to-onnx
        template: convert-model-to-onnx
        dependencies: [train-model-job]
        arguments:
          parameters:
          - {name: train-model-job-model_dir-subpath, value: '{{tasks.train-model-job.outputs.parameters.train-model-job-model_dir-subpath}}'}
      - {name: create-artefacts-blackboard, template: create-artefacts-blackboard}
      - name: deploy-inference-service
        template: deploy-inference-service
        dependencies: [upload-model]
        arguments:
          parameters:
          - {name: minio_url, value: '{{inputs.parameters.minio_url}}'}
          - {name: model_name, value: '{{inputs.parameters.model_name}}'}
      - name: evaluate-model
        template: evaluate-model
        dependencies: [preprocess-dataset, train-model-job]
        arguments:
          parameters:
          - {name: preprocess-dataset-test_dataset_dir-subpath, value: '{{tasks.preprocess-dataset.outputs.parameters.preprocess-dataset-test_dataset_dir-subpath}}'}
          - {name: train-model-job-model_dir-subpath, value: '{{tasks.train-model-job.outputs.parameters.train-model-job-model_dir-subpath}}'}
      - name: load-dataset
        template: load-dataset
        dependencies: [create-artefacts-blackboard]
        arguments:
          parameters:
          - {name: dataset_configuration, value: '{{inputs.parameters.dataset_configuration}}'}
          - {name: dataset_label_columns, value: '{{inputs.parameters.dataset_label_columns}}'}
          - {name: dataset_url, value: '{{inputs.parameters.dataset_url}}'}
      - name: plot-confusion-matrix
        template: plot-confusion-matrix
        dependencies: [load-dataset, preprocess-dataset, train-model-job]
        arguments:
          parameters:
          - {name: load-dataset-labels, value: '{{tasks.load-dataset.outputs.parameters.load-dataset-labels}}'}
          - {name: preprocess-dataset-test_dataset_dir-subpath, value: '{{tasks.preprocess-dataset.outputs.parameters.preprocess-dataset-test_dataset_dir-subpath}}'}
          - {name: train-model-job-model_dir-subpath, value: '{{tasks.train-model-job.outputs.parameters.train-model-job-model_dir-subpath}}'}
      - name: preprocess-dataset
        template: preprocess-dataset
        dependencies: [load-dataset]
        arguments:
          parameters:
          - {name: load-dataset-dataset_dir-subpath, value: '{{tasks.load-dataset.outputs.parameters.load-dataset-dataset_dir-subpath}}'}
      - name: train-model-job
        template: train-model-job
        dependencies: [preprocess-dataset]
        arguments:
          parameters:
          - {name: cluster_configuration_secret, value: '{{inputs.parameters.cluster_configuration_secret}}'}
          - {name: epochs, value: '{{inputs.parameters.epochs}}'}
          - {name: model_name, value: '{{inputs.parameters.model_name}}'}
          - {name: number_of_workers, value: '{{inputs.parameters.number_of_workers}}'}
          - {name: training_gpus, value: '{{inputs.parameters.training_gpus}}'}
          - {name: training_node_selector, value: '{{inputs.parameters.training_node_selector}}'}
          - {name: preprocess-dataset-train_dataset_dir-subpath, value: '{{tasks.preprocess-dataset.outputs.parameters.preprocess-dataset-train_dataset_dir-subpath}}'}
          - {name: preprocess-dataset-validation_dataset_dir-subpath, value: '{{tasks.preprocess-dataset.outputs.parameters.preprocess-dataset-validation_dataset_dir-subpath}}'}
      - name: upload-model
        template: upload-model
        dependencies: [convert-model-to-onnx]
        arguments:
          parameters:
          - {name: minio_url, value: '{{inputs.parameters.minio_url}}'}
          - {name: model_name, value: '{{inputs.parameters.model_name}}'}
          - {name: convert-model-to-onnx-onnx_model_dir-subpath, value: '{{tasks.convert-model-to-onnx.outputs.parameters.convert-model-to-onnx-onnx_model_dir-subpath}}'}
  - name: evaluate-model
    container:
      args: [--test-dataset-dir, /tmp/inputs/test_dataset_dir/data, --model-dir, /tmp/inputs/model_dir/data,
        --batch-size, '20', --mlpipeline-ui-metadata, /tmp/outputs/mlpipeline_ui_metadata/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def evaluate_model(
            test_dataset_dir,
            mlpipeline_ui_metadata_path,
            model_dir,
            batch_size = 20,
        ):
            import json
            import tensorflow as tf
            import os

            test_dataset = tf.data.experimental.load(test_dataset_dir)
            model = tf.keras.models.load_model(model_dir)
            (loss, accuracy) = model.evaluate(test_dataset)

            print((loss, accuracy))

            metadata = {
                "outputs": [
                    {
                        "type": "table",
                        "storage": "inline",
                        "format": "csv",
                        "header": ["Loss", "Accuracy"],
                        "source": f"{loss},{accuracy}",
                    }
                ]
            }

            with open(mlpipeline_ui_metadata_path, "w") as f:
                json.dump(metadata, f)

        import argparse
        _parser = argparse.ArgumentParser(prog='Evaluate model', description='')
        _parser.add_argument("--test-dataset-dir", dest="test_dataset_dir", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--model-dir", dest="model_dir", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--batch-size", dest="batch_size", type=int, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--mlpipeline-ui-metadata", dest="mlpipeline_ui_metadata_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = evaluate_model(**_parsed_args)
      image: quay.io/ibm/kubeflow-notebook-image-ppc64le:elyra3.14.1-py3.9-tf2.10.1-pt1.10.2-v0
      volumeMounts:
      - {mountPath: /tmp/inputs/model_dir, name: data-storage, subPath: '{{inputs.parameters.train-model-job-model_dir-subpath}}',
        readOnly: true}
      - {mountPath: /tmp/inputs/test_dataset_dir, name: data-storage, subPath: '{{inputs.parameters.preprocess-dataset-test_dataset_dir-subpath}}',
        readOnly: true}
      - {mountPath: /tmp/outputs/mlpipeline_ui_metadata, name: data-storage, subPath: 'artefacts/{{workflow.uid}}_{{pod.name}}/mlpipeline-ui-metadata'}
    inputs:
      parameters:
      - {name: train-model-job-model_dir-subpath}
      - {name: preprocess-dataset-test_dataset_dir-subpath}
    outputs:
      parameters:
      - {name: mlpipeline-ui-metadata-subpath, value: 'artefacts/{{workflow.uid}}_{{pod.name}}/mlpipeline-ui-metadata'}
      artifacts:
      - {name: mlpipeline-ui-metadata, path: /tmp/outputs/mlpipeline_ui_metadata/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--test-dataset-dir", {"inputPath": "test_dataset_dir"}, "--model-dir",
          {"inputPath": "model_dir"}, {"if": {"cond": {"isPresent": "batch_size"},
          "then": ["--batch-size", {"inputValue": "batch_size"}]}}, "--mlpipeline-ui-metadata",
          {"outputPath": "mlpipeline_ui_metadata"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef evaluate_model(\n    test_dataset_dir,\n    mlpipeline_ui_metadata_path,\n    model_dir,\n    batch_size
          = 20,\n):\n    import json\n    import tensorflow as tf\n    import os\n\n    test_dataset
          = tf.data.experimental.load(test_dataset_dir)\n    model = tf.keras.models.load_model(model_dir)\n    (loss,
          accuracy) = model.evaluate(test_dataset)\n\n    print((loss, accuracy))\n\n    metadata
          = {\n        \"outputs\": [\n            {\n                \"type\": \"table\",\n                \"storage\":
          \"inline\",\n                \"format\": \"csv\",\n                \"header\":
          [\"Loss\", \"Accuracy\"],\n                \"source\": f\"{loss},{accuracy}\",\n            }\n        ]\n    }\n\n    with
          open(mlpipeline_ui_metadata_path, \"w\") as f:\n        json.dump(metadata,
          f)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Evaluate
          model'', description='''')\n_parser.add_argument(\"--test-dataset-dir\",
          dest=\"test_dataset_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-dir\",
          dest=\"model_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--batch-size\",
          dest=\"batch_size\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlpipeline-ui-metadata\",
          dest=\"mlpipeline_ui_metadata_path\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = evaluate_model(**_parsed_args)\n"], "image": "quay.io/ibm/kubeflow-notebook-image-ppc64le:elyra3.14.1-py3.9-tf2.10.1-pt1.10.2-v0"}},
          "inputs": [{"name": "test_dataset_dir", "type": "String"}, {"name": "model_dir",
          "type": "String"}, {"default": "20", "name": "batch_size", "optional": true,
          "type": "Integer"}], "name": "Evaluate model", "outputs": [{"name": "mlpipeline_ui_metadata",
          "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"batch_size":
          "20"}', pipelines.kubeflow.org/max_cache_staleness: P0D}
  - name: load-dataset
    container:
      args: [--path, '{{inputs.parameters.dataset_url}}', --configuration, '{{inputs.parameters.dataset_configuration}}',
        --label-columns, '{{inputs.parameters.dataset_label_columns}}', --dataset-dir,
        /tmp/outputs/dataset_dir/data, '----output-paths', /tmp/outputs/labels/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def load_dataset(
            path,
            dataset_dir,
            configuration = "",
            label_columns = None,
        ):
            """
            Load a Huggingface Dataset.

                    Parameters:
                            path: Path from which to load the dataset. Huggingfaces hub for datasets is supported. Example: "Lehrig/Monkey-Species-Collection".
                            dataset_dir: Target directory where the dataset will be loaded to. Should be available as a mount from a PVC. Example: "/blackboard/dataset".
                            configuration: Name of the dataset configuration to load. Example: "downsized".
                            label_columns: Optional list of label column names to be fetched as optional, additional output. Example: ["label"].
                    Returns:
                            labels: Dictionary mapping label columns to associated labels, if available. Empty dictionary otherwise. Example: {"labels": ["cat", "dog"]}
            """

            from collections import namedtuple
            from datasets import load_dataset
            from datasets.dataset_dict import DatasetDict
            import logging
            import os
            from PIL.Image import Image
            import sys

            logging.basicConfig(
                stream=sys.stdout,
                level=logging.INFO,
                format="%(levelname)s %(asctime)s: %(message)s",
            )

            if not configuration:
                configuration = None
            logging.info(
                f"Loading dataset from '{path}' using configuration '{configuration}'..."
            )
            dataset = load_dataset(path, configuration)

            logging.info("Reading image files into bytes...")

            # see: https://huggingface.co/docs/datasets/v2.4.0/en/package_reference/main_classes#datasets.Dataset.save_to_disk
            def read_image_file(example):
                for column in example:
                    if isinstance(example[column], Image):
                        with open(example[column].filename, "rb") as f:
                            example[column] = {"bytes": f.read()}
                return example

            # note: batching in map caused caching issues, so not using it for now
            dataset = dataset.map(read_image_file)

            logging.info(f"Saving dataset to '{dataset_dir}'...")
            if not os.path.exists(dataset_dir):
                os.makedirs(dataset_dir)
            dataset.save_to_disk(dataset_dir)

            logging.info(f"Dataset saved. Contents of '{dataset_dir}':")
            logging.info(os.listdir(dataset_dir))

            labels = dict()
            if isinstance(dataset, DatasetDict):
                dataset = next(iter(dataset.values()))
                for label_column in label_columns:
                    logging.info(f"Fetching labels from column '{label_column}'...")
                    labels[label_column] = dataset.features[label_column].names

            output = namedtuple("LoadDatasetOutput", ["labels"])

            logging.info("Finished.")
            return output(labels)

        def _serialize_json(obj) -> str:
            if isinstance(obj, str):
                return obj
            import json

            def default_serializer(obj):
                if hasattr(obj, 'to_struct'):
                    return obj.to_struct()
                else:
                    raise TypeError(
                        "Object of type '%s' is not JSON serializable and does not have .to_struct() method."
                        % obj.__class__.__name__)

            return json.dumps(obj, default=default_serializer, sort_keys=True)

        import json
        import argparse
        _parser = argparse.ArgumentParser(prog='Load dataset', description='Load a Huggingface Dataset.')
        _parser.add_argument("--path", dest="path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--configuration", dest="configuration", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--label-columns", dest="label_columns", type=json.loads, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--dataset-dir", dest="dataset_dir", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = load_dataset(**_parsed_args)

        _output_serializers = [
            _serialize_json,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      image: quay.io/ibm/kubeflow-notebook-image-ppc64le:elyra3.14.1-py3.9-tf2.10.1-pt1.10.2-v0
      volumeMounts:
      - {mountPath: /tmp/outputs/dataset_dir, name: data-storage, subPath: 'artefacts/{{workflow.uid}}_{{pod.name}}/load-dataset-dataset_dir'}
      - {mountPath: /tmp/outputs/labels, name: data-storage, subPath: 'artefacts/{{workflow.uid}}_{{pod.name}}/load-dataset-labels'}
    inputs:
      parameters:
      - {name: dataset_configuration}
      - {name: dataset_label_columns}
      - {name: dataset_url}
    outputs:
      parameters:
      - name: load-dataset-labels
        valueFrom: {path: /tmp/outputs/labels/data}
      - {name: load-dataset-dataset_dir-subpath, value: 'artefacts/{{workflow.uid}}_{{pod.name}}/load-dataset-dataset_dir'}
      - {name: load-dataset-labels-subpath, value: 'artefacts/{{workflow.uid}}_{{pod.name}}/load-dataset-labels'}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Load
          a Huggingface Dataset.", "implementation": {"container": {"args": ["--path",
          {"inputValue": "path"}, {"if": {"cond": {"isPresent": "configuration"},
          "then": ["--configuration", {"inputValue": "configuration"}]}}, {"if": {"cond":
          {"isPresent": "label_columns"}, "then": ["--label-columns", {"inputValue":
          "label_columns"}]}}, "--dataset-dir", {"outputPath": "dataset_dir"}, "----output-paths",
          {"outputPath": "labels"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef load_dataset(\n    path,\n    dataset_dir,\n    configuration
          = \"\",\n    label_columns = None,\n):\n    \"\"\"\n    Load a Huggingface
          Dataset.\n\n            Parameters:\n                    path: Path from
          which to load the dataset. Huggingfaces hub for datasets is supported. Example:
          \"Lehrig/Monkey-Species-Collection\".\n                    dataset_dir:
          Target directory where the dataset will be loaded to. Should be available
          as a mount from a PVC. Example: \"/blackboard/dataset\".\n                    configuration:
          Name of the dataset configuration to load. Example: \"downsized\".\n                    label_columns:
          Optional list of label column names to be fetched as optional, additional
          output. Example: [\"label\"].\n            Returns:\n                    labels:
          Dictionary mapping label columns to associated labels, if available. Empty
          dictionary otherwise. Example: {\"labels\": [\"cat\", \"dog\"]}\n    \"\"\"\n\n    from
          collections import namedtuple\n    from datasets import load_dataset\n    from
          datasets.dataset_dict import DatasetDict\n    import logging\n    import
          os\n    from PIL.Image import Image\n    import sys\n\n    logging.basicConfig(\n        stream=sys.stdout,\n        level=logging.INFO,\n        format=\"%(levelname)s
          %(asctime)s: %(message)s\",\n    )\n\n    if not configuration:\n        configuration
          = None\n    logging.info(\n        f\"Loading dataset from ''{path}'' using
          configuration ''{configuration}''...\"\n    )\n    dataset = load_dataset(path,
          configuration)\n\n    logging.info(\"Reading image files into bytes...\")\n\n    #
          see: https://huggingface.co/docs/datasets/v2.4.0/en/package_reference/main_classes#datasets.Dataset.save_to_disk\n    def
          read_image_file(example):\n        for column in example:\n            if
          isinstance(example[column], Image):\n                with open(example[column].filename,
          \"rb\") as f:\n                    example[column] = {\"bytes\": f.read()}\n        return
          example\n\n    # note: batching in map caused caching issues, so not using
          it for now\n    dataset = dataset.map(read_image_file)\n\n    logging.info(f\"Saving
          dataset to ''{dataset_dir}''...\")\n    if not os.path.exists(dataset_dir):\n        os.makedirs(dataset_dir)\n    dataset.save_to_disk(dataset_dir)\n\n    logging.info(f\"Dataset
          saved. Contents of ''{dataset_dir}'':\")\n    logging.info(os.listdir(dataset_dir))\n\n    labels
          = dict()\n    if isinstance(dataset, DatasetDict):\n        dataset = next(iter(dataset.values()))\n        for
          label_column in label_columns:\n            logging.info(f\"Fetching labels
          from column ''{label_column}''...\")\n            labels[label_column] =
          dataset.features[label_column].names\n\n    output = namedtuple(\"LoadDatasetOutput\",
          [\"labels\"])\n\n    logging.info(\"Finished.\")\n    return output(labels)\n\ndef
          _serialize_json(obj) -> str:\n    if isinstance(obj, str):\n        return
          obj\n    import json\n\n    def default_serializer(obj):\n        if hasattr(obj,
          ''to_struct''):\n            return obj.to_struct()\n        else:\n            raise
          TypeError(\n                \"Object of type ''%s'' is not JSON serializable
          and does not have .to_struct() method.\"\n                % obj.__class__.__name__)\n\n    return
          json.dumps(obj, default=default_serializer, sort_keys=True)\n\nimport json\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Load dataset'', description=''Load
          a Huggingface Dataset.'')\n_parser.add_argument(\"--path\", dest=\"path\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--configuration\",
          dest=\"configuration\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--label-columns\",
          dest=\"label_columns\", type=json.loads, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dataset-dir\",
          dest=\"dataset_dir\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\", dest=\"_output_paths\",
          type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = load_dataset(**_parsed_args)\n\n_output_serializers
          = [\n    _serialize_json,\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "quay.io/ibm/kubeflow-notebook-image-ppc64le:elyra3.14.1-py3.9-tf2.10.1-pt1.10.2-v0"}},
          "inputs": [{"description": "Path from which to load the dataset. Huggingfaces
          hub for datasets is supported. Example: \"Lehrig/Monkey-Species-Collection\".",
          "name": "path", "type": "String"}, {"default": "", "description": "Name
          of the dataset configuration to load. Example: \"downsized\".", "name":
          "configuration", "optional": true, "type": "String"}, {"description": "Optional
          list of label column names to be fetched as optional, additional output.
          Example: [\"label\"].", "name": "label_columns", "optional": true, "type":
          "typing.List[str]"}], "name": "Load dataset", "outputs": [{"description":
          "Target directory where the dataset will be loaded to. Should be available
          as a mount from a PVC. Example: \"/blackboard/dataset\".", "name": "dataset_dir",
          "type": "String"}, {"name": "labels", "type": "typing.Dict[str, typing.List[str]]"}]}',
        pipelines.kubeflow.org/component_ref: '{"digest": "bb2b9407f55153a3b288052382a2b597b0c26491d9aaba69d5e3866b0e603431",
          "url": "/home/jovyan/components/data-collection/load-dataset/component.yaml"}',
        pipelines.kubeflow.org/arguments.parameters: '{"configuration": "{{inputs.parameters.dataset_configuration}}",
          "label_columns": "{{inputs.parameters.dataset_label_columns}}", "path":
          "{{inputs.parameters.dataset_url}}"}', pipelines.kubeflow.org/max_cache_staleness: P0D}
  - name: plot-confusion-matrix
    container:
      args: [--input-columns, '["pixel_values"]', --label-columns, '{{inputs.parameters.load-dataset-labels}}',
        --test-dataset-dir, /tmp/inputs/test_dataset_dir/data, --model-dir, /tmp/inputs/model_dir/data,
        --dataset-split, test, --batch-size, '20', --mlpipeline-ui-metadata, /tmp/outputs/mlpipeline_ui_metadata/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def plot_confusion_matrix(
            input_columns,
            label_columns,
            test_dataset_dir,
            model_dir,
            mlpipeline_ui_metadata_path,
            dataset_split = "test",
            batch_size = 20,
        ):
            """
            Plots a confusion matrix based on a Huggingface Dataset with a test split and a model trained via Keras.

                    Parameters:
                            input_columns: Input columns for the model. Examples: ["mel_spectrogram", "pixel_values"].
                            label_columns: Dictionary mapping each label column to a list of possible labels. Example: {"genre": ["Blues", "Rock", "Country"]}
                            test_dataset_dir: Directory where to load test data from. Example: "/blackboard/prep_dataset".
                            model_dir: Directory where to load the model from. Example: "/blackboard/model".
                            dataset_split: Optional name of a dataset's split. Defaults to "test".
                            batch_size: Optional batch size when processing the input dataset. Example: 20.
                    Returns:
                            mlpipeline_ui_metadata_path: Data to plot a confusion matrix. The plotted confusion matrix can be viewed via Kubeflow UI's Vizualization for this component inside a pipeline run.
            """
            from collections.abc import Iterable
            import json
            import logging
            import numpy as np
            import pandas as pd
            import sys
            import tensorflow as tf

            logging.basicConfig(
                stream=sys.stdout,
                level=logging.INFO,
                format="%(levelname)s %(asctime)s: %(message)s",
            )
            logger = logging.getLogger()

            test_dataset = tf.data.experimental.load(test_dataset_dir)

            model = tf.keras.models.load_model(model_dir)

            # see: https://github.com/huggingface/datasets/issues/4772
            if "labels" in label_columns:
                label_columns["label"] = label_columns.pop("labels")

            def ensure_encoding(label_tensor):
                rank = label_tensor.shape.rank

                if rank != 1 and rank != 2:
                    err = f"Rank of label tensor has to be 1 or 2 but found rank {rank}!"
                    logger.error(err)
                    raise Exception(err)

                # transform one-hot vector into integer
                return tf.math.argmax(label_tensor, axis=rank - 1)

            def prediction_to_encoded_tensor(prediction):
                result = None
                if isinstance(prediction, np.ndarray):
                    if len(prediction) == 1:
                        pred = prediction[0]
                        if isinstance(pred, np.ndarray):
                            if len(pred) == 1:
                                result = round(pred[0])
                            elif len(pred) > 1:
                                result.append(np.argmax(pred, axis=0))
                            else:
                                err = f"Unsupport prediction array size: {len(pred[0])}"
                                logger.error(err)
                                raise Exception(err)
                        elif isinstance(pred, (np.floating, float)):
                            result.append(round(pred))
                        elif isinstance(pred, (np.integer, int)):
                            result.append(pred)
                        else:
                            err = f"Unsupport prediction type: {type(pred)}"
                            logger.error(err)
                            raise Exception(err)
                    elif len(prediction) > 1:
                        result = np.argmax(prediction, axis=0)
                    else:
                        err = f"Unsupport prediction length: {len(prediction)}"
                        logger.error(err)
                        raise Exception(err)
                elif isinstance(prediction, tf.Tensor):
                    result = ensure_encoding(prediction)
                elif isinstance(prediction, (np.floating, float)):
                    result = round(prediction)
                elif isinstance(prediction, (np.integer, int)):
                    result = prediction
                else:
                    err = f"Unsupport model prediction type: {type(prediction)}"
                    logger.error(err)
                    raise Exception(err)

                if isinstance(result, tf.Tensor):
                    if len(result.shape) == 0:
                        result = [result]
                elif isinstance(result, dict) or isinstance(result, list):
                    if len(result) == 0:
                        result = [result]
                elif isinstance(result, (np.integer, int)):
                    result = [result]
                else:
                    err = f"Unsupport result type: {len(result)}"
                    logger.error(err)
                    raise Exception(err)

                return result

            def merge_to_dict(dictionary, key, tensor):
                if key in dictionary:
                    dictionary[key] = tf.concat([dictionary.pop(key), tensor], axis=0)
                else:
                    dictionary[key] = tensor

            def process_tensors(dictionary, tensors, label_columns, index2label):
                if isinstance(tensors, dict):
                    # Multi-label support
                    for key, tensor in tensors.items():
                        encoded_tensor = prediction_to_encoded_tensor(tensor)
                        merge_to_dict(dictionary, key, encoded_tensor)
                elif isinstance(tensors, list) or isinstance(tensors, np.ndarray):
                    # Multi-label support; assume order & get name from dataset
                    for idx, tensor in enumerate(tensors):
                        encoded_tensor = prediction_to_encoded_tensor(tensor)
                        if len(index2label) == 1:
                            label = index2label[0]
                        else:
                            label = index2label[idx]
                        merge_to_dict(dictionary, label, encoded_tensor)
                elif isinstance(tensors, tf.Tensor):
                    # Assuming single label
                    if len(label_columns) > 1:
                        err = f"Model provides only 1 output but got {len(label_columns)} label columns!"
                        logger.error(err)
                        raise Exception(err)
                    key = next(iter(label_columns))

                    for tensor in tensors:
                        encoded_tensor = prediction_to_encoded_tensor(tensor)
                        merge_to_dict(dictionary, key, encoded_tensor)
                else:
                    err = f"Unsupported tensors type: {type(tensors)}!"
                    logger.error(err)
                    raise Exception(err)

            # Get label indexes & names from dataset
            index2label = dict()
            label_specs = test_dataset.element_spec[1]
            if isinstance(label_specs, Iterable):
                for i, label in enumerate(label_specs):
                    index2label[i] = label
            else:
                # Fall-back to Huggingface dataset
                for i, label in enumerate(label_columns):
                    index2label[i] = label
            logging.info(f"Indexes mapped to labels: {index2label}")

            # each dataset_batch is of size batch_size; results are aggregate inside this loop
            y_true = dict()
            y_pred = dict()
            for dataset_batch in test_dataset:
                label_tensors = dataset_batch[1]
                process_tensors(y_true, label_tensors, label_columns, index2label)

                feature_tensors = dataset_batch[0]
                predictions = model.predict(feature_tensors)
                process_tensors(y_pred, predictions, label_columns, index2label)

            logging.info(f"Final labels: {y_true}")
            logging.info(f"Final predictions: {y_pred}")

            confusion_matrices = []
            for label_column, labels in label_columns.items():
                confusion_matrix = tf.math.confusion_matrix(
                    labels=y_true[label_column],
                    predictions=y_pred[label_column],
                    num_classes=len(labels),
                )

                data = []
                for target_index, target_row in enumerate(confusion_matrix):
                    for predicted_index, count in enumerate(target_row):
                        data.append(
                            (labels[target_index], labels[predicted_index], count.numpy())
                        )

                df = pd.DataFrame(data, columns=["target", "predicted", "count"])

                confusion_matrices.append(
                    {
                        "type": "confusion_matrix",
                        "format": "csv",
                        "schema": [
                            {"name": "target", "type": "CATEGORY"},
                            {"name": "predicted", "type": "CATEGORY"},
                            {"name": "count", "type": "NUMBER"},
                        ],
                        "storage": "inline",
                        "source": df.to_csv(
                            columns=["target", "predicted", "count"], header=False, index=False
                        ),
                        "labels": labels,
                    }
                )

            metadata = {"outputs": confusion_matrices}

            logger.info("Dumping mlpipeline_ui_metadata...")
            with open(mlpipeline_ui_metadata_path, "w") as metadata_file:
                json.dump(metadata, metadata_file)

            logger.info("Finished.")

        import json
        import argparse
        _parser = argparse.ArgumentParser(prog='Plot confusion matrix', description='Plots a confusion matrix based on a Huggingface Dataset with a test split and a model trained via Keras.')
        _parser.add_argument("--input-columns", dest="input_columns", type=json.loads, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--label-columns", dest="label_columns", type=json.loads, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--test-dataset-dir", dest="test_dataset_dir", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--model-dir", dest="model_dir", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--dataset-split", dest="dataset_split", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--batch-size", dest="batch_size", type=int, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--mlpipeline-ui-metadata", dest="mlpipeline_ui_metadata_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = plot_confusion_matrix(**_parsed_args)
      image: quay.io/ibm/kubeflow-notebook-image-ppc64le:elyra3.14.1-py3.9-tf2.10.1-pt1.10.2-v0
      volumeMounts:
      - {mountPath: /tmp/inputs/model_dir, name: data-storage, subPath: '{{inputs.parameters.train-model-job-model_dir-subpath}}',
        readOnly: true}
      - {mountPath: /tmp/inputs/test_dataset_dir, name: data-storage, subPath: '{{inputs.parameters.preprocess-dataset-test_dataset_dir-subpath}}',
        readOnly: true}
      - {mountPath: /tmp/outputs/mlpipeline_ui_metadata, name: data-storage, subPath: 'artefacts/{{workflow.uid}}_{{pod.name}}/mlpipeline-ui-metadata'}
    inputs:
      parameters:
      - {name: load-dataset-labels}
      - {name: train-model-job-model_dir-subpath}
      - {name: preprocess-dataset-test_dataset_dir-subpath}
    outputs:
      parameters:
      - {name: mlpipeline-ui-metadata-subpath, value: 'artefacts/{{workflow.uid}}_{{pod.name}}/mlpipeline-ui-metadata'}
      artifacts:
      - {name: mlpipeline-ui-metadata, path: /tmp/outputs/mlpipeline_ui_metadata/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Plots
          a confusion matrix based on a Huggingface Dataset with a test split and
          a model trained via Keras.", "implementation": {"container": {"args": ["--input-columns",
          {"inputValue": "input_columns"}, "--label-columns", {"inputValue": "label_columns"},
          "--test-dataset-dir", {"inputPath": "test_dataset_dir"}, "--model-dir",
          {"inputPath": "model_dir"}, {"if": {"cond": {"isPresent": "dataset_split"},
          "then": ["--dataset-split", {"inputValue": "dataset_split"}]}}, {"if": {"cond":
          {"isPresent": "batch_size"}, "then": ["--batch-size", {"inputValue": "batch_size"}]}},
          "--mlpipeline-ui-metadata", {"outputPath": "mlpipeline_ui_metadata"}], "command":
          ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef plot_confusion_matrix(\n    input_columns,\n    label_columns,\n    test_dataset_dir,\n    model_dir,\n    mlpipeline_ui_metadata_path,\n    dataset_split
          = \"test\",\n    batch_size = 20,\n):\n    \"\"\"\n    Plots a confusion
          matrix based on a Huggingface Dataset with a test split and a model trained
          via Keras.\n\n            Parameters:\n                    input_columns:
          Input columns for the model. Examples: [\"mel_spectrogram\", \"pixel_values\"].\n                    label_columns:
          Dictionary mapping each label column to a list of possible labels. Example:
          {\"genre\": [\"Blues\", \"Rock\", \"Country\"]}\n                    test_dataset_dir:
          Directory where to load test data from. Example: \"/blackboard/prep_dataset\".\n                    model_dir:
          Directory where to load the model from. Example: \"/blackboard/model\".\n                    dataset_split:
          Optional name of a dataset''s split. Defaults to \"test\".\n                    batch_size:
          Optional batch size when processing the input dataset. Example: 20.\n            Returns:\n                    mlpipeline_ui_metadata_path:
          Data to plot a confusion matrix. The plotted confusion matrix can be viewed
          via Kubeflow UI''s Vizualization for this component inside a pipeline run.\n    \"\"\"\n    from
          collections.abc import Iterable\n    import json\n    import logging\n    import
          numpy as np\n    import pandas as pd\n    import sys\n    import tensorflow
          as tf\n\n    logging.basicConfig(\n        stream=sys.stdout,\n        level=logging.INFO,\n        format=\"%(levelname)s
          %(asctime)s: %(message)s\",\n    )\n    logger = logging.getLogger()\n\n    test_dataset
          = tf.data.experimental.load(test_dataset_dir)\n\n    model = tf.keras.models.load_model(model_dir)\n\n    #
          see: https://github.com/huggingface/datasets/issues/4772\n    if \"labels\"
          in label_columns:\n        label_columns[\"label\"] = label_columns.pop(\"labels\")\n\n    def
          ensure_encoding(label_tensor):\n        rank = label_tensor.shape.rank\n\n        if
          rank != 1 and rank != 2:\n            err = f\"Rank of label tensor has
          to be 1 or 2 but found rank {rank}!\"\n            logger.error(err)\n            raise
          Exception(err)\n\n        # transform one-hot vector into integer\n        return
          tf.math.argmax(label_tensor, axis=rank - 1)\n\n    def prediction_to_encoded_tensor(prediction):\n        result
          = None\n        if isinstance(prediction, np.ndarray):\n            if len(prediction)
          == 1:\n                pred = prediction[0]\n                if isinstance(pred,
          np.ndarray):\n                    if len(pred) == 1:\n                        result
          = round(pred[0])\n                    elif len(pred) > 1:\n                        result.append(np.argmax(pred,
          axis=0))\n                    else:\n                        err = f\"Unsupport
          prediction array size: {len(pred[0])}\"\n                        logger.error(err)\n                        raise
          Exception(err)\n                elif isinstance(pred, (np.floating, float)):\n                    result.append(round(pred))\n                elif
          isinstance(pred, (np.integer, int)):\n                    result.append(pred)\n                else:\n                    err
          = f\"Unsupport prediction type: {type(pred)}\"\n                    logger.error(err)\n                    raise
          Exception(err)\n            elif len(prediction) > 1:\n                result
          = np.argmax(prediction, axis=0)\n            else:\n                err
          = f\"Unsupport prediction length: {len(prediction)}\"\n                logger.error(err)\n                raise
          Exception(err)\n        elif isinstance(prediction, tf.Tensor):\n            result
          = ensure_encoding(prediction)\n        elif isinstance(prediction, (np.floating,
          float)):\n            result = round(prediction)\n        elif isinstance(prediction,
          (np.integer, int)):\n            result = prediction\n        else:\n            err
          = f\"Unsupport model prediction type: {type(prediction)}\"\n            logger.error(err)\n            raise
          Exception(err)\n\n        if isinstance(result, tf.Tensor):\n            if
          len(result.shape) == 0:\n                result = [result]\n        elif
          isinstance(result, dict) or isinstance(result, list):\n            if len(result)
          == 0:\n                result = [result]\n        elif isinstance(result,
          (np.integer, int)):\n            result = [result]\n        else:\n            err
          = f\"Unsupport result type: {len(result)}\"\n            logger.error(err)\n            raise
          Exception(err)\n\n        return result\n\n    def merge_to_dict(dictionary,
          key, tensor):\n        if key in dictionary:\n            dictionary[key]
          = tf.concat([dictionary.pop(key), tensor], axis=0)\n        else:\n            dictionary[key]
          = tensor\n\n    def process_tensors(dictionary, tensors, label_columns,
          index2label):\n        if isinstance(tensors, dict):\n            # Multi-label
          support\n            for key, tensor in tensors.items():\n                encoded_tensor
          = prediction_to_encoded_tensor(tensor)\n                merge_to_dict(dictionary,
          key, encoded_tensor)\n        elif isinstance(tensors, list) or isinstance(tensors,
          np.ndarray):\n            # Multi-label support; assume order & get name
          from dataset\n            for idx, tensor in enumerate(tensors):\n                encoded_tensor
          = prediction_to_encoded_tensor(tensor)\n                if len(index2label)
          == 1:\n                    label = index2label[0]\n                else:\n                    label
          = index2label[idx]\n                merge_to_dict(dictionary, label, encoded_tensor)\n        elif
          isinstance(tensors, tf.Tensor):\n            # Assuming single label\n            if
          len(label_columns) > 1:\n                err = f\"Model provides only 1
          output but got {len(label_columns)} label columns!\"\n                logger.error(err)\n                raise
          Exception(err)\n            key = next(iter(label_columns))\n\n            for
          tensor in tensors:\n                encoded_tensor = prediction_to_encoded_tensor(tensor)\n                merge_to_dict(dictionary,
          key, encoded_tensor)\n        else:\n            err = f\"Unsupported tensors
          type: {type(tensors)}!\"\n            logger.error(err)\n            raise
          Exception(err)\n\n    # Get label indexes & names from dataset\n    index2label
          = dict()\n    label_specs = test_dataset.element_spec[1]\n    if isinstance(label_specs,
          Iterable):\n        for i, label in enumerate(label_specs):\n            index2label[i]
          = label\n    else:\n        # Fall-back to Huggingface dataset\n        for
          i, label in enumerate(label_columns):\n            index2label[i] = label\n    logging.info(f\"Indexes
          mapped to labels: {index2label}\")\n\n    # each dataset_batch is of size
          batch_size; results are aggregate inside this loop\n    y_true = dict()\n    y_pred
          = dict()\n    for dataset_batch in test_dataset:\n        label_tensors
          = dataset_batch[1]\n        process_tensors(y_true, label_tensors, label_columns,
          index2label)\n\n        feature_tensors = dataset_batch[0]\n        predictions
          = model.predict(feature_tensors)\n        process_tensors(y_pred, predictions,
          label_columns, index2label)\n\n    logging.info(f\"Final labels: {y_true}\")\n    logging.info(f\"Final
          predictions: {y_pred}\")\n\n    confusion_matrices = []\n    for label_column,
          labels in label_columns.items():\n        confusion_matrix = tf.math.confusion_matrix(\n            labels=y_true[label_column],\n            predictions=y_pred[label_column],\n            num_classes=len(labels),\n        )\n\n        data
          = []\n        for target_index, target_row in enumerate(confusion_matrix):\n            for
          predicted_index, count in enumerate(target_row):\n                data.append(\n                    (labels[target_index],
          labels[predicted_index], count.numpy())\n                )\n\n        df
          = pd.DataFrame(data, columns=[\"target\", \"predicted\", \"count\"])\n\n        confusion_matrices.append(\n            {\n                \"type\":
          \"confusion_matrix\",\n                \"format\": \"csv\",\n                \"schema\":
          [\n                    {\"name\": \"target\", \"type\": \"CATEGORY\"},\n                    {\"name\":
          \"predicted\", \"type\": \"CATEGORY\"},\n                    {\"name\":
          \"count\", \"type\": \"NUMBER\"},\n                ],\n                \"storage\":
          \"inline\",\n                \"source\": df.to_csv(\n                    columns=[\"target\",
          \"predicted\", \"count\"], header=False, index=False\n                ),\n                \"labels\":
          labels,\n            }\n        )\n\n    metadata = {\"outputs\": confusion_matrices}\n\n    logger.info(\"Dumping
          mlpipeline_ui_metadata...\")\n    with open(mlpipeline_ui_metadata_path,
          \"w\") as metadata_file:\n        json.dump(metadata, metadata_file)\n\n    logger.info(\"Finished.\")\n\nimport
          json\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Plot confusion
          matrix'', description=''Plots a confusion matrix based on a Huggingface
          Dataset with a test split and a model trained via Keras.'')\n_parser.add_argument(\"--input-columns\",
          dest=\"input_columns\", type=json.loads, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--label-columns\",
          dest=\"label_columns\", type=json.loads, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--test-dataset-dir\",
          dest=\"test_dataset_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-dir\",
          dest=\"model_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dataset-split\",
          dest=\"dataset_split\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--batch-size\",
          dest=\"batch_size\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlpipeline-ui-metadata\",
          dest=\"mlpipeline_ui_metadata_path\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = plot_confusion_matrix(**_parsed_args)\n"], "image": "quay.io/ibm/kubeflow-notebook-image-ppc64le:elyra3.14.1-py3.9-tf2.10.1-pt1.10.2-v0"}},
          "inputs": [{"description": "Input columns for the model. Examples: [\"mel_spectrogram\",
          \"pixel_values\"].", "name": "input_columns", "type": "typing.List[str]"},
          {"description": "Dictionary mapping each label column to a list of possible
          labels. Example: {\"genre\": [\"Blues\", \"Rock\", \"Country\"]}", "name":
          "label_columns", "type": "typing.Dict[str, typing.List[str]]"}, {"description":
          "Directory where to load test data from. Example: \"/blackboard/prep_dataset\".",
          "name": "test_dataset_dir", "type": "String"}, {"description": "Directory
          where to load the model from. Example: \"/blackboard/model\".", "name":
          "model_dir", "type": "String"}, {"default": "test", "description": "Optional
          name of a dataset''s split. Defaults to \"test\".", "name": "dataset_split",
          "optional": true, "type": "String"}, {"default": "20", "description": "Optional
          batch size when processing the input dataset. Example: 20.", "name": "batch_size",
          "optional": true, "type": "Integer"}], "name": "Plot confusion matrix",
          "outputs": [{"name": "mlpipeline_ui_metadata"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "f77fe925c7193e4f3b0650d5723915e371048ad8a78cac738fd560deeb9d9aaf", "url":
          "/home/jovyan/components/model-building/plot-confusion-matrix/component.yaml"}',
        pipelines.kubeflow.org/arguments.parameters: '{"batch_size": "20", "dataset_split":
          "test", "input_columns": "[\"pixel_values\"]", "label_columns": "{{inputs.parameters.load-dataset-labels}}"}',
        pipelines.kubeflow.org/max_cache_staleness: P0D}
  - name: preprocess-dataset
    container:
      args: [--dataset-dir, /tmp/inputs/dataset_dir/data, --test-size, '0.2', --seed,
        '42', --size, '224', --batch-size, '16', --train-dataset-dir, /tmp/outputs/train_dataset_dir/data,
        --validation-dataset-dir, /tmp/outputs/validation_dataset_dir/data, --test-dataset-dir,
        /tmp/outputs/test_dataset_dir/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def preprocess_dataset(
            dataset_dir,
            train_dataset_dir,
            validation_dataset_dir,
            test_dataset_dir,
            test_size = 0.2,
            seed = 42,
            size = 224,
            batch_size = 16,
        ):
            """Split data into train/dev/test data. Saves result into `prep_dataset_dir`."""

            from datasets import Array3D, DatasetDict, Features, load_from_disk, Sequence, Value
            import numpy as np
            import os
            from transformers import DefaultDataCollator, ImageFeatureExtractionMixin

            print(f"Loading input dataset from {dataset_dir}...")
            dataset = load_from_disk(dataset_dir)
            print("Dataset loaded.")

            # Preprocess
            num_classes = dataset["train"].features["label"].num_classes
            one_hot_matrix = np.eye(num_classes)
            feature_extractor = ImageFeatureExtractionMixin()

            def to_pixels(image):
                image = feature_extractor.resize(image, size=size)
                image = feature_extractor.to_numpy_array(image, channel_first=False)
                image = image / 255.0
                return image

            def process(examples):
                examples["pixel_values"] = [to_pixels(image) for image in examples["image"]]
                examples["label"] = [one_hot_matrix[label] for label in examples["label"]]
                return examples

            features = Features(
                {
                    "pixel_values": Array3D(dtype="float32", shape=(size, size, 3)),
                    "label": Sequence(feature=Value(dtype="int32"), length=num_classes),
                }
            )

            print("Preprocessing dataset...")
            prep_dataset = dataset.map(
                process,
                remove_columns=["image"],
                batched=True,
                batch_size=batch_size,
                num_proc=2,
                features=features,
                keep_in_memory=True,
            )

            # prep_dataset = prep_dataset.with_format("numpy")

            # Split
            print("Splitting dataset...")
            dev_test_dataset = prep_dataset["test"].train_test_split(
                test_size=test_size, shuffle=True, seed=seed
            )

            train_dev_test_dataset = DatasetDict(
                {
                    "train": prep_dataset["train"],
                    "validation": dev_test_dataset["train"],
                    "test": dev_test_dataset["test"],
                }
            )

            def save_as_tfdataset(
                dataset, columns, label_columns, data_collator, directory, shuffle
            ):
                import tensorflow as tf

                tf_dataset = dataset.to_tf_dataset(
                    columns=columns,
                    label_cols=label_columns,
                    shuffle=shuffle,
                    batch_size=batch_size,
                    collate_fn=data_collator,
                )

                print(f"Saving pre-processed dataset to '{directory}'...")
                if not os.path.exists(directory):
                    os.makedirs(directory)
                tf.data.experimental.save(tf_dataset, directory)

                print(f"Pre-processed dataset saved. Contents of '{directory}':")
                print(os.listdir(directory))

            data_collator = DefaultDataCollator(return_tensors="tf")
            columns = ["pixel_values"]
            label_columns = ["labels"]
            save_as_tfdataset(
                train_dev_test_dataset["train"],
                columns,
                label_columns,
                data_collator,
                train_dataset_dir,
                True,
            )
            save_as_tfdataset(
                train_dev_test_dataset["validation"],
                columns,
                label_columns,
                data_collator,
                validation_dataset_dir,
                False,
            )
            save_as_tfdataset(
                train_dev_test_dataset["test"],
                columns,
                label_columns,
                data_collator,
                test_dataset_dir,
                False,
            )

            print("Finished.")

        import argparse
        _parser = argparse.ArgumentParser(prog='Preprocess dataset', description='Split data into train/dev/test data. Saves result into `prep_dataset_dir`.')
        _parser.add_argument("--dataset-dir", dest="dataset_dir", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--test-size", dest="test_size", type=float, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--seed", dest="seed", type=int, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--size", dest="size", type=int, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--batch-size", dest="batch_size", type=int, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--train-dataset-dir", dest="train_dataset_dir", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--validation-dataset-dir", dest="validation_dataset_dir", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--test-dataset-dir", dest="test_dataset_dir", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = preprocess_dataset(**_parsed_args)
      image: quay.io/ibm/kubeflow-notebook-image-ppc64le:elyra3.14.1-py3.9-tf2.10.1-pt1.10.2-v0
      volumeMounts:
      - {mountPath: /tmp/inputs/dataset_dir, name: data-storage, subPath: '{{inputs.parameters.load-dataset-dataset_dir-subpath}}',
        readOnly: true}
      - {mountPath: /tmp/outputs/test_dataset_dir, name: data-storage, subPath: 'artefacts/{{workflow.uid}}_{{pod.name}}/preprocess-dataset-test_dataset_dir'}
      - {mountPath: /tmp/outputs/train_dataset_dir, name: data-storage, subPath: 'artefacts/{{workflow.uid}}_{{pod.name}}/preprocess-dataset-train_dataset_dir'}
      - {mountPath: /tmp/outputs/validation_dataset_dir, name: data-storage, subPath: 'artefacts/{{workflow.uid}}_{{pod.name}}/preprocess-dataset-validation_dataset_dir'}
    inputs:
      parameters:
      - {name: load-dataset-dataset_dir-subpath}
    outputs:
      parameters:
      - {name: preprocess-dataset-test_dataset_dir-subpath, value: 'artefacts/{{workflow.uid}}_{{pod.name}}/preprocess-dataset-test_dataset_dir'}
      - {name: preprocess-dataset-train_dataset_dir-subpath, value: 'artefacts/{{workflow.uid}}_{{pod.name}}/preprocess-dataset-train_dataset_dir'}
      - {name: preprocess-dataset-validation_dataset_dir-subpath, value: 'artefacts/{{workflow.uid}}_{{pod.name}}/preprocess-dataset-validation_dataset_dir'}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Split
          data into train/dev/test data. Saves result into `prep_dataset_dir`.", "implementation":
          {"container": {"args": ["--dataset-dir", {"inputPath": "dataset_dir"}, {"if":
          {"cond": {"isPresent": "test_size"}, "then": ["--test-size", {"inputValue":
          "test_size"}]}}, {"if": {"cond": {"isPresent": "seed"}, "then": ["--seed",
          {"inputValue": "seed"}]}}, {"if": {"cond": {"isPresent": "size"}, "then":
          ["--size", {"inputValue": "size"}]}}, {"if": {"cond": {"isPresent": "batch_size"},
          "then": ["--batch-size", {"inputValue": "batch_size"}]}}, "--train-dataset-dir",
          {"outputPath": "train_dataset_dir"}, "--validation-dataset-dir", {"outputPath":
          "validation_dataset_dir"}, "--test-dataset-dir", {"outputPath": "test_dataset_dir"}],
          "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" >
          \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef preprocess_dataset(\n    dataset_dir,\n    train_dataset_dir,\n    validation_dataset_dir,\n    test_dataset_dir,\n    test_size
          = 0.2,\n    seed = 42,\n    size = 224,\n    batch_size = 16,\n):\n    \"\"\"Split
          data into train/dev/test data. Saves result into `prep_dataset_dir`.\"\"\"\n\n    from
          datasets import Array3D, DatasetDict, Features, load_from_disk, Sequence,
          Value\n    import numpy as np\n    import os\n    from transformers import
          DefaultDataCollator, ImageFeatureExtractionMixin\n\n    print(f\"Loading
          input dataset from {dataset_dir}...\")\n    dataset = load_from_disk(dataset_dir)\n    print(\"Dataset
          loaded.\")\n\n    # Preprocess\n    num_classes = dataset[\"train\"].features[\"label\"].num_classes\n    one_hot_matrix
          = np.eye(num_classes)\n    feature_extractor = ImageFeatureExtractionMixin()\n\n    def
          to_pixels(image):\n        image = feature_extractor.resize(image, size=size)\n        image
          = feature_extractor.to_numpy_array(image, channel_first=False)\n        image
          = image / 255.0\n        return image\n\n    def process(examples):\n        examples[\"pixel_values\"]
          = [to_pixels(image) for image in examples[\"image\"]]\n        examples[\"label\"]
          = [one_hot_matrix[label] for label in examples[\"label\"]]\n        return
          examples\n\n    features = Features(\n        {\n            \"pixel_values\":
          Array3D(dtype=\"float32\", shape=(size, size, 3)),\n            \"label\":
          Sequence(feature=Value(dtype=\"int32\"), length=num_classes),\n        }\n    )\n\n    print(\"Preprocessing
          dataset...\")\n    prep_dataset = dataset.map(\n        process,\n        remove_columns=[\"image\"],\n        batched=True,\n        batch_size=batch_size,\n        num_proc=2,\n        features=features,\n        keep_in_memory=True,\n    )\n\n    #
          prep_dataset = prep_dataset.with_format(\"numpy\")\n\n    # Split\n    print(\"Splitting
          dataset...\")\n    dev_test_dataset = prep_dataset[\"test\"].train_test_split(\n        test_size=test_size,
          shuffle=True, seed=seed\n    )\n\n    train_dev_test_dataset = DatasetDict(\n        {\n            \"train\":
          prep_dataset[\"train\"],\n            \"validation\": dev_test_dataset[\"train\"],\n            \"test\":
          dev_test_dataset[\"test\"],\n        }\n    )\n\n    def save_as_tfdataset(\n        dataset,
          columns, label_columns, data_collator, directory, shuffle\n    ):\n        import
          tensorflow as tf\n\n        tf_dataset = dataset.to_tf_dataset(\n            columns=columns,\n            label_cols=label_columns,\n            shuffle=shuffle,\n            batch_size=batch_size,\n            collate_fn=data_collator,\n        )\n\n        print(f\"Saving
          pre-processed dataset to ''{directory}''...\")\n        if not os.path.exists(directory):\n            os.makedirs(directory)\n        tf.data.experimental.save(tf_dataset,
          directory)\n\n        print(f\"Pre-processed dataset saved. Contents of
          ''{directory}'':\")\n        print(os.listdir(directory))\n\n    data_collator
          = DefaultDataCollator(return_tensors=\"tf\")\n    columns = [\"pixel_values\"]\n    label_columns
          = [\"labels\"]\n    save_as_tfdataset(\n        train_dev_test_dataset[\"train\"],\n        columns,\n        label_columns,\n        data_collator,\n        train_dataset_dir,\n        True,\n    )\n    save_as_tfdataset(\n        train_dev_test_dataset[\"validation\"],\n        columns,\n        label_columns,\n        data_collator,\n        validation_dataset_dir,\n        False,\n    )\n    save_as_tfdataset(\n        train_dev_test_dataset[\"test\"],\n        columns,\n        label_columns,\n        data_collator,\n        test_dataset_dir,\n        False,\n    )\n\n    print(\"Finished.\")\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Preprocess dataset'',
          description=''Split data into train/dev/test data. Saves result into `prep_dataset_dir`.'')\n_parser.add_argument(\"--dataset-dir\",
          dest=\"dataset_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--test-size\",
          dest=\"test_size\", type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--seed\",
          dest=\"seed\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--size\",
          dest=\"size\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--batch-size\",
          dest=\"batch_size\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--train-dataset-dir\",
          dest=\"train_dataset_dir\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--validation-dataset-dir\",
          dest=\"validation_dataset_dir\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--test-dataset-dir\",
          dest=\"test_dataset_dir\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = preprocess_dataset(**_parsed_args)\n"], "image": "quay.io/ibm/kubeflow-notebook-image-ppc64le:elyra3.14.1-py3.9-tf2.10.1-pt1.10.2-v0"}},
          "inputs": [{"name": "dataset_dir", "type": "String"}, {"default": "0.2",
          "name": "test_size", "optional": true, "type": "Float"}, {"default": "42",
          "name": "seed", "optional": true, "type": "Integer"}, {"default": "224",
          "name": "size", "optional": true, "type": "Integer"}, {"default": "16",
          "name": "batch_size", "optional": true, "type": "Integer"}], "name": "Preprocess
          dataset", "outputs": [{"name": "train_dataset_dir", "type": "String"}, {"name":
          "validation_dataset_dir", "type": "String"}, {"name": "test_dataset_dir",
          "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"batch_size":
          "16", "seed": "42", "size": "224", "test_size": "0.2"}', pipelines.kubeflow.org/max_cache_staleness: P0D}
  - name: train-model-job
    container:
      args:
      - --train-dataset-dir
      - /tmp/inputs/train_dataset_dir/data
      - --validation-dataset-dir
      - /tmp/inputs/validation_dataset_dir/data
      - --train-specification
      - |
        name: Train distributed model
        description: Uses transfer learning on a prepared dataset. Once trained, the model
          is persisted to `model_dir`.
        inputs:
        - {name: train_dataset_dir, type: String}
        - {name: validation_dataset_dir, type: String}
        - {name: epochs, type: Integer, default: '100', optional: true}
        - {name: batch_size, type: Integer, default: '32', optional: true}
        outputs:
        - {name: model_dir, type: String}
        implementation:
          container:
            image: quay.io/ibm/kubeflow-notebook-image-ppc64le:elyra3.14.1-py3.9-tf2.10.1-pt1.10.2-v0
            command:
            - sh
            - -ec
            - |
              program_path=$(mktemp)
              printf "%s" "$0" > "$program_path"
              python3 -u "$program_path" "$@"
            - |
              def _make_parent_dirs_and_return_path(file_path: str):
                  import os
                  os.makedirs(os.path.dirname(file_path), exist_ok=True)
                  return file_path

              def train_distributed_model(
                  train_dataset_dir,
                  validation_dataset_dir,
                  model_dir,
                  epochs = 100,
                  batch_size = 32,
              ):
                  """Uses transfer learning on a prepared dataset. Once trained, the model is persisted to `model_dir`."""

                  import horovod.tensorflow.keras as hvd
                  import os
                  import tensorflow as tf
                  from tensorflow.keras import Sequential
                  from tensorflow.keras.applications import InceptionV3
                  from tensorflow.keras.layers import (
                      BatchNormalization,
                      Dense,
                      Dropout,
                      GlobalAveragePooling2D,
                  )
                  from tensorflow.keras.callbacks import (
                      EarlyStopping,
                      ModelCheckpoint,
                      ReduceLROnPlateau,
                      TensorBoard,
                  )
                  from horovod.tensorflow.keras.callbacks import MetricAverageCallback
                  import tensorflow_datasets as tfds
                  import time

                  def load_datasets():
                      train_dataset = tf.data.experimental.load(train_dataset_dir)
                      validation_dataset = tf.data.experimental.load(validation_dataset_dir)
                      return (train_dataset, validation_dataset)

                  def build_model(shape):
                      backbone = InceptionV3(include_top=False, weights="imagenet", input_shape=shape)

                      for layer in backbone.layers:
                          layer.trainable = False

                      model = Sequential()
                      model.add(backbone)
                      model.add(GlobalAveragePooling2D())
                      model.add(Dense(128, activation="relu"))
                      model.add(BatchNormalization())
                      model.add(Dropout(0.3))
                      model.add(Dense(64, activation="relu"))
                      model.add(BatchNormalization())
                      model.add(Dropout(0.3))
                      model.add(Dense(10, activation="softmax"))

                      return model

                  print("Initializing Horovod/MPI for distributed training...")
                  hvd.init()

                  # Pin GPU to be used to process local rank (one GPU per process)
                  gpus = tf.config.experimental.list_physical_devices("GPU")
                  for gpu in gpus:
                      tf.config.experimental.set_memory_growth(gpu, True)
                  if gpus:
                      tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], "GPU")

                  # Prepare distributed training with GPU support
                  os.environ["NCCL_DEBUG"] = "INFO"
                  tfds.disable_progress_bar()

                  if hvd.rank() == 0:
                      if not os.path.exists(model_dir):
                          os.makedirs(model_dir)

                  # see https://horovod.readthedocs.io/en/stable/api.html
                  print("==============================================")
                  print(f"hvd.rank(): {str(hvd.rank())}")
                  print(f"hvd.local_rank(): {str(hvd.local_rank())}")
                  print(f"hvd.size(): {str(hvd.size())}")
                  print(f"hvd.local_size(): {str(hvd.local_size())}")
                  print("gpus:")
                  print(gpus)
                  print("==============================================")

                  print("Loading datasets...")
                  train_dataset, validation_dataset = load_datasets()
                  shape = (224, 224, 3)

                  print("Making traininig dataset ready for distributed training...")
                  # Best shuffling needs a buffer with size equal to the size of the
                  # dataset. Approximate values should be fine here.
                  dataset_elements = 1400  # hard to determine dynamically in TFDataset
                  approx_shard_train_size = dataset_elements // hvd.size() + 1

                  # References:
                  # - shard: https://github.com/horovod/horovod/issues/2623#issuecomment-768435610
                  # - cache & prefetch: https://stackoverflow.com/questions/59228816/what-do-the-tensorflow-datasets-functions-cache-and-prefetch-do
                  # - shuffle: https://stackoverflow.com/questions/64372390/what-does-buffer-size-do-in-tensorflow-dataset-shuffling
                  distributed_train_dataset = (
                      train_dataset.unbatch()  # Batch after sharding
                      .shard(num_shards=hvd.size(), index=hvd.rank())  # 1 shard per worker
                      .cache()  # Reuse data on next epoch
                      .shuffle(
                          buffer_size=approx_shard_train_size, seed=42, reshuffle_each_iteration=False
                      )  # Randomize shards
                      .batch(batch_size)
                      .repeat()  # Avoid last batch being of unequal size
                      .prefetch(tf.data.AUTOTUNE)  # Overlap preprocessing and training
                  )

                  print("Building model...")
                  model = build_model(shape)
                  print(model.summary())

                  opt = tf.keras.optimizers.Adam(learning_rate=0.001 * hvd.size())
                  # Horovod: add Horovod DistributedOptimizer.
                  opt = hvd.DistributedOptimizer(opt)

                  print("Compiling model...")
                  # Horovod: Specify `experimental_run_tf_function=False` to ensure TensorFlow
                  # uses hvd.DistributedOptimizer() to compute gradients.
                  model.compile(
                      optimizer=opt,
                      loss="categorical_crossentropy",
                      metrics=["categorical_accuracy"],
                      experimental_run_tf_function=False,
                  )

                  print("Initializing training callbacks...")
                  callbacks = [
                      # MetricAverageCallback is used to synchrnoize metrics between the workers,
                      # That is important when using ReduceLROnPlateau and/or EarlyStopping.
                      # https://www.olcf.ornl.gov/wp-content/uploads/2019/12/ORNL-Scaling-20200210.pdf (chart 28)
                      MetricAverageCallback(),
                      ReduceLROnPlateau(
                          monitor="val_loss",
                          factor=0.1,
                          patience=7,
                          verbose=1,
                          min_delta=0.0001,
                          mode="min",
                      ),
                      EarlyStopping(monitor="val_loss", patience=20, verbose=1, mode="min"),
                      TensorBoard(
                          log_dir=f"s3://mlpipeline/tensorboard/{os.environ['JOB_NAME']}",
                          histogram_freq=1,
                      ),
                      # Horovod: broadcast initial variable states from rank 0 to all other processes.
                      # This is necessary to ensure consistent initialization of all workers when
                      # training is started with random weights or restored from a checkpoint.
                      hvd.callbacks.BroadcastGlobalVariablesCallback(0),
                  ]
                  # Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them.
                  if hvd.rank() == 0:
                      callbacks.append(
                          ModelCheckpoint(
                              f"{model_dir}/best_model.keras",
                              monitor="val_loss",
                              save_best_only=True,
                              save_weights_only=True,
                              mode="min",
                          )
                      )

                  print("Starting model training...")
                  start = time.time()
                  hist = model.fit(
                      distributed_train_dataset,
                      validation_data=validation_dataset,
                      epochs=epochs,
                      steps_per_epoch=approx_shard_train_size // batch_size
                      + 1,  # steps_per_epoch is needed when using repeat()
                      callbacks=callbacks,
                      verbose=1 if hvd.rank() == 0 else 0,
                  )

                  if hvd.rank() == 0:
                      print("\n\nTraining took ", time.time() - start, "seconds")

                      print("Model train history:")
                      print(hist.history)

                      print(f"Saving model to: {model_dir}")
                      model.save(model_dir)
                      print(f"Model saved to: {model_dir}")

                  print(f"Finished. {hvd.rank()}")

              import argparse
              _parser = argparse.ArgumentParser(prog='Train distributed model', description='Uses transfer learning on a prepared dataset. Once trained, the model is persisted to `model_dir`.')
              _parser.add_argument("--train-dataset-dir", dest="train_dataset_dir", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--validation-dataset-dir", dest="validation_dataset_dir", type=str, required=True, default=argparse.SUPPRESS)
              _parser.add_argument("--epochs", dest="epochs", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--batch-size", dest="batch_size", type=int, required=False, default=argparse.SUPPRESS)
              _parser.add_argument("--model-dir", dest="model_dir", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
              _parsed_args = vars(_parser.parse_args())

              _outputs = train_distributed_model(**_parsed_args)
            args:
            - --train-dataset-dir
            - {inputPath: train_dataset_dir}
            - --validation-dataset-dir
            - {inputPath: validation_dataset_dir}
            - if:
                cond: {isPresent: epochs}
                then:
                - --epochs
                - {inputValue: epochs}
            - if:
                cond: {isPresent: batch_size}
                then:
                - --batch-size
                - {inputValue: batch_size}
            - --model-dir
            - {outputPath: model_dir}
      - --train-parameters
      - '{"epochs": "{{inputs.parameters.epochs}}", "model_dir": "model_dir", "train_dataset_dir":
        "train_dataset_dir", "validation_dataset_dir": "validation_dataset_dir"}'
      - --train-mount
      - /train
      - --model-name
      - '{{inputs.parameters.model_name}}'
      - --node-selector
      - '{{inputs.parameters.training_node_selector}}'
      - --pvc-name
      - ''
      - --pvc-size
      - 10Gi
      - --cpus
      - ''
      - --gpus
      - '{{inputs.parameters.training_gpus}}'
      - --memory
      - ''
      - --cluster-configuration-secret
      - '{{inputs.parameters.cluster_configuration_secret}}'
      - --distribution-specification
      - '{"distribution_type": "MPI", "number_of_workers": "{{inputs.parameters.number_of_workers}}"}'
      - --model-dir
      - /tmp/outputs/model_dir/data
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def train_model_job(
            train_dataset_dir,
            validation_dataset_dir,
            train_specification,
            train_parameters,
            model_dir,
            train_mount = "/train",
            model_name = "my-model",
            node_selector = "",
            pvc_name = "",
            pvc_size = "10Gi",
            cpus = "",
            gpus = 0,
            memory = "",
            cluster_configuration_secret = "",
            distribution_specification = None,
        ):
            """
            Trains a model. Once trained, the model is persisted to model_dir.

                    Parameters:
                            train_dataset_dir: Path to the directory with training data.
                            validation_dataset_dir: Path to the directory with validation data to be used during training.
                            train_specification: Training command as generated from a Python function using kfp.components.func_to_component_text.
                            train_parameters: Dictionary mapping formal to actual parameters for the training spacification.
                            model_dir: Target path where the model will be stored.
                            train_mount: Optional mounting point for training data of an existing PVC. Example: "/train".
                            model_name: Optional name of the model. Must be unique for the targeted namespace and conform Kubernetes naming conventions. Example: my-model.
                            node_selector: Optional node selector for worker nodes. Example: nvidia.com/gpu.product: "Tesla-V100-SXM2-32GB".
                            pvc_name: Optional name to an existing persistent volume claim (pvc). If given, this pvc is mounted into the training job. Example: "music-genre-classification-j4ssf-training-pvc".
                            pvc_size: Optional size of the storage during model training. Storage is mounted into to the Job based on a persitent volume claim of the given size. Example: 10Gi.
                            cpus: Optional CPU limit for the job. Leave empty for cluster defaults (typically no limit). Example: "1000m".
                            gpus: Optional number of GPUs for the job. Example: 2.
                            memory: Optional memory limit for the job. Leave empty for cluster defaults (typically no limit). Example: "1Gi".
                            cluster_configuration_secret: Optional secret name configuring a (remote) Kubernetes cluster to run the job in and the backing MinIO object store. All secret's data values are optional and appropriate defaults are chosen if not present. The secret may provide a suitable kubernetes bearer token, the associated namespace, a host, etc. Example: "remote-power-cluster".
                            distribution_specification: Optional dictionary specifiying the distribution behavior. By default, no distributed training is executed, which results in an ordinary Kubernetes Job  for training. Otherwise, dictionary entries determine the distribution behavior. The "distribution_type" entry determines the distribution type: "Job" (no distribution; ordinary Kubernetes job), "MPI" (all-reduce style distribution via Horovod), or "TF" (parameter-server style distribution via distributed training with TensorFlow). Depending on the distribution type, additional dictionary entries can be processed. For distributed training jobs, the "number_of_workers" (e.g., 2) determines the number of worker replicas for training. Individual resource limits can be controlled via "worker_cpus" (e.g., "1000m") and "worker_memory" (e.g., "1Gi"). MPI additionally provides a fine-grained control of launcher cpu and memory limits via "launcher_cpus" (e.g., "1000m") and "launcher_memory" (e.g., "1Gi"). Full example with MPI: {"distribution_type": "MPI", "number_of_workers": 2, "worker_cpus": "8", "worker_memory": "32Gi", "launcher_cpus": "2", "launcher_memory": "8Gi"}
            """
            from datetime import datetime
            import errno
            import json
            import kfp
            from kubernetes import client, config, utils, watch
            import logging
            import os
            import shutil
            import sys
            import yaml

            logging.basicConfig(
                stream=sys.stdout,
                level=logging.INFO,
                format="%(levelname)s %(asctime)s: %(message)s",
            )
            logger = logging.getLogger()

            ###########################################################################
            # Helper Functions
            ###########################################################################

            def establish_local_cluster_connection():
                config.load_incluster_config()
                return client.ApiClient()

            def get_cluster_configuration(api_client, cluster_configuration_secret):
                import base64
                from kubernetes.client.rest import ApiException

                def decode(secret, key):
                    data = secret.data[key]
                    decoded_data = base64.b64decode(data)
                    return decoded_data.decode("utf-8")

                def update_with_secret(secret, dictionary):
                    for key in dictionary:
                        if key in secret.data:
                            dictionary[key] = decode(secret, key)

                cluster_configuration = {
                    "access-mode": "ReadWriteMany",
                    "minio-accesskey": "minio",
                    "minio-bucket": "mlpipeline",
                    "minio-job-folder": "jobs",
                    "minio-secretkey": "minio123",
                    "minio-url": "http://minio-service.kubeflow:9000",
                    "remote-host": "",
                    "remote-namespace": "",
                    "remote-token": "",
                }

                try:
                    default_minio_secret = client.CoreV1Api(api_client).read_namespaced_secret(
                        "mlpipeline-minio-artifact", get_current_namespace()
                    )

                    if default_minio_secret.data is None:
                        logger.info(
                            "MinIO secret (mlpipeline-minio-artifact) includes no data - progressing with default values."
                        )
                    else:
                        logger.info(
                            "Found default MinIO secret (mlpipeline-minio-artifact) - updating cluster configuration accordingly."
                        )
                        cluster_configuration["minio-accesskey"] = decode(
                            default_minio_secret, "accesskey"
                        )
                        cluster_configuration["minio-secretkey"] = decode(
                            default_minio_secret, "secretkey"
                        )
                except ApiException as e:
                    if e.status == 404:
                        logger.info(
                            "Found no default MinIO secret (mlpipeline-minio-artifact) - progressing with default values."
                        )

                if cluster_configuration_secret == "":
                    logger.info(
                        "No cluster configuration secret specified - progressing with default values."
                    )
                    return cluster_configuration

                try:
                    secret = client.CoreV1Api(api_client).read_namespaced_secret(
                        cluster_configuration_secret, get_current_namespace()
                    )
                    if secret.data is None:
                        logger.info(
                            f"Cluster configuration secret ({cluster_configuration_secret}) includes no data - progressing with default values."
                        )
                    else:
                        logger.info(
                            f"Found cluster configuration secret ({cluster_configuration_secret}) - updating cluster configuration accordingly."
                        )
                        update_with_secret(secret, cluster_configuration)
                except ApiException as e:
                    if e.status == 404:
                        logger.info(
                            f"Found no cluster configuration secret ({cluster_configuration_secret}) - progressing with default values."
                        )

                return cluster_configuration

            def establish_training_cluster_connection(local_api_client, cluster_configuration):
                is_remote = False
                if (
                    cluster_configuration["remote-host"] == ""
                    or cluster_configuration["remote-token"] == ""
                ):
                    logger.info(
                        "Remote cluster not configured. Using in-cluster configuration..."
                    )
                    logger.info(
                        "Note: assign the name of a secret to the 'cluster_configuration_secret' pipeline argument and add the secret to your cluster."
                    )
                    logger.info("Example secret:")
                    logger.info("---")
                    logger.info("apiVersion: v1")
                    logger.info("kind: Secret")
                    logger.info("metadata:")
                    logger.info("  name: my-remote-cluster")
                    logger.info("stringData:")
                    logger.info("  access-mode: ReadWriteOnce")
                    logger.info("  minio-accesskey: minio")
                    logger.info("  minio-bucket: mlpipeline")
                    logger.info("  minio-job-folder: jobs")
                    logger.info("  minio-secretkey: minio123")
                    logger.info("  minio-url: http://minio-service.kubeflow:9000")
                    logger.info(
                        "  remote-host: https://istio-ingressgateway-istio-system.apps.mydomain.ai:6443"
                    )
                    logger.info("  remote-namespace: default")
                    logger.info("  remote-token: eyJh...")
                    logger.info("---")
                    logger.info(
                        "Where you get the remote-token from your remote cluster as described here:"
                    )
                    logger.info(
                        "https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/#without-kubectl-proxy"
                    )

                    api_client = local_api_client
                    if not os.path.exists(train_mount):
                        logger.warning(
                            f"No local mount to {train_mount} found. Therefore, switching to remote data synchronization mode via MinIO. This will work but is slower compared to local mounts. Consider adding a mount to '{train_mount}' for this component by using a PVC inside your pipeline."
                        )
                        is_remote = True
                else:
                    # see: https://github.com/kubernetes-client/python/blob/6d4587e18064288d031ed9bbf5ab5b8245460b3c/examples/remote_cluster.py
                    logger.info(
                        "Remote host and token found. Using remote cluster configuration..."
                    )
                    configuration = client.Configuration()
                    configuration.host = cluster_configuration["remote-host"]
                    configuration.verify_ssl = False
                    configuration.api_key = {
                        "authorization": "Bearer " + cluster_configuration["remote-token"]
                    }
                    api_client = client.ApiClient(configuration)
                    is_remote = True

                return (api_client, is_remote)

            def clone_path(source, target):
                try:
                    logger.info(f"Cloning source path {source} to {target} of training job...")
                    shutil.copytree(source, target)
                    logger.info("Cloning finished. Target path contents:")
                    logger.info(os.listdir(target))
                except OSError as e:
                    if e.errno in (errno.ENOTDIR, errno.EINVAL):
                        shutil.copy(source, target)
                    else:
                        raise

            def sync_with_minio(
                cluster_configuration,
                inputs,
                job_name,
                is_upload,
                remove_minio_files = False,
            ):
                import boto3
                import botocore
                from botocore.client import Config
                import json
                import logging
                import os
                import sys
                import tarfile

                logging.basicConfig(
                    stream=sys.stdout,
                    level=logging.INFO,
                    format="%(levelname)s %(asctime)s: %(message)s",
                )
                logger = logging.getLogger()

                def establish_minio_connection(cluster_configuration):
                    if ("minio-accesskey" in cluster_configuration) and (
                        "minio-secretkey" in cluster_configuration
                    ):
                        minio_user = cluster_configuration["minio-accesskey"]
                        minio_pass = cluster_configuration["minio-secretkey"]
                    else:
                        minio_user = os.getenv("MINIO_USER")
                        minio_pass = os.getenv("MINIO_PASS")

                    if minio_user == "" or minio_pass == "":
                        err = "Environment variables MINIO_USER and MINIO_PASS need externally to be provided to this component using k8s_secret_key_to_env!"
                        raise Exception(err)

                    return boto3.session.Session().resource(
                        service_name="s3",
                        endpoint_url=cluster_configuration["minio-url"],
                        aws_access_key_id=minio_user,
                        aws_secret_access_key=minio_pass,
                        config=Config(signature_version="s3v4"),
                    )

                def path_to_tarfilename(pathname):
                    return f"{pathname.replace(os.sep, '-')}.tar.gz"

                def make_tarfile(output_filename, source_dir):
                    with tarfile.open(output_filename, "w:gz") as tar:
                        tar.add(source_dir, arcname=".")

                # see: https://stackoverflow.com/a/47565719/2625096
                def bucket_exists(minio_client, bucket):
                    try:
                        minio_client.meta.client.head_bucket(Bucket=bucket.name)
                        return True
                    except botocore.exceptions.ClientError as e:
                        error_code = int(e.response["Error"]["Code"])
                        if error_code == 403:
                            # Forbidden Access -> Private Bucket
                            return True
                        elif error_code == 404:
                            return False

                def upload_to_minio(file, upload_bucket, job_folder, job_name, minio_client):
                    bucket = minio_client.Bucket(upload_bucket)

                    if not bucket_exists(minio_client, bucket):
                        minio_client.create_bucket(Bucket=bucket.name)

                    bucket.upload_file(file, f"{job_folder}/{job_name}/{file}")

                def download_from_minio(
                    file, upload_bucket, job_folder, job_name, minio_client, remove_minio_file
                ):
                    bucket = minio_client.Bucket(upload_bucket)
                    key = f"{job_folder}/{job_name}/{file}"

                    bucket.download_file(key, file)

                    if remove_minio_file:
                        bucket.Object(key).delete()

                def extract_tarfile(tarfile_name, target):
                    with tarfile.open(tarfile_name, "r:gz") as tar_gz_ref:
                        tar_gz_ref.extractall(target)

                if isinstance(cluster_configuration, str):
                    cluster_configuration = json.loads(cluster_configuration)

                if isinstance(inputs, str):
                    inputs = json.loads(inputs)

                if isinstance(is_upload, str):
                    if is_upload == "True":
                        is_upload = True
                    else:
                        is_upload = False

                logger.info("Establishing MinIO connection...")
                minio_client = establish_minio_connection(cluster_configuration)

                for (source, target) in inputs:
                    tarfilename = path_to_tarfilename(source)

                    if is_upload:
                        logger.info(f"Tar.gz input {source} into {tarfilename}...")
                        make_tarfile(tarfilename, source)

                        logger.info(
                            f'Uploading {tarfilename} to {cluster_configuration["minio-bucket"]}/{cluster_configuration["minio-job-folder"]}/{job_name}/{tarfilename}...'
                        )
                        upload_to_minio(
                            tarfilename,
                            cluster_configuration["minio-bucket"],
                            cluster_configuration["minio-job-folder"],
                            job_name,
                            minio_client,
                        )
                    else:
                        logger.info(
                            f'Downloading {cluster_configuration["minio-bucket"]}/{cluster_configuration["minio-job-folder"]}/{job_name}/{tarfilename} to {tarfilename}...'
                        )
                        download_from_minio(
                            tarfilename,
                            cluster_configuration["minio-bucket"],
                            cluster_configuration["minio-job-folder"],
                            job_name,
                            minio_client,
                            remove_minio_files,
                        )

                        logger.info(f"Extracting {tarfilename} to {target}...")
                        extract_tarfile(tarfilename, target)

                        logger.info("Result:")
                        logger.info(os.listdir(target))

            def generate_unique_job_name(model_name):
                epoch = datetime.today().strftime("%Y%m%d%H%M%S")
                return f"job-{model_name}-{epoch}"

            def get_current_namespace():
                SA_NAMESPACE = "/var/run/secrets/kubernetes.io/serviceaccount/namespace"
                with open(SA_NAMESPACE) as f:
                    return f.read()

            def initialize_namespace(namespace):
                if namespace == "":
                    namespace = get_current_namespace()
                namespace_spec = f"namespace: {namespace}"

                return (namespace, namespace_spec)

            def initialize_nodeselector(node_selector):
                if node_selector != "":
                    node_selector = f"nodeSelector:\n        {node_selector}"
                return node_selector

            def initialize_init_container(
                base_image,
                cluster_configuration,
                inputs,
                is_remote,
                job_name,
                minio_secret,
                mount_path,
            ):
                if not is_remote:
                    return ""

                command_specification = kfp.components.func_to_component_text(
                    func=sync_with_minio
                )

                # inner components loose type information as needed by lists/dicts
                # -> cluster_configuration & inputs need to be a string (using json)
                cluster_configuration_json = json.dumps(
                    {
                        "minio-bucket": cluster_configuration["minio-bucket"],
                        "minio-job-folder": cluster_configuration["minio-job-folder"],
                        "minio-url": cluster_configuration["minio-url"],
                    }
                )
                inputs_json = json.dumps(inputs)
                parameters = {
                    "cluster_configuration": cluster_configuration_json,
                    "inputs": inputs_json,
                    "job_name": job_name,
                    "is_upload": "False",
                }

                command, _, _, _ = initialize_command(command_specification, parameters)

                init_container = f"""initContainers:
                  - name: init-inputs
                    image: {base_image}
                    command: {command}
                    volumeMounts:
                    - mountPath: {mount_path}
                      name: training
                    env:
                    - name: MINIO_USER
                      valueFrom:
                        secretKeyRef:
                          name: {minio_secret}
                          key: accesskey
                          optional: false
                    - name: MINIO_PASS
                      valueFrom:
                        secretKeyRef:
                          name: {minio_secret}
                          key: secretkey
                          optional: false
        """
                return init_container

            def initialize_command(
                specification,
                parameters,
                path_parameters = {},
                mount_path = "/tmp",
            ):
                component_yaml = yaml.safe_load(specification)
                container_yaml = component_yaml["implementation"]["container"]
                training_base_image = container_yaml["image"]
                command = container_yaml["command"]
                args = container_yaml["args"]

                actual_args = list()
                inputs = list()
                outputs = list()
                for idx, arg in enumerate(args):
                    if type(arg) is dict:
                        if "inputValue" in arg:
                            # required parameter (value)
                            key = arg["inputValue"]
                            if key in parameters:
                                actual_args.append(parameters[key])
                            else:
                                err = f"Required parameter '{key}' missing in component input!"
                                raise Exception(err)
                        elif "if" in arg:
                            # optional parameter
                            key = arg["if"]["cond"]["isPresent"]
                            if key in parameters:
                                actual_args.append(f"--{key}")
                                actual_args.append(parameters[key])
                        elif "inputPath" in arg:
                            # required InputPath
                            key = arg["inputPath"]
                            if key in parameters:
                                path_key = parameters[key]
                                if path_key in path_parameters:
                                    mount = f"{mount_path}{path_parameters[path_key]}"
                                    inputs.append((path_parameters[path_key], mount))
                                    actual_args.append(mount)
                                else:
                                    err = f"InputPath '{path_key}' unavailable in training component!"
                                    raise Exception(err)
                            else:
                                err = f"Required parameter '{key}' missing in component input!"
                                raise Exception(err)
                        elif "outputPath" in arg:
                            # required OutputPath
                            key = arg["outputPath"]
                            if key in parameters:
                                path_key = parameters[key]
                                if path_key in path_parameters:
                                    mount = f"{mount_path}{path_parameters[path_key]}"
                                    outputs.append((mount, path_parameters[path_key]))
                                    actual_args.append(mount)
                                else:
                                    err = f"OutputPath '{path_key}' unavailable in training component!"
                                    raise Exception(err)
                            else:
                                err = f"Required parameter '{key}' missing in component input!"
                                raise Exception(err)
                    else:
                        # required parameter (key)
                        actual_args.append(arg)

                command_with_initialized_args = json.dumps(command + actual_args)

                return command_with_initialized_args, inputs, outputs, training_base_image

            def initialize_fetch_command(
                cluster_configuration,
                job_name,
                outputs,
            ):
                command_specification = kfp.components.func_to_component_text(
                    func=sync_with_minio
                )

                # inner components loose type information as needed by lists/dicts
                # -> cluster_configuration & inputs need to be a string (using json)
                cluster_configuration_json = json.dumps(
                    {
                        "minio-bucket": cluster_configuration["minio-bucket"],
                        "minio-job-folder": cluster_configuration["minio-job-folder"],
                        "minio-url": cluster_configuration["minio-url"],
                    }
                )
                outputs_json = json.dumps(outputs)
                parameters = {
                    "cluster_configuration": cluster_configuration_json,
                    "inputs": outputs_json,
                    "job_name": job_name,
                    "is_upload": "True",
                }
                command, _, _, _ = initialize_command(command_specification, parameters)
                return command

            def create_pvc_spec(pvc_name, namespace_spec, access_mode, pvc_size):
                pvc_spec = f"""apiVersion: batch/v1
        apiVersion: v1
        kind: PersistentVolumeClaim
        metadata:
          name: {pvc_name}
          {namespace_spec}
        spec:
          accessModes:
          - {access_mode}
          resources:
            requests:
              storage: {pvc_size}
        """
                return yaml.safe_load(pvc_spec)

            def create_minio_secret_spec(cluster_configuration, minio_secret, namespace_spec):
                minio_secret_spec = f"""apiVersion: v1
        kind: Secret
        metadata:
          name: {minio_secret}
          {namespace_spec}
        stringData:
          accesskey: {cluster_configuration["minio-accesskey"]}
          secretkey: {cluster_configuration["minio-secretkey"]}
        """
                return yaml.safe_load(minio_secret_spec)

            def create_train_job_configuration(
                job_name,
                namespace_spec,
                node_selector,
                base_image,
                train_command,
                train_mount,
                cpus,
                memory,
                gpus,
                init_container,
                pvc_name,
                distribution_specification,
                minio_url,
                minio_secret,
            ):
                if cpus:
                    cpu_spec = f"cpu: {cpus}"
                else:
                    cpu_spec = ""

                if memory:
                    memory_spec = f"memory: {memory}"
                else:
                    memory_spec = ""

                if gpus:
                    gpu_spec = f"nvidia.com/gpu: {gpus}"
                else:
                    gpu_spec = ""

                if distribution_specification is None:
                    distribution_specification = dict()

                if "distribution_type" not in distribution_specification:
                    distribution_specification["distribution_type"] = "Job"

                if gpus < 1:
                    slots_per_worker = 1
                else:
                    slots_per_worker = gpus

                if "number_of_workers" in distribution_specification:
                    number_of_workers = distribution_specification["number_of_workers"]
                else:
                    number_of_workers = 2

                number_of_processes = number_of_workers * slots_per_worker

                if "launcher_cpus" in distribution_specification:
                    launcher_cpu_spec = f"cpu: {distribution_specification['launcher_cpus']}"
                else:
                    launcher_cpu_spec = ""

                if "launcher_memory" in distribution_specification:
                    launcher_memory_spec = (
                        f"memory: {distribution_specification['launcher_memory']}"
                    )
                else:
                    launcher_memory_spec = ""

                if "worker_cpus" in distribution_specification:
                    worker_cpu_spec = f"cpu: {distribution_specification['worker_cpus']}"
                else:
                    worker_cpu_spec = ""

                if "worker_memory" in distribution_specification:
                    worker_memory_spec = (
                        f"memory: {distribution_specification['worker_memory']}"
                    )
                else:
                    worker_memory_spec = ""

                if distribution_specification["distribution_type"] == "Job":
                    job_spec = f"""apiVersion: batch/v1
        kind: Job
        metadata:
          name: {job_name}
          labels:
            train-model-job: {job_name}
          {namespace_spec}
        spec:
          template:
            metadata:
              annotations:
                sidecar.istio.io/inject: "false"
            spec:
              {node_selector}
              containers:
                - name: training-container
                  image: {base_image}
                  command: {train_command}
                  volumeMounts:
                    - mountPath: {train_mount}
                      name: training
                  restartPolicy: Never
                  env:
                    - name: S3_ENDPOINT
                      value: {minio_url}
                    - name: AWS_ACCESS_KEY_ID
                      valueFrom:
                        secretKeyRef:
                          name: {minio_secret}
                          key: accesskey
                          optional: false
                    - name: AWS_SECRET_ACCESS_KEY
                      valueFrom:
                        secretKeyRef:
                          name: {minio_secret}
                          key: secretkey
                          optional: false
                    - name: AWS_S3_SIGNATURE_VERSION
                      value: "s3v4"
                    - name: JOB_NAME
                      valueFrom:
                        fieldRef:
                          fieldPath: metadata.name
                  resources:
                    limits:
                      {cpu_spec}
                      {memory_spec}
                      {gpu_spec}
              {init_container}
              volumes:
                - name: training
                  persistentVolumeClaim:
                    claimName: {pvc_name}
              restartPolicy: Never
        """
                    job_config = {
                        "group": "batch",
                        "version": "v1",
                        "plural": "jobs",
                        "label": "job-name",
                    }
                elif distribution_specification["distribution_type"] == "MPI":
                    job_spec = f"""apiVersion: kubeflow.org/v1
        kind: MPIJob
        metadata:
          name: {job_name}
          labels:
            train-model-job: {job_name}
          {namespace_spec}
        spec:
          slotsPerWorker: {slots_per_worker}
          runPolicy:
            cleanPodPolicy: Running
          mpiReplicaSpecs:
            Launcher:
              replicas: 1
              template:
                metadata:
                  annotations:
                    sidecar.istio.io/inject: "false"
                spec:
                  {init_container}
                  volumes:
                    - name: training
                      persistentVolumeClaim:
                        claimName: {pvc_name}
                  containers:
                  - image: {base_image}
                    name: mpi-launcher
                    command:
                    - mpirun
                    - -np
                    - "{number_of_processes}"
                    - --allow-run-as-root
                    - -bind-to
                    - none
                    - -map-by
                    - slot
                    - --prefix
                    - /opt/conda
                    - -mca
                    - pml
                    - ob1
                    - -mca
                    - btl
                    - ^openib
                    - -x
                    - NCCL_DEBUG=INFO
                    args: {train_command}
                    resources:
                      limits:
                        {launcher_cpu_spec}
                        {launcher_memory_spec}
            Worker:
              replicas: {number_of_workers}
              template:
                metadata:
                  annotations:
                    sidecar.istio.io/inject: "false"
                spec:
                  {node_selector}
                  containers:
                  - image: {base_image}
                    name: mpi-worker
                    env:
                    - name: S3_ENDPOINT
                      value: {minio_url}
                    - name: AWS_ACCESS_KEY_ID
                      valueFrom:
                        secretKeyRef:
                          name: {minio_secret}
                          key: accesskey
                          optional: false
                    - name: AWS_SECRET_ACCESS_KEY
                      valueFrom:
                        secretKeyRef:
                          name: {minio_secret}
                          key: secretkey
                          optional: false
                    - name: AWS_S3_SIGNATURE_VERSION
                      value: "s3v4"
                    - name: JOB_NAME
                      valueFrom:
                        fieldRef:
                          fieldPath: metadata.name
                    volumeMounts:
                      - mountPath: /train
                        name: training
                    resources:
                      limits:
                        {worker_cpu_spec}
                        {worker_memory_spec}
                        {gpu_spec}
                  volumes:
                    - name: training
                      persistentVolumeClaim:
                        claimName: {pvc_name}
        """
                    job_config = {
                        "group": "kubeflow.org",
                        "version": "v1",
                        "plural": "mpijobs",
                        "label": "training.kubeflow.org/replica-type=launcher,training.kubeflow.org/job-name",
                    }
                elif distribution_specification["distribution_type"] == "TF":
                    job_spec = f"""apiVersion: kubeflow.org/v1
        kind: TFJob
        metadata:
          name: {job_name}
          labels:
            train-model-job: {job_name}
          {namespace_spec}
        spec:
          runPolicy:
            cleanPodPolicy: None
          tfReplicaSpecs:
            Worker:
              replicas: {number_of_workers}
              restartPolicy: OnFailure
              template:
                metadata:
                  annotations:
                    sidecar.istio.io/inject: "false"
                spec:
                  {node_selector}
                  containers:
                    - name: tensorflow
                      image: {base_image}
                      command: {train_command}
                      env:
                      - name: S3_ENDPOINT
                        value: {minio_url}
                      - name: AWS_ACCESS_KEY_ID
                        valueFrom:
                          secretKeyRef:
                            name: {minio_secret}
                            key: accesskey
                            optional: false
                      - name: AWS_SECRET_ACCESS_KEY
                        valueFrom:
                          secretKeyRef:
                            name: {minio_secret}
                            key: secretkey
                            optional: false
                      - name: AWS_S3_SIGNATURE_VERSION
                        value: "s3v4"
                      - name: JOB_NAME
                        valueFrom:
                          fieldRef:
                            fieldPath: metadata.name
                      volumeMounts:
                        - mountPath: /train
                          name: training
                      resources:
                        limits:
                          {worker_cpu_spec}
                          {worker_memory_spec}
                          {gpu_spec}
                  volumes:
                    - name: training
                      persistentVolumeClaim:
                        claimName: {pvc_name}
        """
                    job_config = {
                        "group": "kubeflow.org",
                        "version": "v1",
                        "plural": "tfjobs",
                        "label": "tf-job-name",
                    }
                else:
                    err = f"Job failed while executing - unknown distribution_type: {distribution_specification['distribution_type']}"
                    raise Exception(err)

                job_config["job_spec"] = yaml.safe_load(job_spec)
                return job_config

            def create_fetch_job_configuration(
                job_name,
                namespace_spec,
                base_image,
                fetch_command,
                train_mount,
                minio_secret,
                pvc_name,
            ):
                job_spec = f"""apiVersion: batch/v1
        kind: Job
        metadata:
          name: {job_name}
          labels:
            train-model-job: {job_name}
          {namespace_spec}
        spec:
          template:
            metadata:
              annotations:
                sidecar.istio.io/inject: "false"
            spec:
              containers:
                - name: training-container
                  image: {base_image}
                  command: {fetch_command}
                  volumeMounts:
                    - mountPath: {train_mount}
                      name: training
                  restartPolicy: Never
                  env:
                  - name: MINIO_USER
                    valueFrom:
                      secretKeyRef:
                        name: {minio_secret}
                        key: accesskey
                        optional: false
                  - name: MINIO_PASS
                    valueFrom:
                      secretKeyRef:
                        name: {minio_secret}
                        key: secretkey
                        optional: false
              volumes:
                - name: training
                  persistentVolumeClaim:
                    claimName: {pvc_name}
              restartPolicy: Never
        """
                job_config = {
                    "group": "batch",
                    "version": "v1",
                    "plural": "jobs",
                    "job_spec": yaml.safe_load(job_spec),
                    "label": "job-name",
                }
                return job_config

            def submit_and_monitor_job(
                api_client, job_config, namespace, additional_job_resources=[]
            ):
                job_spec = job_config["job_spec"]
                job_resource = custom_object_api.create_namespaced_custom_object(
                    group=job_config["group"],
                    version=job_config["version"],
                    namespace=namespace,
                    plural=job_config["plural"],
                    body=job_spec,
                )
                job_name = job_resource["metadata"]["name"]
                job_uid = job_resource["metadata"]["uid"]

                logger.info("Creating additional job resource...")
                if additional_job_resources:
                    for resource in additional_job_resources:
                        resource["metadata"]["ownerReferences"] = [
                            {
                                "apiVersion": job_spec["apiVersion"],
                                "kind": job_spec["kind"],
                                "name": job_name,
                                "uid": job_uid,
                            }
                        ]
                    utils.create_from_yaml(api_client, yaml_objects=additional_job_resources)

                logger.info("Waiting for job to succeed...")
                job_is_monitored = False
                pods_being_monitored = set()
                job_watch = watch.Watch()
                for job_event in job_watch.stream(
                    custom_object_api.list_namespaced_custom_object,
                    group=job_config["group"],
                    version=job_config["version"],
                    plural=job_config["plural"],
                    namespace=namespace,
                    label_selector=f"train-model-job={job_name}",
                    timeout_seconds=0,
                ):
                    logger.info(f"job_event: {job_event}")
                    job = job_event["object"]
                    if "status" not in job and "items" in job:
                        job = job["items"][0]

                    if "status" not in job:
                        logger.info("Skipping event (no status information found)...")
                        continue

                    job_status = dict()
                    if "active" in job["status"]:
                        job_status["active"] = job["status"]["active"]
                    else:
                        job_status["active"] = 0
                    if "completionTime" in job["status"]:
                        job_status["completionTime"] = job["status"]["completionTime"]
                    if "failed" in job["status"]:
                        job_status["failed"] = job["status"]["failed"]
                    else:
                        job_status["failed"] = 0
                    if "ready" in job["status"]:
                        job_status["ready"] = job["status"]["ready"]
                    else:
                        job_status["ready"] = 0
                    if "startTime" in job["status"]:
                        job_status["startTime"] = job["status"]["startTime"]
                    if "succeeded" in job["status"]:
                        job_status["succeeded"] = job["status"]["succeeded"]
                    else:
                        job_status["succeeded"] = 0

                    # MPI
                    job_status["Complete"] = "False"
                    job_status["Created"] = "False"
                    job_status["Failed"] = "False"
                    job_status["Running"] = "False"
                    job_status["Succeeded"] = "False"
                    if "conditions" in job["status"]:
                        for condition in job["status"]["conditions"]:
                            job_status[condition["type"]] = condition["status"]

                    logger.info(f"Job status: {job_status}")

                    def start_monitoring(job_name, job_status):
                        return (not job_is_monitored) and (
                            job_status["active"] > 0
                            or job_status["Running"] == "True"
                            or job_status["failed"] > 0
                            or job_status["Failed"] == "True"
                            or job_status["ready"] > 0
                            or job_status["Complete"] == "True"
                            or job_status["Succeeded"] == "True"
                        )

                    if start_monitoring(job_name, job_status):
                        job_is_monitored = True
                        logger.info("Monitoring pods of job...")

                        # See https://stackoverflow.com/questions/65938572/kubernetes-python-client-equivalent-of-kubectl-wait-for-command
                        pod_watch = watch.Watch()
                        for pod_event in pod_watch.stream(
                            func=core_api.list_namespaced_pod,
                            namespace=namespace,
                            label_selector=f"{job_config['label']}={job_name}",
                            timeout_seconds=0,
                        ):
                            pod = pod_event["object"]
                            pod_name = pod.metadata.name

                            logger.info(
                                f"Pod {pod_name}: {pod_event['type']} - {pod.status.phase}"
                            )

                            if pod_name in pods_being_monitored:
                                pod_watch.stop()
                            elif pod_name not in pods_being_monitored and (
                                pod.status.phase == "Running"
                                or pod.status.phase == "Succeeded"
                                or pod.status.phase == "Failed"
                            ):
                                pods_being_monitored.add(pod_name)
                                logger.info(
                                    "=============================================================================="
                                )
                                logger.info(
                                    "=============================================================================="
                                )
                                logger.info(f"=== Streaming logs of pod {pod_name}...")
                                logger.info(
                                    "=============================================================================="
                                )
                                logger.info(
                                    "=============================================================================="
                                )

                                log_watch = watch.Watch()
                                for log_event in log_watch.stream(
                                    core_api.read_namespaced_pod_log,
                                    name=pod_name,
                                    namespace=namespace,
                                    follow=True,
                                    _return_http_data_only=True,
                                    _preload_content=False,
                                ):
                                    print(log_event)
                                logger.info(
                                    "=============================================================================="
                                )
                                logger.info(
                                    "=============================================================================="
                                )

                                pod_watch.stop()

                                if pod.status.phase == "Failed":
                                    err = "Job failed while executing."
                                    raise Exception(err)
                                break
                            if pod_event["type"] == "DELETED":
                                err = "Pod was deleted while we where waiting for it to start."
                                raise Exception(err)
                    elif (
                        job_status["succeeded"] > 0
                        or job_status["Complete"] == "True"
                        or job_status["Succeeded"] == "True"
                    ):
                        job_watch.stop()
                        logger.info("Job finished successfully.")
                        break
                    elif not (job_status["active"] > 0 or job_status["Running"] == "True") and (
                        job_status["failed"] > 0 or job_status["Failed"] == "True"
                    ):
                        job_watch.stop()
                        raise Exception("Job failed!")
                    else:
                        logger.info(f"Waiting for job updates. Current status: {job_status}")

            ###########################################################################
            # Main Workflow
            ###########################################################################

            logger.info("Establishing local cluster connection...")
            local_api_client = establish_local_cluster_connection()

            logger.info("Receiving training cluster configuration...")
            cluster_configuration = get_cluster_configuration(
                local_api_client, cluster_configuration_secret
            )

            logger.info("Establishing training cluster connection...")
            api_client, is_remote = establish_training_cluster_connection(
                local_api_client, cluster_configuration
            )
            batch_api = client.BatchV1Api(api_client)
            core_api = client.CoreV1Api(api_client)
            custom_object_api = client.CustomObjectsApi(api_client)

            logger.info("Initializing resources...")
            job_name = generate_unique_job_name(model_name)
            job_minio_secret = f"{job_name}-minio-secret"
            namespace, namespace_spec = initialize_namespace(
                cluster_configuration["remote-namespace"]
            )
            pvc_name = f"{job_name}-pvc"
            node_selector = initialize_nodeselector(node_selector)

            path_parameters = {
                "train_dataset_dir": train_dataset_dir,
                "validation_dataset_dir": validation_dataset_dir,
                "model_dir": model_dir,
            }
            train_command, inputs, outputs, base_image = initialize_command(
                train_specification, train_parameters, path_parameters, train_mount
            )

            init_container = initialize_init_container(
                base_image,
                cluster_configuration,
                inputs,
                is_remote,
                job_name,
                job_minio_secret,
                train_mount,
            )

            logger.info("=======================================")
            logger.info("Derived configurations")
            logger.info("=======================================")
            logger.info(f"job_name: {job_name}")
            logger.info(f"namespace: {namespace}")
            logger.info(f"is_remote: {is_remote}")
            logger.info(f"minio_url: {cluster_configuration['minio-url']}")
            logger.info(f"job_minio_secret: {job_minio_secret}")
            logger.info("inputs (input paths send to job):")
            for source, target in inputs:
                logger.info(
                    f"- {source} -> {cluster_configuration['minio-bucket']}/{cluster_configuration['minio-job-folder']}/{job_name}/{target}"
                )
            logger.info("outputs (output paths returning from job):")
            for source, target in outputs:
                logger.info(
                    f"- {target} <- {cluster_configuration['minio-bucket']}/{cluster_configuration['minio-job-folder']}/{job_name}/{source}"
                )
            logger.info(f"distribution_specification: {distribution_specification}")
            logger.info(f"train_command: {train_command}")
            logger.info("=======================================")

            additional_job_resources = []

            if is_remote:
                logger.info("Using MinIO to sync data with a new remote PVC for the job...")
                sync_with_minio(cluster_configuration, inputs, job_name, is_upload=True)
                additional_job_resources.append(
                    create_pvc_spec(
                        pvc_name, namespace_spec, cluster_configuration["access-mode"], pvc_size
                    )
                )
                additional_job_resources.append(
                    create_minio_secret_spec(
                        cluster_configuration, job_minio_secret, namespace_spec
                    )
                )
            else:
                logger.info(
                    f"Pushing inputs to local {train_mount} mount as shared with job environment..."
                )
                for (source, target) in inputs:
                    clone_path(source, target)

            logger.info("Creating train job configuration...")
            train_job_config = create_train_job_configuration(
                job_name,
                namespace_spec,
                node_selector,
                base_image,
                train_command,
                train_mount,
                cpus,
                memory,
                gpus,
                init_container,
                pvc_name,
                distribution_specification,
                cluster_configuration["minio-url"],
                job_minio_secret,
            )

            logger.info(f"Starting train job '{namespace}.{job_name}'...")
            submit_and_monitor_job(
                api_client,
                train_job_config,
                namespace,
                additional_job_resources,
            )

            logger.info("Receiving training outputs...")
            if not os.path.exists(model_dir):
                os.makedirs(model_dir)
            if is_remote:
                fetch_command = initialize_fetch_command(
                    cluster_configuration, job_name, outputs
                )
                fetch_job_name = f"{job_name}-fetch"

                logger.info("Creating fetch job configuration...")
                fetch_job_config = create_fetch_job_configuration(
                    fetch_job_name,
                    namespace_spec,
                    base_image,
                    fetch_command,
                    train_mount,
                    job_minio_secret,
                    pvc_name,
                )

                logger.info(f"Starting fetch job '{namespace}.{fetch_job_name}'...")
                submit_and_monitor_job(api_client, fetch_job_config, namespace)

                logger.info("Fetching output data from MinIO & deleting it afterwards...")
                sync_with_minio(
                    cluster_configuration,
                    outputs,
                    job_name,
                    is_upload=False,
                    remove_minio_files=True,
                )

                logger.info(f"Deleting Job {fetch_job_name}...")
                batch_api.delete_namespaced_job(fetch_job_name, namespace)
            else:
                logger.info(
                    f"Fetching outputs to local {train_mount} mount as shared with job environment..."
                )
                for (source, target) in outputs:
                    clone_path(source, target)

            logger.info(f"Deleting Job {job_name}...")
            custom_object_api.delete_namespaced_custom_object(
                train_job_config["group"],
                train_job_config["version"],
                namespace,
                train_job_config["plural"],
                job_name,
            )

            logger.info("Finished.")

        import json
        import argparse
        _parser = argparse.ArgumentParser(prog='Train model job', description='Trains a model. Once trained, the model is persisted to model_dir.')
        _parser.add_argument("--train-dataset-dir", dest="train_dataset_dir", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--validation-dataset-dir", dest="validation_dataset_dir", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--train-specification", dest="train_specification", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--train-parameters", dest="train_parameters", type=json.loads, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--train-mount", dest="train_mount", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--model-name", dest="model_name", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--node-selector", dest="node_selector", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--pvc-name", dest="pvc_name", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--pvc-size", dest="pvc_size", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--cpus", dest="cpus", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--gpus", dest="gpus", type=int, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--memory", dest="memory", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--cluster-configuration-secret", dest="cluster_configuration_secret", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--distribution-specification", dest="distribution_specification", type=json.loads, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--model-dir", dest="model_dir", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = train_model_job(**_parsed_args)
      image: quay.io/ibm/kubeflow-notebook-image-ppc64le:elyra3.14.1-py3.9-tf2.10.1-pt1.10.2-v0
      volumeMounts:
      - {mountPath: /tmp/inputs/train_dataset_dir, name: data-storage, subPath: '{{inputs.parameters.preprocess-dataset-train_dataset_dir-subpath}}',
        readOnly: true}
      - {mountPath: /tmp/inputs/validation_dataset_dir, name: data-storage, subPath: '{{inputs.parameters.preprocess-dataset-validation_dataset_dir-subpath}}',
        readOnly: true}
      - {mountPath: /tmp/outputs/model_dir, name: data-storage, subPath: 'artefacts/{{workflow.uid}}_{{pod.name}}/train-model-job-model_dir'}
    inputs:
      parameters:
      - {name: cluster_configuration_secret}
      - {name: epochs}
      - {name: model_name}
      - {name: number_of_workers}
      - {name: training_gpus}
      - {name: training_node_selector}
      - {name: preprocess-dataset-train_dataset_dir-subpath}
      - {name: preprocess-dataset-validation_dataset_dir-subpath}
    outputs:
      parameters:
      - {name: train-model-job-model_dir-subpath, value: 'artefacts/{{workflow.uid}}_{{pod.name}}/train-model-job-model_dir'}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Trains
          a model. Once trained, the model is persisted to model_dir.", "implementation":
          {"container": {"args": ["--train-dataset-dir", {"inputPath": "train_dataset_dir"},
          "--validation-dataset-dir", {"inputPath": "validation_dataset_dir"}, "--train-specification",
          {"inputValue": "train_specification"}, "--train-parameters", {"inputValue":
          "train_parameters"}, {"if": {"cond": {"isPresent": "train_mount"}, "then":
          ["--train-mount", {"inputValue": "train_mount"}]}}, {"if": {"cond": {"isPresent":
          "model_name"}, "then": ["--model-name", {"inputValue": "model_name"}]}},
          {"if": {"cond": {"isPresent": "node_selector"}, "then": ["--node-selector",
          {"inputValue": "node_selector"}]}}, {"if": {"cond": {"isPresent": "pvc_name"},
          "then": ["--pvc-name", {"inputValue": "pvc_name"}]}}, {"if": {"cond": {"isPresent":
          "pvc_size"}, "then": ["--pvc-size", {"inputValue": "pvc_size"}]}}, {"if":
          {"cond": {"isPresent": "cpus"}, "then": ["--cpus", {"inputValue": "cpus"}]}},
          {"if": {"cond": {"isPresent": "gpus"}, "then": ["--gpus", {"inputValue":
          "gpus"}]}}, {"if": {"cond": {"isPresent": "memory"}, "then": ["--memory",
          {"inputValue": "memory"}]}}, {"if": {"cond": {"isPresent": "cluster_configuration_secret"},
          "then": ["--cluster-configuration-secret", {"inputValue": "cluster_configuration_secret"}]}},
          {"if": {"cond": {"isPresent": "distribution_specification"}, "then": ["--distribution-specification",
          {"inputValue": "distribution_specification"}]}}, "--model-dir", {"outputPath":
          "model_dir"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef train_model_job(\n    train_dataset_dir,\n    validation_dataset_dir,\n    train_specification,\n    train_parameters,\n    model_dir,\n    train_mount
          = \"/train\",\n    model_name = \"my-model\",\n    node_selector = \"\",\n    pvc_name
          = \"\",\n    pvc_size = \"10Gi\",\n    cpus = \"\",\n    gpus = 0,\n    memory
          = \"\",\n    cluster_configuration_secret = \"\",\n    distribution_specification
          = None,\n):\n    \"\"\"\n    Trains a model. Once trained, the model is
          persisted to model_dir.\n\n            Parameters:\n                    train_dataset_dir:
          Path to the directory with training data.\n                    validation_dataset_dir:
          Path to the directory with validation data to be used during training.\n                    train_specification:
          Training command as generated from a Python function using kfp.components.func_to_component_text.\n                    train_parameters:
          Dictionary mapping formal to actual parameters for the training spacification.\n                    model_dir:
          Target path where the model will be stored.\n                    train_mount:
          Optional mounting point for training data of an existing PVC. Example: \"/train\".\n                    model_name:
          Optional name of the model. Must be unique for the targeted namespace and
          conform Kubernetes naming conventions. Example: my-model.\n                    node_selector:
          Optional node selector for worker nodes. Example: nvidia.com/gpu.product:
          \"Tesla-V100-SXM2-32GB\".\n                    pvc_name: Optional name to
          an existing persistent volume claim (pvc). If given, this pvc is mounted
          into the training job. Example: \"music-genre-classification-j4ssf-training-pvc\".\n                    pvc_size:
          Optional size of the storage during model training. Storage is mounted into
          to the Job based on a persitent volume claim of the given size. Example:
          10Gi.\n                    cpus: Optional CPU limit for the job. Leave empty
          for cluster defaults (typically no limit). Example: \"1000m\".\n                    gpus:
          Optional number of GPUs for the job. Example: 2.\n                    memory:
          Optional memory limit for the job. Leave empty for cluster defaults (typically
          no limit). Example: \"1Gi\".\n                    cluster_configuration_secret:
          Optional secret name configuring a (remote) Kubernetes cluster to run the
          job in and the backing MinIO object store. All secret''s data values are
          optional and appropriate defaults are chosen if not present. The secret
          may provide a suitable kubernetes bearer token, the associated namespace,
          a host, etc. Example: \"remote-power-cluster\".\n                    distribution_specification:
          Optional dictionary specifiying the distribution behavior. By default, no
          distributed training is executed, which results in an ordinary Kubernetes
          Job  for training. Otherwise, dictionary entries determine the distribution
          behavior. The \"distribution_type\" entry determines the distribution type:
          \"Job\" (no distribution; ordinary Kubernetes job), \"MPI\" (all-reduce
          style distribution via Horovod), or \"TF\" (parameter-server style distribution
          via distributed training with TensorFlow). Depending on the distribution
          type, additional dictionary entries can be processed. For distributed training
          jobs, the \"number_of_workers\" (e.g., 2) determines the number of worker
          replicas for training. Individual resource limits can be controlled via
          \"worker_cpus\" (e.g., \"1000m\") and \"worker_memory\" (e.g., \"1Gi\").
          MPI additionally provides a fine-grained control of launcher cpu and memory
          limits via \"launcher_cpus\" (e.g., \"1000m\") and \"launcher_memory\" (e.g.,
          \"1Gi\"). Full example with MPI: {\"distribution_type\": \"MPI\", \"number_of_workers\":
          2, \"worker_cpus\": \"8\", \"worker_memory\": \"32Gi\", \"launcher_cpus\":
          \"2\", \"launcher_memory\": \"8Gi\"}\n    \"\"\"\n    from datetime import
          datetime\n    import errno\n    import json\n    import kfp\n    from kubernetes
          import client, config, utils, watch\n    import logging\n    import os\n    import
          shutil\n    import sys\n    import yaml\n\n    logging.basicConfig(\n        stream=sys.stdout,\n        level=logging.INFO,\n        format=\"%(levelname)s
          %(asctime)s: %(message)s\",\n    )\n    logger = logging.getLogger()\n\n    ###########################################################################\n    #
          Helper Functions\n    ###########################################################################\n\n    def
          establish_local_cluster_connection():\n        config.load_incluster_config()\n        return
          client.ApiClient()\n\n    def get_cluster_configuration(api_client, cluster_configuration_secret):\n        import
          base64\n        from kubernetes.client.rest import ApiException\n\n        def
          decode(secret, key):\n            data = secret.data[key]\n            decoded_data
          = base64.b64decode(data)\n            return decoded_data.decode(\"utf-8\")\n\n        def
          update_with_secret(secret, dictionary):\n            for key in dictionary:\n                if
          key in secret.data:\n                    dictionary[key] = decode(secret,
          key)\n\n        cluster_configuration = {\n            \"access-mode\":
          \"ReadWriteMany\",\n            \"minio-accesskey\": \"minio\",\n            \"minio-bucket\":
          \"mlpipeline\",\n            \"minio-job-folder\": \"jobs\",\n            \"minio-secretkey\":
          \"minio123\",\n            \"minio-url\": \"http://minio-service.kubeflow:9000\",\n            \"remote-host\":
          \"\",\n            \"remote-namespace\": \"\",\n            \"remote-token\":
          \"\",\n        }\n\n        try:\n            default_minio_secret = client.CoreV1Api(api_client).read_namespaced_secret(\n                \"mlpipeline-minio-artifact\",
          get_current_namespace()\n            )\n\n            if default_minio_secret.data
          is None:\n                logger.info(\n                    \"MinIO secret
          (mlpipeline-minio-artifact) includes no data - progressing with default
          values.\"\n                )\n            else:\n                logger.info(\n                    \"Found
          default MinIO secret (mlpipeline-minio-artifact) - updating cluster configuration
          accordingly.\"\n                )\n                cluster_configuration[\"minio-accesskey\"]
          = decode(\n                    default_minio_secret, \"accesskey\"\n                )\n                cluster_configuration[\"minio-secretkey\"]
          = decode(\n                    default_minio_secret, \"secretkey\"\n                )\n        except
          ApiException as e:\n            if e.status == 404:\n                logger.info(\n                    \"Found
          no default MinIO secret (mlpipeline-minio-artifact) - progressing with default
          values.\"\n                )\n\n        if cluster_configuration_secret
          == \"\":\n            logger.info(\n                \"No cluster configuration
          secret specified - progressing with default values.\"\n            )\n            return
          cluster_configuration\n\n        try:\n            secret = client.CoreV1Api(api_client).read_namespaced_secret(\n                cluster_configuration_secret,
          get_current_namespace()\n            )\n            if secret.data is None:\n                logger.info(\n                    f\"Cluster
          configuration secret ({cluster_configuration_secret}) includes no data -
          progressing with default values.\"\n                )\n            else:\n                logger.info(\n                    f\"Found
          cluster configuration secret ({cluster_configuration_secret}) - updating
          cluster configuration accordingly.\"\n                )\n                update_with_secret(secret,
          cluster_configuration)\n        except ApiException as e:\n            if
          e.status == 404:\n                logger.info(\n                    f\"Found
          no cluster configuration secret ({cluster_configuration_secret}) - progressing
          with default values.\"\n                )\n\n        return cluster_configuration\n\n    def
          establish_training_cluster_connection(local_api_client, cluster_configuration):\n        is_remote
          = False\n        if (\n            cluster_configuration[\"remote-host\"]
          == \"\"\n            or cluster_configuration[\"remote-token\"] == \"\"\n        ):\n            logger.info(\n                \"Remote
          cluster not configured. Using in-cluster configuration...\"\n            )\n            logger.info(\n                \"Note:
          assign the name of a secret to the ''cluster_configuration_secret'' pipeline
          argument and add the secret to your cluster.\"\n            )\n            logger.info(\"Example
          secret:\")\n            logger.info(\"---\")\n            logger.info(\"apiVersion:
          v1\")\n            logger.info(\"kind: Secret\")\n            logger.info(\"metadata:\")\n            logger.info(\"  name:
          my-remote-cluster\")\n            logger.info(\"stringData:\")\n            logger.info(\"  access-mode:
          ReadWriteOnce\")\n            logger.info(\"  minio-accesskey: minio\")\n            logger.info(\"  minio-bucket:
          mlpipeline\")\n            logger.info(\"  minio-job-folder: jobs\")\n            logger.info(\"  minio-secretkey:
          minio123\")\n            logger.info(\"  minio-url: http://minio-service.kubeflow:9000\")\n            logger.info(\n                \"  remote-host:
          https://istio-ingressgateway-istio-system.apps.mydomain.ai:6443\"\n            )\n            logger.info(\"  remote-namespace:
          default\")\n            logger.info(\"  remote-token: eyJh...\")\n            logger.info(\"---\")\n            logger.info(\n                \"Where
          you get the remote-token from your remote cluster as described here:\"\n            )\n            logger.info(\n                \"https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/#without-kubectl-proxy\"\n            )\n\n            api_client
          = local_api_client\n            if not os.path.exists(train_mount):\n                logger.warning(\n                    f\"No
          local mount to {train_mount} found. Therefore, switching to remote data
          synchronization mode via MinIO. This will work but is slower compared to
          local mounts. Consider adding a mount to ''{train_mount}'' for this component
          by using a PVC inside your pipeline.\"\n                )\n                is_remote
          = True\n        else:\n            # see: https://github.com/kubernetes-client/python/blob/6d4587e18064288d031ed9bbf5ab5b8245460b3c/examples/remote_cluster.py\n            logger.info(\n                \"Remote
          host and token found. Using remote cluster configuration...\"\n            )\n            configuration
          = client.Configuration()\n            configuration.host = cluster_configuration[\"remote-host\"]\n            configuration.verify_ssl
          = False\n            configuration.api_key = {\n                \"authorization\":
          \"Bearer \" + cluster_configuration[\"remote-token\"]\n            }\n            api_client
          = client.ApiClient(configuration)\n            is_remote = True\n\n        return
          (api_client, is_remote)\n\n    def clone_path(source, target):\n        try:\n            logger.info(f\"Cloning
          source path {source} to {target} of training job...\")\n            shutil.copytree(source,
          target)\n            logger.info(\"Cloning finished. Target path contents:\")\n            logger.info(os.listdir(target))\n        except
          OSError as e:\n            if e.errno in (errno.ENOTDIR, errno.EINVAL):\n                shutil.copy(source,
          target)\n            else:\n                raise\n\n    def sync_with_minio(\n        cluster_configuration,\n        inputs,\n        job_name,\n        is_upload,\n        remove_minio_files
          = False,\n    ):\n        import boto3\n        import botocore\n        from
          botocore.client import Config\n        import json\n        import logging\n        import
          os\n        import sys\n        import tarfile\n\n        logging.basicConfig(\n            stream=sys.stdout,\n            level=logging.INFO,\n            format=\"%(levelname)s
          %(asctime)s: %(message)s\",\n        )\n        logger = logging.getLogger()\n\n        def
          establish_minio_connection(cluster_configuration):\n            if (\"minio-accesskey\"
          in cluster_configuration) and (\n                \"minio-secretkey\" in
          cluster_configuration\n            ):\n                minio_user = cluster_configuration[\"minio-accesskey\"]\n                minio_pass
          = cluster_configuration[\"minio-secretkey\"]\n            else:\n                minio_user
          = os.getenv(\"MINIO_USER\")\n                minio_pass = os.getenv(\"MINIO_PASS\")\n\n            if
          minio_user == \"\" or minio_pass == \"\":\n                err = \"Environment
          variables MINIO_USER and MINIO_PASS need externally to be provided to this
          component using k8s_secret_key_to_env!\"\n                raise Exception(err)\n\n            return
          boto3.session.Session().resource(\n                service_name=\"s3\",\n                endpoint_url=cluster_configuration[\"minio-url\"],\n                aws_access_key_id=minio_user,\n                aws_secret_access_key=minio_pass,\n                config=Config(signature_version=\"s3v4\"),\n            )\n\n        def
          path_to_tarfilename(pathname):\n            return f\"{pathname.replace(os.sep,
          ''-'')}.tar.gz\"\n\n        def make_tarfile(output_filename, source_dir):\n            with
          tarfile.open(output_filename, \"w:gz\") as tar:\n                tar.add(source_dir,
          arcname=\".\")\n\n        # see: https://stackoverflow.com/a/47565719/2625096\n        def
          bucket_exists(minio_client, bucket):\n            try:\n                minio_client.meta.client.head_bucket(Bucket=bucket.name)\n                return
          True\n            except botocore.exceptions.ClientError as e:\n                error_code
          = int(e.response[\"Error\"][\"Code\"])\n                if error_code ==
          403:\n                    # Forbidden Access -> Private Bucket\n                    return
          True\n                elif error_code == 404:\n                    return
          False\n\n        def upload_to_minio(file, upload_bucket, job_folder, job_name,
          minio_client):\n            bucket = minio_client.Bucket(upload_bucket)\n\n            if
          not bucket_exists(minio_client, bucket):\n                minio_client.create_bucket(Bucket=bucket.name)\n\n            bucket.upload_file(file,
          f\"{job_folder}/{job_name}/{file}\")\n\n        def download_from_minio(\n            file,
          upload_bucket, job_folder, job_name, minio_client, remove_minio_file\n        ):\n            bucket
          = minio_client.Bucket(upload_bucket)\n            key = f\"{job_folder}/{job_name}/{file}\"\n\n            bucket.download_file(key,
          file)\n\n            if remove_minio_file:\n                bucket.Object(key).delete()\n\n        def
          extract_tarfile(tarfile_name, target):\n            with tarfile.open(tarfile_name,
          \"r:gz\") as tar_gz_ref:\n                tar_gz_ref.extractall(target)\n\n        if
          isinstance(cluster_configuration, str):\n            cluster_configuration
          = json.loads(cluster_configuration)\n\n        if isinstance(inputs, str):\n            inputs
          = json.loads(inputs)\n\n        if isinstance(is_upload, str):\n            if
          is_upload == \"True\":\n                is_upload = True\n            else:\n                is_upload
          = False\n\n        logger.info(\"Establishing MinIO connection...\")\n        minio_client
          = establish_minio_connection(cluster_configuration)\n\n        for (source,
          target) in inputs:\n            tarfilename = path_to_tarfilename(source)\n\n            if
          is_upload:\n                logger.info(f\"Tar.gz input {source} into {tarfilename}...\")\n                make_tarfile(tarfilename,
          source)\n\n                logger.info(\n                    f''Uploading
          {tarfilename} to {cluster_configuration[\"minio-bucket\"]}/{cluster_configuration[\"minio-job-folder\"]}/{job_name}/{tarfilename}...''\n                )\n                upload_to_minio(\n                    tarfilename,\n                    cluster_configuration[\"minio-bucket\"],\n                    cluster_configuration[\"minio-job-folder\"],\n                    job_name,\n                    minio_client,\n                )\n            else:\n                logger.info(\n                    f''Downloading
          {cluster_configuration[\"minio-bucket\"]}/{cluster_configuration[\"minio-job-folder\"]}/{job_name}/{tarfilename}
          to {tarfilename}...''\n                )\n                download_from_minio(\n                    tarfilename,\n                    cluster_configuration[\"minio-bucket\"],\n                    cluster_configuration[\"minio-job-folder\"],\n                    job_name,\n                    minio_client,\n                    remove_minio_files,\n                )\n\n                logger.info(f\"Extracting
          {tarfilename} to {target}...\")\n                extract_tarfile(tarfilename,
          target)\n\n                logger.info(\"Result:\")\n                logger.info(os.listdir(target))\n\n    def
          generate_unique_job_name(model_name):\n        epoch = datetime.today().strftime(\"%Y%m%d%H%M%S\")\n        return
          f\"job-{model_name}-{epoch}\"\n\n    def get_current_namespace():\n        SA_NAMESPACE
          = \"/var/run/secrets/kubernetes.io/serviceaccount/namespace\"\n        with
          open(SA_NAMESPACE) as f:\n            return f.read()\n\n    def initialize_namespace(namespace):\n        if
          namespace == \"\":\n            namespace = get_current_namespace()\n        namespace_spec
          = f\"namespace: {namespace}\"\n\n        return (namespace, namespace_spec)\n\n    def
          initialize_nodeselector(node_selector):\n        if node_selector != \"\":\n            node_selector
          = f\"nodeSelector:\\n        {node_selector}\"\n        return node_selector\n\n    def
          initialize_init_container(\n        base_image,\n        cluster_configuration,\n        inputs,\n        is_remote,\n        job_name,\n        minio_secret,\n        mount_path,\n    ):\n        if
          not is_remote:\n            return \"\"\n\n        command_specification
          = kfp.components.func_to_component_text(\n            func=sync_with_minio\n        )\n\n        #
          inner components loose type information as needed by lists/dicts\n        #
          -> cluster_configuration & inputs need to be a string (using json)\n        cluster_configuration_json
          = json.dumps(\n            {\n                \"minio-bucket\": cluster_configuration[\"minio-bucket\"],\n                \"minio-job-folder\":
          cluster_configuration[\"minio-job-folder\"],\n                \"minio-url\":
          cluster_configuration[\"minio-url\"],\n            }\n        )\n        inputs_json
          = json.dumps(inputs)\n        parameters = {\n            \"cluster_configuration\":
          cluster_configuration_json,\n            \"inputs\": inputs_json,\n            \"job_name\":
          job_name,\n            \"is_upload\": \"False\",\n        }\n\n        command,
          _, _, _ = initialize_command(command_specification, parameters)\n\n        init_container
          = f\"\"\"initContainers:\n          - name: init-inputs\n            image:
          {base_image}\n            command: {command}\n            volumeMounts:\n            -
          mountPath: {mount_path}\n              name: training\n            env:\n            -
          name: MINIO_USER\n              valueFrom:\n                secretKeyRef:\n                  name:
          {minio_secret}\n                  key: accesskey\n                  optional:
          false\n            - name: MINIO_PASS\n              valueFrom:\n                secretKeyRef:\n                  name:
          {minio_secret}\n                  key: secretkey\n                  optional:
          false\n\"\"\"\n        return init_container\n\n    def initialize_command(\n        specification,\n        parameters,\n        path_parameters
          = {},\n        mount_path = \"/tmp\",\n    ):\n        component_yaml =
          yaml.safe_load(specification)\n        container_yaml = component_yaml[\"implementation\"][\"container\"]\n        training_base_image
          = container_yaml[\"image\"]\n        command = container_yaml[\"command\"]\n        args
          = container_yaml[\"args\"]\n\n        actual_args = list()\n        inputs
          = list()\n        outputs = list()\n        for idx, arg in enumerate(args):\n            if
          type(arg) is dict:\n                if \"inputValue\" in arg:\n                    #
          required parameter (value)\n                    key = arg[\"inputValue\"]\n                    if
          key in parameters:\n                        actual_args.append(parameters[key])\n                    else:\n                        err
          = f\"Required parameter ''{key}'' missing in component input!\"\n                        raise
          Exception(err)\n                elif \"if\" in arg:\n                    #
          optional parameter\n                    key = arg[\"if\"][\"cond\"][\"isPresent\"]\n                    if
          key in parameters:\n                        actual_args.append(f\"--{key}\")\n                        actual_args.append(parameters[key])\n                elif
          \"inputPath\" in arg:\n                    # required InputPath\n                    key
          = arg[\"inputPath\"]\n                    if key in parameters:\n                        path_key
          = parameters[key]\n                        if path_key in path_parameters:\n                            mount
          = f\"{mount_path}{path_parameters[path_key]}\"\n                            inputs.append((path_parameters[path_key],
          mount))\n                            actual_args.append(mount)\n                        else:\n                            err
          = f\"InputPath ''{path_key}'' unavailable in training component!\"\n                            raise
          Exception(err)\n                    else:\n                        err =
          f\"Required parameter ''{key}'' missing in component input!\"\n                        raise
          Exception(err)\n                elif \"outputPath\" in arg:\n                    #
          required OutputPath\n                    key = arg[\"outputPath\"]\n                    if
          key in parameters:\n                        path_key = parameters[key]\n                        if
          path_key in path_parameters:\n                            mount = f\"{mount_path}{path_parameters[path_key]}\"\n                            outputs.append((mount,
          path_parameters[path_key]))\n                            actual_args.append(mount)\n                        else:\n                            err
          = f\"OutputPath ''{path_key}'' unavailable in training component!\"\n                            raise
          Exception(err)\n                    else:\n                        err =
          f\"Required parameter ''{key}'' missing in component input!\"\n                        raise
          Exception(err)\n            else:\n                # required parameter
          (key)\n                actual_args.append(arg)\n\n        command_with_initialized_args
          = json.dumps(command + actual_args)\n\n        return command_with_initialized_args,
          inputs, outputs, training_base_image\n\n    def initialize_fetch_command(\n        cluster_configuration,\n        job_name,\n        outputs,\n    ):\n        command_specification
          = kfp.components.func_to_component_text(\n            func=sync_with_minio\n        )\n\n        #
          inner components loose type information as needed by lists/dicts\n        #
          -> cluster_configuration & inputs need to be a string (using json)\n        cluster_configuration_json
          = json.dumps(\n            {\n                \"minio-bucket\": cluster_configuration[\"minio-bucket\"],\n                \"minio-job-folder\":
          cluster_configuration[\"minio-job-folder\"],\n                \"minio-url\":
          cluster_configuration[\"minio-url\"],\n            }\n        )\n        outputs_json
          = json.dumps(outputs)\n        parameters = {\n            \"cluster_configuration\":
          cluster_configuration_json,\n            \"inputs\": outputs_json,\n            \"job_name\":
          job_name,\n            \"is_upload\": \"True\",\n        }\n        command,
          _, _, _ = initialize_command(command_specification, parameters)\n        return
          command\n\n    def create_pvc_spec(pvc_name, namespace_spec, access_mode,
          pvc_size):\n        pvc_spec = f\"\"\"apiVersion: batch/v1\napiVersion:
          v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: {pvc_name}\n  {namespace_spec}\nspec:\n  accessModes:\n  -
          {access_mode}\n  resources:\n    requests:\n      storage: {pvc_size}\n\"\"\"\n        return
          yaml.safe_load(pvc_spec)\n\n    def create_minio_secret_spec(cluster_configuration,
          minio_secret, namespace_spec):\n        minio_secret_spec = f\"\"\"apiVersion:
          v1\nkind: Secret\nmetadata:\n  name: {minio_secret}\n  {namespace_spec}\nstringData:\n  accesskey:
          {cluster_configuration[\"minio-accesskey\"]}\n  secretkey: {cluster_configuration[\"minio-secretkey\"]}\n\"\"\"\n        return
          yaml.safe_load(minio_secret_spec)\n\n    def create_train_job_configuration(\n        job_name,\n        namespace_spec,\n        node_selector,\n        base_image,\n        train_command,\n        train_mount,\n        cpus,\n        memory,\n        gpus,\n        init_container,\n        pvc_name,\n        distribution_specification,\n        minio_url,\n        minio_secret,\n    ):\n        if
          cpus:\n            cpu_spec = f\"cpu: {cpus}\"\n        else:\n            cpu_spec
          = \"\"\n\n        if memory:\n            memory_spec = f\"memory: {memory}\"\n        else:\n            memory_spec
          = \"\"\n\n        if gpus:\n            gpu_spec = f\"nvidia.com/gpu: {gpus}\"\n        else:\n            gpu_spec
          = \"\"\n\n        if distribution_specification is None:\n            distribution_specification
          = dict()\n\n        if \"distribution_type\" not in distribution_specification:\n            distribution_specification[\"distribution_type\"]
          = \"Job\"\n\n        if gpus < 1:\n            slots_per_worker = 1\n        else:\n            slots_per_worker
          = gpus\n\n        if \"number_of_workers\" in distribution_specification:\n            number_of_workers
          = distribution_specification[\"number_of_workers\"]\n        else:\n            number_of_workers
          = 2\n\n        number_of_processes = number_of_workers * slots_per_worker\n\n        if
          \"launcher_cpus\" in distribution_specification:\n            launcher_cpu_spec
          = f\"cpu: {distribution_specification[''launcher_cpus'']}\"\n        else:\n            launcher_cpu_spec
          = \"\"\n\n        if \"launcher_memory\" in distribution_specification:\n            launcher_memory_spec
          = (\n                f\"memory: {distribution_specification[''launcher_memory'']}\"\n            )\n        else:\n            launcher_memory_spec
          = \"\"\n\n        if \"worker_cpus\" in distribution_specification:\n            worker_cpu_spec
          = f\"cpu: {distribution_specification[''worker_cpus'']}\"\n        else:\n            worker_cpu_spec
          = \"\"\n\n        if \"worker_memory\" in distribution_specification:\n            worker_memory_spec
          = (\n                f\"memory: {distribution_specification[''worker_memory'']}\"\n            )\n        else:\n            worker_memory_spec
          = \"\"\n\n        if distribution_specification[\"distribution_type\"] ==
          \"Job\":\n            job_spec = f\"\"\"apiVersion: batch/v1\nkind: Job\nmetadata:\n  name:
          {job_name}\n  labels:\n    train-model-job: {job_name}\n  {namespace_spec}\nspec:\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject:
          \"false\"\n    spec:\n      {node_selector}\n      containers:\n        -
          name: training-container\n          image: {base_image}\n          command:
          {train_command}\n          volumeMounts:\n            - mountPath: {train_mount}\n              name:
          training\n          restartPolicy: Never\n          env:\n            -
          name: S3_ENDPOINT\n              value: {minio_url}\n            - name:
          AWS_ACCESS_KEY_ID\n              valueFrom:\n                secretKeyRef:\n                  name:
          {minio_secret}\n                  key: accesskey\n                  optional:
          false\n            - name: AWS_SECRET_ACCESS_KEY\n              valueFrom:\n                secretKeyRef:\n                  name:
          {minio_secret}\n                  key: secretkey\n                  optional:
          false\n            - name: AWS_S3_SIGNATURE_VERSION\n              value:
          \"s3v4\"\n            - name: JOB_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath:
          metadata.name\n          resources:\n            limits:\n              {cpu_spec}\n              {memory_spec}\n              {gpu_spec}\n      {init_container}\n      volumes:\n        -
          name: training\n          persistentVolumeClaim:\n            claimName:
          {pvc_name}\n      restartPolicy: Never\n\"\"\"\n            job_config =
          {\n                \"group\": \"batch\",\n                \"version\": \"v1\",\n                \"plural\":
          \"jobs\",\n                \"label\": \"job-name\",\n            }\n        elif
          distribution_specification[\"distribution_type\"] == \"MPI\":\n            job_spec
          = f\"\"\"apiVersion: kubeflow.org/v1\nkind: MPIJob\nmetadata:\n  name: {job_name}\n  labels:\n    train-model-job:
          {job_name}\n  {namespace_spec}\nspec:\n  slotsPerWorker: {slots_per_worker}\n  runPolicy:\n    cleanPodPolicy:
          Running\n  mpiReplicaSpecs:\n    Launcher:\n      replicas: 1\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject:
          \"false\"\n        spec:\n          {init_container}\n          volumes:\n            -
          name: training\n              persistentVolumeClaim:\n                claimName:
          {pvc_name}\n          containers:\n          - image: {base_image}\n            name:
          mpi-launcher\n            command:\n            - mpirun\n            -
          -np\n            - \"{number_of_processes}\"\n            - --allow-run-as-root\n            -
          -bind-to\n            - none\n            - -map-by\n            - slot\n            -
          --prefix\n            - /opt/conda\n            - -mca\n            - pml\n            -
          ob1\n            - -mca\n            - btl\n            - ^openib\n            -
          -x\n            - NCCL_DEBUG=INFO\n            args: {train_command}\n            resources:\n              limits:\n                {launcher_cpu_spec}\n                {launcher_memory_spec}\n    Worker:\n      replicas:
          {number_of_workers}\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject:
          \"false\"\n        spec:\n          {node_selector}\n          containers:\n          -
          image: {base_image}\n            name: mpi-worker\n            env:\n            -
          name: S3_ENDPOINT\n              value: {minio_url}\n            - name:
          AWS_ACCESS_KEY_ID\n              valueFrom:\n                secretKeyRef:\n                  name:
          {minio_secret}\n                  key: accesskey\n                  optional:
          false\n            - name: AWS_SECRET_ACCESS_KEY\n              valueFrom:\n                secretKeyRef:\n                  name:
          {minio_secret}\n                  key: secretkey\n                  optional:
          false\n            - name: AWS_S3_SIGNATURE_VERSION\n              value:
          \"s3v4\"\n            - name: JOB_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath:
          metadata.name\n            volumeMounts:\n              - mountPath: /train\n                name:
          training\n            resources:\n              limits:\n                {worker_cpu_spec}\n                {worker_memory_spec}\n                {gpu_spec}\n          volumes:\n            -
          name: training\n              persistentVolumeClaim:\n                claimName:
          {pvc_name}\n\"\"\"\n            job_config = {\n                \"group\":
          \"kubeflow.org\",\n                \"version\": \"v1\",\n                \"plural\":
          \"mpijobs\",\n                \"label\": \"training.kubeflow.org/replica-type=launcher,training.kubeflow.org/job-name\",\n            }\n        elif
          distribution_specification[\"distribution_type\"] == \"TF\":\n            job_spec
          = f\"\"\"apiVersion: kubeflow.org/v1\nkind: TFJob\nmetadata:\n  name: {job_name}\n  labels:\n    train-model-job:
          {job_name}\n  {namespace_spec}\nspec:\n  runPolicy:\n    cleanPodPolicy:
          None\n  tfReplicaSpecs:\n    Worker:\n      replicas: {number_of_workers}\n      restartPolicy:
          OnFailure\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject:
          \"false\"\n        spec:\n          {node_selector}\n          containers:\n            -
          name: tensorflow\n              image: {base_image}\n              command:
          {train_command}\n              env:\n              - name: S3_ENDPOINT\n                value:
          {minio_url}\n              - name: AWS_ACCESS_KEY_ID\n                valueFrom:\n                  secretKeyRef:\n                    name:
          {minio_secret}\n                    key: accesskey\n                    optional:
          false\n              - name: AWS_SECRET_ACCESS_KEY\n                valueFrom:\n                  secretKeyRef:\n                    name:
          {minio_secret}\n                    key: secretkey\n                    optional:
          false\n              - name: AWS_S3_SIGNATURE_VERSION\n                value:
          \"s3v4\"\n              - name: JOB_NAME\n                valueFrom:\n                  fieldRef:\n                    fieldPath:
          metadata.name\n              volumeMounts:\n                - mountPath:
          /train\n                  name: training\n              resources:\n                limits:\n                  {worker_cpu_spec}\n                  {worker_memory_spec}\n                  {gpu_spec}\n          volumes:\n            -
          name: training\n              persistentVolumeClaim:\n                claimName:
          {pvc_name}\n\"\"\"\n            job_config = {\n                \"group\":
          \"kubeflow.org\",\n                \"version\": \"v1\",\n                \"plural\":
          \"tfjobs\",\n                \"label\": \"tf-job-name\",\n            }\n        else:\n            err
          = f\"Job failed while executing - unknown distribution_type: {distribution_specification[''distribution_type'']}\"\n            raise
          Exception(err)\n\n        job_config[\"job_spec\"] = yaml.safe_load(job_spec)\n        return
          job_config\n\n    def create_fetch_job_configuration(\n        job_name,\n        namespace_spec,\n        base_image,\n        fetch_command,\n        train_mount,\n        minio_secret,\n        pvc_name,\n    ):\n        job_spec
          = f\"\"\"apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: {job_name}\n  labels:\n    train-model-job:
          {job_name}\n  {namespace_spec}\nspec:\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject:
          \"false\"\n    spec:\n      containers:\n        - name: training-container\n          image:
          {base_image}\n          command: {fetch_command}\n          volumeMounts:\n            -
          mountPath: {train_mount}\n              name: training\n          restartPolicy:
          Never\n          env:\n          - name: MINIO_USER\n            valueFrom:\n              secretKeyRef:\n                name:
          {minio_secret}\n                key: accesskey\n                optional:
          false\n          - name: MINIO_PASS\n            valueFrom:\n              secretKeyRef:\n                name:
          {minio_secret}\n                key: secretkey\n                optional:
          false\n      volumes:\n        - name: training\n          persistentVolumeClaim:\n            claimName:
          {pvc_name}\n      restartPolicy: Never\n\"\"\"\n        job_config = {\n            \"group\":
          \"batch\",\n            \"version\": \"v1\",\n            \"plural\": \"jobs\",\n            \"job_spec\":
          yaml.safe_load(job_spec),\n            \"label\": \"job-name\",\n        }\n        return
          job_config\n\n    def submit_and_monitor_job(\n        api_client, job_config,
          namespace, additional_job_resources=[]\n    ):\n        job_spec = job_config[\"job_spec\"]\n        job_resource
          = custom_object_api.create_namespaced_custom_object(\n            group=job_config[\"group\"],\n            version=job_config[\"version\"],\n            namespace=namespace,\n            plural=job_config[\"plural\"],\n            body=job_spec,\n        )\n        job_name
          = job_resource[\"metadata\"][\"name\"]\n        job_uid = job_resource[\"metadata\"][\"uid\"]\n\n        logger.info(\"Creating
          additional job resource...\")\n        if additional_job_resources:\n            for
          resource in additional_job_resources:\n                resource[\"metadata\"][\"ownerReferences\"]
          = [\n                    {\n                        \"apiVersion\": job_spec[\"apiVersion\"],\n                        \"kind\":
          job_spec[\"kind\"],\n                        \"name\": job_name,\n                        \"uid\":
          job_uid,\n                    }\n                ]\n            utils.create_from_yaml(api_client,
          yaml_objects=additional_job_resources)\n\n        logger.info(\"Waiting
          for job to succeed...\")\n        job_is_monitored = False\n        pods_being_monitored
          = set()\n        job_watch = watch.Watch()\n        for job_event in job_watch.stream(\n            custom_object_api.list_namespaced_custom_object,\n            group=job_config[\"group\"],\n            version=job_config[\"version\"],\n            plural=job_config[\"plural\"],\n            namespace=namespace,\n            label_selector=f\"train-model-job={job_name}\",\n            timeout_seconds=0,\n        ):\n            logger.info(f\"job_event:
          {job_event}\")\n            job = job_event[\"object\"]\n            if
          \"status\" not in job and \"items\" in job:\n                job = job[\"items\"][0]\n\n            if
          \"status\" not in job:\n                logger.info(\"Skipping event (no
          status information found)...\")\n                continue\n\n            job_status
          = dict()\n            if \"active\" in job[\"status\"]:\n                job_status[\"active\"]
          = job[\"status\"][\"active\"]\n            else:\n                job_status[\"active\"]
          = 0\n            if \"completionTime\" in job[\"status\"]:\n                job_status[\"completionTime\"]
          = job[\"status\"][\"completionTime\"]\n            if \"failed\" in job[\"status\"]:\n                job_status[\"failed\"]
          = job[\"status\"][\"failed\"]\n            else:\n                job_status[\"failed\"]
          = 0\n            if \"ready\" in job[\"status\"]:\n                job_status[\"ready\"]
          = job[\"status\"][\"ready\"]\n            else:\n                job_status[\"ready\"]
          = 0\n            if \"startTime\" in job[\"status\"]:\n                job_status[\"startTime\"]
          = job[\"status\"][\"startTime\"]\n            if \"succeeded\" in job[\"status\"]:\n                job_status[\"succeeded\"]
          = job[\"status\"][\"succeeded\"]\n            else:\n                job_status[\"succeeded\"]
          = 0\n\n            # MPI\n            job_status[\"Complete\"] = \"False\"\n            job_status[\"Created\"]
          = \"False\"\n            job_status[\"Failed\"] = \"False\"\n            job_status[\"Running\"]
          = \"False\"\n            job_status[\"Succeeded\"] = \"False\"\n            if
          \"conditions\" in job[\"status\"]:\n                for condition in job[\"status\"][\"conditions\"]:\n                    job_status[condition[\"type\"]]
          = condition[\"status\"]\n\n            logger.info(f\"Job status: {job_status}\")\n\n            def
          start_monitoring(job_name, job_status):\n                return (not job_is_monitored)
          and (\n                    job_status[\"active\"] > 0\n                    or
          job_status[\"Running\"] == \"True\"\n                    or job_status[\"failed\"]
          > 0\n                    or job_status[\"Failed\"] == \"True\"\n                    or
          job_status[\"ready\"] > 0\n                    or job_status[\"Complete\"]
          == \"True\"\n                    or job_status[\"Succeeded\"] == \"True\"\n                )\n\n            if
          start_monitoring(job_name, job_status):\n                job_is_monitored
          = True\n                logger.info(\"Monitoring pods of job...\")\n\n                #
          See https://stackoverflow.com/questions/65938572/kubernetes-python-client-equivalent-of-kubectl-wait-for-command\n                pod_watch
          = watch.Watch()\n                for pod_event in pod_watch.stream(\n                    func=core_api.list_namespaced_pod,\n                    namespace=namespace,\n                    label_selector=f\"{job_config[''label'']}={job_name}\",\n                    timeout_seconds=0,\n                ):\n                    pod
          = pod_event[\"object\"]\n                    pod_name = pod.metadata.name\n\n                    logger.info(\n                        f\"Pod
          {pod_name}: {pod_event[''type'']} - {pod.status.phase}\"\n                    )\n\n                    if
          pod_name in pods_being_monitored:\n                        pod_watch.stop()\n                    elif
          pod_name not in pods_being_monitored and (\n                        pod.status.phase
          == \"Running\"\n                        or pod.status.phase == \"Succeeded\"\n                        or
          pod.status.phase == \"Failed\"\n                    ):\n                        pods_being_monitored.add(pod_name)\n                        logger.info(\n                            \"==============================================================================\"\n                        )\n                        logger.info(\n                            \"==============================================================================\"\n                        )\n                        logger.info(f\"===
          Streaming logs of pod {pod_name}...\")\n                        logger.info(\n                            \"==============================================================================\"\n                        )\n                        logger.info(\n                            \"==============================================================================\"\n                        )\n\n                        log_watch
          = watch.Watch()\n                        for log_event in log_watch.stream(\n                            core_api.read_namespaced_pod_log,\n                            name=pod_name,\n                            namespace=namespace,\n                            follow=True,\n                            _return_http_data_only=True,\n                            _preload_content=False,\n                        ):\n                            print(log_event)\n                        logger.info(\n                            \"==============================================================================\"\n                        )\n                        logger.info(\n                            \"==============================================================================\"\n                        )\n\n                        pod_watch.stop()\n\n                        if
          pod.status.phase == \"Failed\":\n                            err = \"Job
          failed while executing.\"\n                            raise Exception(err)\n                        break\n                    if
          pod_event[\"type\"] == \"DELETED\":\n                        err = \"Pod
          was deleted while we where waiting for it to start.\"\n                        raise
          Exception(err)\n            elif (\n                job_status[\"succeeded\"]
          > 0\n                or job_status[\"Complete\"] == \"True\"\n                or
          job_status[\"Succeeded\"] == \"True\"\n            ):\n                job_watch.stop()\n                logger.info(\"Job
          finished successfully.\")\n                break\n            elif not (job_status[\"active\"]
          > 0 or job_status[\"Running\"] == \"True\") and (\n                job_status[\"failed\"]
          > 0 or job_status[\"Failed\"] == \"True\"\n            ):\n                job_watch.stop()\n                raise
          Exception(\"Job failed!\")\n            else:\n                logger.info(f\"Waiting
          for job updates. Current status: {job_status}\")\n\n    ###########################################################################\n    #
          Main Workflow\n    ###########################################################################\n\n    logger.info(\"Establishing
          local cluster connection...\")\n    local_api_client = establish_local_cluster_connection()\n\n    logger.info(\"Receiving
          training cluster configuration...\")\n    cluster_configuration = get_cluster_configuration(\n        local_api_client,
          cluster_configuration_secret\n    )\n\n    logger.info(\"Establishing training
          cluster connection...\")\n    api_client, is_remote = establish_training_cluster_connection(\n        local_api_client,
          cluster_configuration\n    )\n    batch_api = client.BatchV1Api(api_client)\n    core_api
          = client.CoreV1Api(api_client)\n    custom_object_api = client.CustomObjectsApi(api_client)\n\n    logger.info(\"Initializing
          resources...\")\n    job_name = generate_unique_job_name(model_name)\n    job_minio_secret
          = f\"{job_name}-minio-secret\"\n    namespace, namespace_spec = initialize_namespace(\n        cluster_configuration[\"remote-namespace\"]\n    )\n    pvc_name
          = f\"{job_name}-pvc\"\n    node_selector = initialize_nodeselector(node_selector)\n\n    path_parameters
          = {\n        \"train_dataset_dir\": train_dataset_dir,\n        \"validation_dataset_dir\":
          validation_dataset_dir,\n        \"model_dir\": model_dir,\n    }\n    train_command,
          inputs, outputs, base_image = initialize_command(\n        train_specification,
          train_parameters, path_parameters, train_mount\n    )\n\n    init_container
          = initialize_init_container(\n        base_image,\n        cluster_configuration,\n        inputs,\n        is_remote,\n        job_name,\n        job_minio_secret,\n        train_mount,\n    )\n\n    logger.info(\"=======================================\")\n    logger.info(\"Derived
          configurations\")\n    logger.info(\"=======================================\")\n    logger.info(f\"job_name:
          {job_name}\")\n    logger.info(f\"namespace: {namespace}\")\n    logger.info(f\"is_remote:
          {is_remote}\")\n    logger.info(f\"minio_url: {cluster_configuration[''minio-url'']}\")\n    logger.info(f\"job_minio_secret:
          {job_minio_secret}\")\n    logger.info(\"inputs (input paths send to job):\")\n    for
          source, target in inputs:\n        logger.info(\n            f\"- {source}
          -> {cluster_configuration[''minio-bucket'']}/{cluster_configuration[''minio-job-folder'']}/{job_name}/{target}\"\n        )\n    logger.info(\"outputs
          (output paths returning from job):\")\n    for source, target in outputs:\n        logger.info(\n            f\"-
          {target} <- {cluster_configuration[''minio-bucket'']}/{cluster_configuration[''minio-job-folder'']}/{job_name}/{source}\"\n        )\n    logger.info(f\"distribution_specification:
          {distribution_specification}\")\n    logger.info(f\"train_command: {train_command}\")\n    logger.info(\"=======================================\")\n\n    additional_job_resources
          = []\n\n    if is_remote:\n        logger.info(\"Using MinIO to sync data
          with a new remote PVC for the job...\")\n        sync_with_minio(cluster_configuration,
          inputs, job_name, is_upload=True)\n        additional_job_resources.append(\n            create_pvc_spec(\n                pvc_name,
          namespace_spec, cluster_configuration[\"access-mode\"], pvc_size\n            )\n        )\n        additional_job_resources.append(\n            create_minio_secret_spec(\n                cluster_configuration,
          job_minio_secret, namespace_spec\n            )\n        )\n    else:\n        logger.info(\n            f\"Pushing
          inputs to local {train_mount} mount as shared with job environment...\"\n        )\n        for
          (source, target) in inputs:\n            clone_path(source, target)\n\n    logger.info(\"Creating
          train job configuration...\")\n    train_job_config = create_train_job_configuration(\n        job_name,\n        namespace_spec,\n        node_selector,\n        base_image,\n        train_command,\n        train_mount,\n        cpus,\n        memory,\n        gpus,\n        init_container,\n        pvc_name,\n        distribution_specification,\n        cluster_configuration[\"minio-url\"],\n        job_minio_secret,\n    )\n\n    logger.info(f\"Starting
          train job ''{namespace}.{job_name}''...\")\n    submit_and_monitor_job(\n        api_client,\n        train_job_config,\n        namespace,\n        additional_job_resources,\n    )\n\n    logger.info(\"Receiving
          training outputs...\")\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n    if
          is_remote:\n        fetch_command = initialize_fetch_command(\n            cluster_configuration,
          job_name, outputs\n        )\n        fetch_job_name = f\"{job_name}-fetch\"\n\n        logger.info(\"Creating
          fetch job configuration...\")\n        fetch_job_config = create_fetch_job_configuration(\n            fetch_job_name,\n            namespace_spec,\n            base_image,\n            fetch_command,\n            train_mount,\n            job_minio_secret,\n            pvc_name,\n        )\n\n        logger.info(f\"Starting
          fetch job ''{namespace}.{fetch_job_name}''...\")\n        submit_and_monitor_job(api_client,
          fetch_job_config, namespace)\n\n        logger.info(\"Fetching output data
          from MinIO & deleting it afterwards...\")\n        sync_with_minio(\n            cluster_configuration,\n            outputs,\n            job_name,\n            is_upload=False,\n            remove_minio_files=True,\n        )\n\n        logger.info(f\"Deleting
          Job {fetch_job_name}...\")\n        batch_api.delete_namespaced_job(fetch_job_name,
          namespace)\n    else:\n        logger.info(\n            f\"Fetching outputs
          to local {train_mount} mount as shared with job environment...\"\n        )\n        for
          (source, target) in outputs:\n            clone_path(source, target)\n\n    logger.info(f\"Deleting
          Job {job_name}...\")\n    custom_object_api.delete_namespaced_custom_object(\n        train_job_config[\"group\"],\n        train_job_config[\"version\"],\n        namespace,\n        train_job_config[\"plural\"],\n        job_name,\n    )\n\n    logger.info(\"Finished.\")\n\nimport
          json\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Train model
          job'', description=''Trains a model. Once trained, the model is persisted
          to model_dir.'')\n_parser.add_argument(\"--train-dataset-dir\", dest=\"train_dataset_dir\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--validation-dataset-dir\",
          dest=\"validation_dataset_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--train-specification\",
          dest=\"train_specification\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--train-parameters\",
          dest=\"train_parameters\", type=json.loads, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--train-mount\",
          dest=\"train_mount\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-name\",
          dest=\"model_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--node-selector\",
          dest=\"node_selector\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--pvc-name\",
          dest=\"pvc_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--pvc-size\",
          dest=\"pvc_size\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--cpus\",
          dest=\"cpus\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--gpus\",
          dest=\"gpus\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--memory\",
          dest=\"memory\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--cluster-configuration-secret\",
          dest=\"cluster_configuration_secret\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--distribution-specification\",
          dest=\"distribution_specification\", type=json.loads, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-dir\",
          dest=\"model_dir\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = train_model_job(**_parsed_args)\n"], "image": "quay.io/ibm/kubeflow-notebook-image-ppc64le:elyra3.14.1-py3.9-tf2.10.1-pt1.10.2-v0"}},
          "inputs": [{"description": "Path to the directory with training data.",
          "name": "train_dataset_dir", "type": "String"}, {"description": "Path to
          the directory with validation data to be used during training.", "name":
          "validation_dataset_dir", "type": "String"}, {"description": "Training command
          as generated from a Python function using kfp.components.func_to_component_text.",
          "name": "train_specification", "type": "String"}, {"description": "Dictionary
          mapping formal to actual parameters for the training spacification.", "name":
          "train_parameters", "type": "typing.Dict[str, str]"}, {"default": "/train",
          "description": "Optional mounting point for training data of an existing
          PVC. Example: \"/train\".", "name": "train_mount", "optional": true, "type":
          "String"}, {"default": "my-model", "description": "Optional name of the
          model. Must be unique for the targeted namespace and conform Kubernetes
          naming conventions. Example: my-model.", "name": "model_name", "optional":
          true, "type": "String"}, {"default": "", "description": "Optional node selector
          for worker nodes. Example: nvidia.com/gpu.product: \"Tesla-V100-SXM2-32GB\".",
          "name": "node_selector", "optional": true, "type": "String"}, {"default":
          "", "description": "Optional name to an existing persistent volume claim
          (pvc). If given, this pvc is mounted into the training job. Example: \"music-genre-classification-j4ssf-training-pvc\".",
          "name": "pvc_name", "optional": true, "type": "String"}, {"default": "10Gi",
          "description": "Optional size of the storage during model training. Storage
          is mounted into to the Job based on a persitent volume claim of the given
          size. Example: 10Gi.", "name": "pvc_size", "optional": true, "type": "String"},
          {"default": "", "description": "Optional CPU limit for the job. Leave empty
          for cluster defaults (typically no limit). Example: \"1000m\".", "name":
          "cpus", "optional": true, "type": "String"}, {"default": "0", "description":
          "Optional number of GPUs for the job. Example: 2.", "name": "gpus", "optional":
          true, "type": "Integer"}, {"default": "", "description": "Optional memory
          limit for the job. Leave empty for cluster defaults (typically no limit).
          Example: \"1Gi\".", "name": "memory", "optional": true, "type": "String"},
          {"default": "", "description": "Optional secret name configuring a (remote)
          Kubernetes cluster to run the job in and the backing MinIO object store.
          All secret''s data values are optional and appropriate defaults are chosen
          if not present. The secret may provide a suitable kubernetes bearer token,
          the associated namespace, a host, etc. Example: \"remote-power-cluster\".",
          "name": "cluster_configuration_secret", "optional": true, "type": "String"},
          {"description": "Optional dictionary specifiying the distribution behavior.
          By default, no distributed training is executed, which results in an ordinary
          Kubernetes Job  for training. Otherwise, dictionary entries determine the
          distribution behavior. The \"distribution_type\" entry determines the distribution
          type: \"Job\" (no distribution; ordinary Kubernetes job), \"MPI\" (all-reduce
          style distribution via Horovod), or \"TF\" (parameter-server style distribution
          via distributed training with TensorFlow). Depending on the distribution
          type, additional dictionary entries can be processed. For distributed training
          jobs, the \"number_of_workers\" (e.g., 2) determines the number of worker
          replicas for training. Individual resource limits can be controlled via
          \"worker_cpus\" (e.g., \"1000m\") and \"worker_memory\" (e.g., \"1Gi\").
          MPI additionally provides a fine-grained control of launcher cpu and memory
          limits via \"launcher_cpus\" (e.g., \"1000m\") and \"launcher_memory\" (e.g.,
          \"1Gi\"). Full example with MPI: {\"distribution_type\": \"MPI\", \"number_of_workers\":
          2, \"worker_cpus\": \"8\", \"worker_memory\": \"32Gi\", \"launcher_cpus\":
          \"2\", \"launcher_memory\": \"8Gi\"}", "name": "distribution_specification",
          "optional": true, "type": "typing.Dict[str, str]"}], "name": "Train model
          job", "outputs": [{"description": "Target path where the model will be stored.",
          "name": "model_dir", "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "c29ed30c3d44b4af3a27f741d7b565dad0b7ce33a6bae6f201d5a4438cda4091", "url":
          "/home/jovyan/components/model-building/train-model-job/component.yaml"}',
        pipelines.kubeflow.org/arguments.parameters: '{"cluster_configuration_secret":
          "{{inputs.parameters.cluster_configuration_secret}}", "cpus": "", "distribution_specification":
          "{\"distribution_type\": \"MPI\", \"number_of_workers\": \"{{inputs.parameters.number_of_workers}}\"}",
          "gpus": "{{inputs.parameters.training_gpus}}", "memory": "", "model_name":
          "{{inputs.parameters.model_name}}", "node_selector": "{{inputs.parameters.training_node_selector}}",
          "pvc_name": "", "pvc_size": "10Gi", "train_mount": "/train", "train_parameters":
          "{\"epochs\": \"{{inputs.parameters.epochs}}\", \"model_dir\": \"model_dir\",
          \"train_dataset_dir\": \"train_dataset_dir\", \"validation_dataset_dir\":
          \"validation_dataset_dir\"}", "train_specification": "name: Train distributed
          model\ndescription: Uses transfer learning on a prepared dataset. Once trained,
          the model\n  is persisted to `model_dir`.\ninputs:\n- {name: train_dataset_dir,
          type: String}\n- {name: validation_dataset_dir, type: String}\n- {name:
          epochs, type: Integer, default: ''100'', optional: true}\n- {name: batch_size,
          type: Integer, default: ''32'', optional: true}\noutputs:\n- {name: model_dir,
          type: String}\nimplementation:\n  container:\n    image: quay.io/ibm/kubeflow-notebook-image-ppc64le:elyra3.14.1-py3.9-tf2.10.1-pt1.10.2-v0\n    command:\n    -
          sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\"
          \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    -
          |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import
          os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return
          file_path\n\n      def train_distributed_model(\n          train_dataset_dir,\n          validation_dataset_dir,\n          model_dir,\n          epochs
          = 100,\n          batch_size = 32,\n      ):\n          \"\"\"Uses transfer
          learning on a prepared dataset. Once trained, the model is persisted to
          `model_dir`.\"\"\"\n\n          import horovod.tensorflow.keras as hvd\n          import
          os\n          import tensorflow as tf\n          from tensorflow.keras import
          Sequential\n          from tensorflow.keras.applications import InceptionV3\n          from
          tensorflow.keras.layers import (\n              BatchNormalization,\n              Dense,\n              Dropout,\n              GlobalAveragePooling2D,\n          )\n          from
          tensorflow.keras.callbacks import (\n              EarlyStopping,\n              ModelCheckpoint,\n              ReduceLROnPlateau,\n              TensorBoard,\n          )\n          from
          horovod.tensorflow.keras.callbacks import MetricAverageCallback\n          import
          tensorflow_datasets as tfds\n          import time\n\n          def load_datasets():\n              train_dataset
          = tf.data.experimental.load(train_dataset_dir)\n              validation_dataset
          = tf.data.experimental.load(validation_dataset_dir)\n              return
          (train_dataset, validation_dataset)\n\n          def build_model(shape):\n              backbone
          = InceptionV3(include_top=False, weights=\"imagenet\", input_shape=shape)\n\n              for
          layer in backbone.layers:\n                  layer.trainable = False\n\n              model
          = Sequential()\n              model.add(backbone)\n              model.add(GlobalAveragePooling2D())\n              model.add(Dense(128,
          activation=\"relu\"))\n              model.add(BatchNormalization())\n              model.add(Dropout(0.3))\n              model.add(Dense(64,
          activation=\"relu\"))\n              model.add(BatchNormalization())\n              model.add(Dropout(0.3))\n              model.add(Dense(10,
          activation=\"softmax\"))\n\n              return model\n\n          print(\"Initializing
          Horovod/MPI for distributed training...\")\n          hvd.init()\n\n          #
          Pin GPU to be used to process local rank (one GPU per process)\n          gpus
          = tf.config.experimental.list_physical_devices(\"GPU\")\n          for gpu
          in gpus:\n              tf.config.experimental.set_memory_growth(gpu, True)\n          if
          gpus:\n              tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()],
          \"GPU\")\n\n          # Prepare distributed training with GPU support\n          os.environ[\"NCCL_DEBUG\"]
          = \"INFO\"\n          tfds.disable_progress_bar()\n\n          if hvd.rank()
          == 0:\n              if not os.path.exists(model_dir):\n                  os.makedirs(model_dir)\n\n          #
          see https://horovod.readthedocs.io/en/stable/api.html\n          print(\"==============================================\")\n          print(f\"hvd.rank():
          {str(hvd.rank())}\")\n          print(f\"hvd.local_rank(): {str(hvd.local_rank())}\")\n          print(f\"hvd.size():
          {str(hvd.size())}\")\n          print(f\"hvd.local_size(): {str(hvd.local_size())}\")\n          print(\"gpus:\")\n          print(gpus)\n          print(\"==============================================\")\n\n          print(\"Loading
          datasets...\")\n          train_dataset, validation_dataset = load_datasets()\n          shape
          = (224, 224, 3)\n\n          print(\"Making traininig dataset ready for
          distributed training...\")\n          # Best shuffling needs a buffer with
          size equal to the size of the\n          # dataset. Approximate values should
          be fine here.\n          dataset_elements = 1400  # hard to determine dynamically
          in TFDataset\n          approx_shard_train_size = dataset_elements // hvd.size()
          + 1\n\n          # References:\n          # - shard: https://github.com/horovod/horovod/issues/2623#issuecomment-768435610\n          #
          - cache & prefetch: https://stackoverflow.com/questions/59228816/what-do-the-tensorflow-datasets-functions-cache-and-prefetch-do\n          #
          - shuffle: https://stackoverflow.com/questions/64372390/what-does-buffer-size-do-in-tensorflow-dataset-shuffling\n          distributed_train_dataset
          = (\n              train_dataset.unbatch()  # Batch after sharding\n              .shard(num_shards=hvd.size(),
          index=hvd.rank())  # 1 shard per worker\n              .cache()  # Reuse
          data on next epoch\n              .shuffle(\n                  buffer_size=approx_shard_train_size,
          seed=42, reshuffle_each_iteration=False\n              )  # Randomize shards\n              .batch(batch_size)\n              .repeat()  #
          Avoid last batch being of unequal size\n              .prefetch(tf.data.AUTOTUNE)  #
          Overlap preprocessing and training\n          )\n\n          print(\"Building
          model...\")\n          model = build_model(shape)\n          print(model.summary())\n\n          opt
          = tf.keras.optimizers.Adam(learning_rate=0.001 * hvd.size())\n          #
          Horovod: add Horovod DistributedOptimizer.\n          opt = hvd.DistributedOptimizer(opt)\n\n          print(\"Compiling
          model...\")\n          # Horovod: Specify `experimental_run_tf_function=False`
          to ensure TensorFlow\n          # uses hvd.DistributedOptimizer() to compute
          gradients.\n          model.compile(\n              optimizer=opt,\n              loss=\"categorical_crossentropy\",\n              metrics=[\"categorical_accuracy\"],\n              experimental_run_tf_function=False,\n          )\n\n          print(\"Initializing
          training callbacks...\")\n          callbacks = [\n              # MetricAverageCallback
          is used to synchrnoize metrics between the workers,\n              # That
          is important when using ReduceLROnPlateau and/or EarlyStopping.\n              #
          https://www.olcf.ornl.gov/wp-content/uploads/2019/12/ORNL-Scaling-20200210.pdf
          (chart 28)\n              MetricAverageCallback(),\n              ReduceLROnPlateau(\n                  monitor=\"val_loss\",\n                  factor=0.1,\n                  patience=7,\n                  verbose=1,\n                  min_delta=0.0001,\n                  mode=\"min\",\n              ),\n              EarlyStopping(monitor=\"val_loss\",
          patience=20, verbose=1, mode=\"min\"),\n              TensorBoard(\n                  log_dir=f\"s3://mlpipeline/tensorboard/{os.environ[''JOB_NAME'']}\",\n                  histogram_freq=1,\n              ),\n              #
          Horovod: broadcast initial variable states from rank 0 to all other processes.\n              #
          This is necessary to ensure consistent initialization of all workers when\n              #
          training is started with random weights or restored from a checkpoint.\n              hvd.callbacks.BroadcastGlobalVariablesCallback(0),\n          ]\n          #
          Horovod: save checkpoints only on worker 0 to prevent other workers from
          corrupting them.\n          if hvd.rank() == 0:\n              callbacks.append(\n                  ModelCheckpoint(\n                      f\"{model_dir}/best_model.keras\",\n                      monitor=\"val_loss\",\n                      save_best_only=True,\n                      save_weights_only=True,\n                      mode=\"min\",\n                  )\n              )\n\n          print(\"Starting
          model training...\")\n          start = time.time()\n          hist = model.fit(\n              distributed_train_dataset,\n              validation_data=validation_dataset,\n              epochs=epochs,\n              steps_per_epoch=approx_shard_train_size
          // batch_size\n              + 1,  # steps_per_epoch is needed when using
          repeat()\n              callbacks=callbacks,\n              verbose=1 if
          hvd.rank() == 0 else 0,\n          )\n\n          if hvd.rank() == 0:\n              print(\"\\n\\nTraining
          took \", time.time() - start, \"seconds\")\n\n              print(\"Model
          train history:\")\n              print(hist.history)\n\n              print(f\"Saving
          model to: {model_dir}\")\n              model.save(model_dir)\n              print(f\"Model
          saved to: {model_dir}\")\n\n          print(f\"Finished. {hvd.rank()}\")\n\n      import
          argparse\n      _parser = argparse.ArgumentParser(prog=''Train distributed
          model'', description=''Uses transfer learning on a prepared dataset. Once
          trained, the model is persisted to `model_dir`.'')\n      _parser.add_argument(\"--train-dataset-dir\",
          dest=\"train_dataset_dir\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--validation-dataset-dir\",
          dest=\"validation_dataset_dir\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--epochs\",
          dest=\"epochs\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--batch-size\",
          dest=\"batch_size\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--model-dir\",
          dest=\"model_dir\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs
          = train_distributed_model(**_parsed_args)\n    args:\n    - --train-dataset-dir\n    -
          {inputPath: train_dataset_dir}\n    - --validation-dataset-dir\n    - {inputPath:
          validation_dataset_dir}\n    - if:\n        cond: {isPresent: epochs}\n        then:\n        -
          --epochs\n        - {inputValue: epochs}\n    - if:\n        cond: {isPresent:
          batch_size}\n        then:\n        - --batch-size\n        - {inputValue:
          batch_size}\n    - --model-dir\n    - {outputPath: model_dir}\n"}', pipelines.kubeflow.org/max_cache_staleness: P0D}
  - name: upload-model
    container:
      args: [--file-dir, /tmp/inputs/file_dir/data, --minio-url, '{{inputs.parameters.minio_url}}',
        --minio-secret, mlpipeline-minio-artifact, --export-bucket, '{{workflow.namespace}}-{{inputs.parameters.model_name}}',
        --model-name, '{{inputs.parameters.model_name}}', --model-version, '1', --model-format,
        onnx, '----output-paths', /tmp/outputs/s3_address/data, /tmp/outputs/triton_s3_address/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def upload_model(
            file_dir,
            minio_url = "minio-service.kubeflow:9000",
            minio_secret = "mlpipeline-minio-artifact",
            export_bucket = "models",
            model_name = "my-model",
            model_version = 1,
            model_format = "onnx",
        ):
            """Uploads a model file to MinIO artifact store."""

            from collections import namedtuple
            from kubernetes import (
                client,
                config
            )
            import logging
            from minio import Minio
            import sys

            logging.basicConfig(
                stream=sys.stdout,
                level=logging.INFO,
                format='%(levelname)s %(asctime)s: %(message)s'
            )
            logger = logging.getLogger()

            def get_minio_client(minio_secret):
                import base64
                from kubernetes.client.rest import ApiException

                def get_current_namespace():
                    SA_NAMESPACE = "/var/run/secrets/kubernetes.io/serviceaccount/namespace"
                    with open(SA_NAMESPACE) as f:
                        return f.read()

                def decode(text):
                    return base64.b64decode(text).decode('utf-8')

                config.load_incluster_config()
                api_client = client.ApiClient()

                try:
                    secret = client.CoreV1Api(api_client).read_namespaced_secret(minio_secret, get_current_namespace())

                    minio_user = decode(secret.data['accesskey'])
                    minio_pass = decode(secret.data['secretkey'])

                    return Minio(minio_url,
                                 access_key=minio_user,
                                 secret_key=minio_pass,
                                 secure=False)
                except ApiException as e:
                    if e.status == 404:
                        logger.error("Failed to get secret 'mlpipeline-minio-artifact', which is needed for communicating with MinIO!")
                    raise Exception(e)

            logger.info(f"Establishing MinIO connection to '{minio_url}'...")
            minio_client = get_minio_client(minio_secret)

            # Create export bucket if it does not yet exist
            response = minio_client.list_buckets()
            export_bucket_exists = False
            for bucket in response:
                if bucket.name == export_bucket:
                    export_bucket_exists = True

            if not export_bucket_exists:
                logger.info(f"Creating bucket '{export_bucket}'...")
                minio_client.make_bucket(bucket_name=export_bucket)

            model_path = f"{model_name}/{model_version}/model.{model_format}"
            s3_address = f"s3://{minio_url}/{export_bucket}/{model_format}"
            triton_s3_address = f"{s3_address}/{model_path}"

            logger.info(f"Saving onnx file to MinIO (s3 address: {s3_address})...")
            minio_client.fput_object(
                bucket_name=export_bucket,  # bucket name in Minio
                object_name=f"{model_format}/{model_path}",  # file name in bucket of Minio / for Triton name MUST be model.onnx!
                file_path=file_dir,  # file path / name in local system
            )

            logger.info("Finished.")
            out_tuple = namedtuple("UploadOutput", ["s3_address", "triton_s3_address"])
            return out_tuple(s3_address, triton_s3_address)

        def _serialize_str(str_value: str) -> str:
            if not isinstance(str_value, str):
                raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                    str(str_value), str(type(str_value))))
            return str_value

        import argparse
        _parser = argparse.ArgumentParser(prog='Upload model', description='Uploads a model file to MinIO artifact store.')
        _parser.add_argument("--file-dir", dest="file_dir", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--minio-url", dest="minio_url", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--minio-secret", dest="minio_secret", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--export-bucket", dest="export_bucket", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--model-name", dest="model_name", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--model-version", dest="model_version", type=int, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--model-format", dest="model_format", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=2)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = upload_model(**_parsed_args)

        _output_serializers = [
            _serialize_str,
            _serialize_str,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      image: quay.io/ibm/kubeflow-notebook-image-ppc64le:latest
      volumeMounts:
      - {mountPath: /tmp/inputs/file_dir, name: data-storage, subPath: '{{inputs.parameters.convert-model-to-onnx-onnx_model_dir-subpath}}',
        readOnly: true}
      - {mountPath: /tmp/outputs/s3_address, name: data-storage, subPath: 'artefacts/{{workflow.uid}}_{{pod.name}}/upload-model-s3_address'}
      - {mountPath: /tmp/outputs/triton_s3_address, name: data-storage, subPath: 'artefacts/{{workflow.uid}}_{{pod.name}}/upload-model-triton_s3_address'}
    inputs:
      parameters:
      - {name: minio_url}
      - {name: model_name}
      - {name: convert-model-to-onnx-onnx_model_dir-subpath}
    outputs:
      parameters:
      - {name: upload-model-s3_address-subpath, value: 'artefacts/{{workflow.uid}}_{{pod.name}}/upload-model-s3_address'}
      - {name: upload-model-triton_s3_address-subpath, value: 'artefacts/{{workflow.uid}}_{{pod.name}}/upload-model-triton_s3_address'}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Uploads
          a model file to MinIO artifact store.", "implementation": {"container":
          {"args": ["--file-dir", {"inputPath": "file_dir"}, {"if": {"cond": {"isPresent":
          "minio_url"}, "then": ["--minio-url", {"inputValue": "minio_url"}]}}, {"if":
          {"cond": {"isPresent": "minio_secret"}, "then": ["--minio-secret", {"inputValue":
          "minio_secret"}]}}, {"if": {"cond": {"isPresent": "export_bucket"}, "then":
          ["--export-bucket", {"inputValue": "export_bucket"}]}}, {"if": {"cond":
          {"isPresent": "model_name"}, "then": ["--model-name", {"inputValue": "model_name"}]}},
          {"if": {"cond": {"isPresent": "model_version"}, "then": ["--model-version",
          {"inputValue": "model_version"}]}}, {"if": {"cond": {"isPresent": "model_format"},
          "then": ["--model-format", {"inputValue": "model_format"}]}}, "----output-paths",
          {"outputPath": "s3_address"}, {"outputPath": "triton_s3_address"}], "command":
          ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def upload_model(\n    file_dir,\n    minio_url
          = \"minio-service.kubeflow:9000\",\n    minio_secret = \"mlpipeline-minio-artifact\",\n    export_bucket
          = \"models\",\n    model_name = \"my-model\",\n    model_version = 1,\n    model_format
          = \"onnx\",\n):\n    \"\"\"Uploads a model file to MinIO artifact store.\"\"\"\n\n    from
          collections import namedtuple\n    from kubernetes import (\n        client,\n        config\n    )\n    import
          logging\n    from minio import Minio\n    import sys\n\n    logging.basicConfig(\n        stream=sys.stdout,\n        level=logging.INFO,\n        format=''%(levelname)s
          %(asctime)s: %(message)s''\n    )\n    logger = logging.getLogger()\n\n    def
          get_minio_client(minio_secret):\n        import base64\n        from kubernetes.client.rest
          import ApiException\n\n        def get_current_namespace():\n            SA_NAMESPACE
          = \"/var/run/secrets/kubernetes.io/serviceaccount/namespace\"\n            with
          open(SA_NAMESPACE) as f:\n                return f.read()\n\n        def
          decode(text):\n            return base64.b64decode(text).decode(''utf-8'')\n\n        config.load_incluster_config()\n        api_client
          = client.ApiClient()\n\n        try:\n            secret = client.CoreV1Api(api_client).read_namespaced_secret(minio_secret,
          get_current_namespace())\n\n            minio_user = decode(secret.data[''accesskey''])\n            minio_pass
          = decode(secret.data[''secretkey''])\n\n            return Minio(minio_url,\n                         access_key=minio_user,\n                         secret_key=minio_pass,\n                         secure=False)\n        except
          ApiException as e:\n            if e.status == 404:\n                logger.error(\"Failed
          to get secret ''mlpipeline-minio-artifact'', which is needed for communicating
          with MinIO!\")\n            raise Exception(e)\n\n    logger.info(f\"Establishing
          MinIO connection to ''{minio_url}''...\")\n    minio_client = get_minio_client(minio_secret)\n\n    #
          Create export bucket if it does not yet exist\n    response = minio_client.list_buckets()\n    export_bucket_exists
          = False\n    for bucket in response:\n        if bucket.name == export_bucket:\n            export_bucket_exists
          = True\n\n    if not export_bucket_exists:\n        logger.info(f\"Creating
          bucket ''{export_bucket}''...\")\n        minio_client.make_bucket(bucket_name=export_bucket)\n\n    model_path
          = f\"{model_name}/{model_version}/model.{model_format}\"\n    s3_address
          = f\"s3://{minio_url}/{export_bucket}/{model_format}\"\n    triton_s3_address
          = f\"{s3_address}/{model_path}\"\n\n    logger.info(f\"Saving onnx file
          to MinIO (s3 address: {s3_address})...\")\n    minio_client.fput_object(\n        bucket_name=export_bucket,  #
          bucket name in Minio\n        object_name=f\"{model_format}/{model_path}\",  #
          file name in bucket of Minio / for Triton name MUST be model.onnx!\n        file_path=file_dir,  #
          file path / name in local system\n    )\n\n    logger.info(\"Finished.\")\n    out_tuple
          = namedtuple(\"UploadOutput\", [\"s3_address\", \"triton_s3_address\"])\n    return
          out_tuple(s3_address, triton_s3_address)\n\ndef _serialize_str(str_value:
          str) -> str:\n    if not isinstance(str_value, str):\n        raise TypeError(''Value
          \"{}\" has type \"{}\" instead of str.''.format(\n            str(str_value),
          str(type(str_value))))\n    return str_value\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Upload model'', description=''Uploads a
          model file to MinIO artifact store.'')\n_parser.add_argument(\"--file-dir\",
          dest=\"file_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--minio-url\",
          dest=\"minio_url\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--minio-secret\",
          dest=\"minio_secret\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--export-bucket\",
          dest=\"export_bucket\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-name\",
          dest=\"model_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-version\",
          dest=\"model_version\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-format\",
          dest=\"model_format\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=2)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = upload_model(**_parsed_args)\n\n_output_serializers
          = [\n    _serialize_str,\n    _serialize_str,\n\n]\n\nimport os\nfor idx,
          output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "quay.io/ibm/kubeflow-notebook-image-ppc64le:latest"}}, "inputs":
          [{"name": "file_dir", "type": "String"}, {"default": "minio-service.kubeflow:9000",
          "name": "minio_url", "optional": true, "type": "String"}, {"default": "mlpipeline-minio-artifact",
          "name": "minio_secret", "optional": true, "type": "String"}, {"default":
          "models", "name": "export_bucket", "optional": true, "type": "String"},
          {"default": "my-model", "name": "model_name", "optional": true, "type":
          "String"}, {"default": "1", "name": "model_version", "optional": true, "type":
          "Integer"}, {"default": "onnx", "name": "model_format", "optional": true,
          "type": "String"}], "name": "Upload model", "outputs": [{"name": "s3_address",
          "type": "String"}, {"name": "triton_s3_address", "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "a2d50683fd032a165ddeab601c5b2d94403f7899fe0563f16ab45b39d762f058", "url":
          "/home/jovyan/components/model-building/upload-model/component.yaml"}',
        pipelines.kubeflow.org/arguments.parameters: '{"export_bucket": "{{workflow.namespace}}-{{inputs.parameters.model_name}}",
          "minio_secret": "mlpipeline-minio-artifact", "minio_url": "{{inputs.parameters.minio_url}}",
          "model_format": "onnx", "model_name": "{{inputs.parameters.model_name}}",
          "model_version": "1"}', pipelines.kubeflow.org/max_cache_staleness: P0D}
  arguments:
    parameters:
    - {name: dataset_url, value: Lehrig/Monkey-Species-Collection}
    - {name: dataset_configuration, value: downsized}
    - {name: dataset_label_columns, value: '["label"]'}
    - {name: model_name, value: monkey-classification}
    - {name: cluster_configuration_secret, value: ''}
    - {name: training_gpus, value: '1'}
    - {name: number_of_workers, value: '2'}
    - {name: training_node_selector, value: ''}
    - {name: epochs, value: '100'}
    - {name: minio_url, value: 'minio-service.kubeflow:9000'}
  serviceAccountName: pipeline-runner
  volumes:
  - name: data-storage
    persistentVolumeClaim: {claimName: '{{workflow.name}}-artefacts'}
