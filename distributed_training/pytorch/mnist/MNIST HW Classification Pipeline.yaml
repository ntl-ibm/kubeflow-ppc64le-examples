apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: handwritten-digit-classification-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.18, pipelines.kubeflow.org/pipeline_compilation_time: '2023-12-05T21:30:10.680820',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "An example pipeline that
      trains using distributed pytorch", "inputs": [{"name": "notebook_pvc_name",
      "type": "String"}], "name": "Handwritten digit classification"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.18}
spec:
  entrypoint: handwritten-digit-classification
  templates:
  - name: copy-data
    container:
      args: [--source, /workspace/mnist_model.pt, --dest, /target/mnist_model.pt]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def copy_data(source, dest):
            import os
            import shutil

            # Make target directories if needed
            parent_dirs = os.path.basename(dest)
            if not os.path.exists(parent_dirs):
                os.makedirs(parent_dirs)

            if os.path.isdir(source):
                shutil.copytree(source, dest)
            else:
                shutil.copyfile(source, dest)

        import argparse
        _parser = argparse.ArgumentParser(prog='Copy data', description='')
        _parser.add_argument("--source", dest="source", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--dest", dest="dest", type=str, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = copy_data(**_parsed_args)
      image: quay.io/ntlawrence/mnist-dist-pytorch:1.0.6
      volumeMounts:
      - {mountPath: /workspace, name: create-shared-volume-for-training}
      - {mountPath: /target, name: '{{inputs.parameters.notebook_pvc_name}}'}
    inputs:
      parameters:
      - {name: create-shared-volume-for-training-name}
      - {name: notebook_pvc_name}
    nodeSelector: {ai.accelerator: V100}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: Copy Model to target
          PVC, pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--source", {"inputValue": "source"}, "--dest", {"inputValue":
          "dest"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\"
          \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def
          copy_data(source, dest):\n    import os\n    import shutil\n\n    # Make
          target directories if needed\n    parent_dirs = os.path.basename(dest)\n    if
          not os.path.exists(parent_dirs):\n        os.makedirs(parent_dirs)\n\n    if
          os.path.isdir(source):\n        shutil.copytree(source, dest)\n    else:\n        shutil.copyfile(source,
          dest)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Copy
          data'', description='''')\n_parser.add_argument(\"--source\", dest=\"source\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dest\",
          dest=\"dest\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = copy_data(**_parsed_args)\n"],
          "image": "quay.io/ntlawrence/mnist-dist-pytorch:1.0.6"}}, "inputs": [{"name":
          "source", "type": "String"}, {"name": "dest", "type": "String"}], "name":
          "Copy data"}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"dest":
          "/target/mnist_model.pt", "source": "/workspace/mnist_model.pt"}', pipelines.kubeflow.org/max_cache_staleness: P0D}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
    volumes:
    - name: create-shared-volume-for-training
      persistentVolumeClaim: {claimName: '{{inputs.parameters.create-shared-volume-for-training-name}}'}
    - name: '{{inputs.parameters.notebook_pvc_name}}'
      persistentVolumeClaim: {claimName: '{{inputs.parameters.notebook_pvc_name}}'}
  - name: create-shared-volume-for-training
    resource:
      action: create
      setOwnerReference: true
      manifest: |
        apiVersion: v1
        kind: PersistentVolumeClaim
        metadata:
          name: '{{workflow.name}}-shared-pvc'
        spec:
          accessModes:
          - ReadWriteMany
          resources:
            requests:
              storage: 4Gi
    outputs:
      parameters:
      - name: create-shared-volume-for-training-manifest
        valueFrom: {jsonPath: '{}'}
      - name: create-shared-volume-for-training-name
        valueFrom: {jsonPath: '{.metadata.name}'}
      - name: create-shared-volume-for-training-size
        valueFrom: {jsonPath: '{.status.capacity.storage}'}
    metadata:
      annotations: {pipelines.kubeflow.org/max_cache_staleness: P0D}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: handwritten-digit-classification
    inputs:
      parameters:
      - {name: notebook_pvc_name}
    dag:
      tasks:
      - name: copy-data
        template: copy-data
        dependencies: [create-shared-volume-for-training, train-and-test-model]
        arguments:
          parameters:
          - {name: create-shared-volume-for-training-name, value: '{{tasks.create-shared-volume-for-training.outputs.parameters.create-shared-volume-for-training-name}}'}
          - {name: notebook_pvc_name, value: '{{inputs.parameters.notebook_pvc_name}}'}
      - {name: create-shared-volume-for-training, template: create-shared-volume-for-training}
      - name: prepare-shared-storage
        template: prepare-shared-storage
        dependencies: [create-shared-volume-for-training]
        arguments:
          parameters:
          - {name: create-shared-volume-for-training-name, value: '{{tasks.create-shared-volume-for-training.outputs.parameters.create-shared-volume-for-training-name}}'}
      - name: train-and-test-model
        template: train-and-test-model
        dependencies: [create-shared-volume-for-training, prepare-shared-storage]
        arguments:
          parameters:
          - {name: create-shared-volume-for-training-name, value: '{{tasks.create-shared-volume-for-training.outputs.parameters.create-shared-volume-for-training-name}}'}
  - name: prepare-shared-storage
    container:
      args: []
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def prepare_shared_storage():
            from torchvision.datasets import MNIST
            import urllib

            # Download training data
            _ = MNIST("/workspace", download=True, train=True)
            _ = MNIST("/workspace", download=True, train=False)

            # Download python model training script
            r = urllib.request.urlretrieve(
                "https://raw.githubusercontent.com/ntl-ibm/kubeflow-ppc64le-examples/2.0.3/distributed_training/pytorch/mnist/mnist.py",
                "/workspace/mnist.py",
            )

        import argparse
        _parser = argparse.ArgumentParser(prog='Prepare shared storage', description='')
        _parsed_args = vars(_parser.parse_args())

        _outputs = prepare_shared_storage(**_parsed_args)
      image: quay.io/ntlawrence/mnist-dist-pytorch:1.0.6
      volumeMounts:
      - {mountPath: /workspace, name: create-shared-volume-for-training}
    inputs:
      parameters:
      - {name: create-shared-volume-for-training-name}
    nodeSelector: {ai.accelerator: V100}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\"
          \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def
          prepare_shared_storage():\n    from torchvision.datasets import MNIST\n    import
          urllib\n\n    # Download training data\n    _ = MNIST(\"/workspace\", download=True,
          train=True)\n    _ = MNIST(\"/workspace\", download=True, train=False)\n\n    #
          Download python model training script\n    r = urllib.request.urlretrieve(\n        \"https://raw.githubusercontent.com/ntl-ibm/kubeflow-ppc64le-examples/2.0.3/distributed_training/pytorch/mnist/mnist.py\",\n        \"/workspace/mnist.py\",\n    )\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Prepare shared storage'',
          description='''')\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = prepare_shared_storage(**_parsed_args)\n"], "image": "quay.io/ntlawrence/mnist-dist-pytorch:1.0.6"}},
          "name": "Prepare shared storage"}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/max_cache_staleness: P0D}
    volumes:
    - name: create-shared-volume-for-training
      persistentVolumeClaim: {claimName: '{{inputs.parameters.create-shared-volume-for-training-name}}'}
  - name: train-and-test-model
    container:
      args: [--shared-pvc-name, '{{inputs.parameters.create-shared-volume-for-training-name}}',
        --worker-image, 'quay.io/ntlawrence/mnist-dist-pytorch:1.0.6', '----output-paths',
        /tmp/outputs/mlpipeline_metrics/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def train_and_test_model(
            shared_pvc_name, worker_image
        ):

            import json
            import distributed_kf_tools.deploy as deploy
            from distributed_kf_tools.template import OwningWorkFlow, PvcMount
            from collections import namedtuple

            for retries in range(5):
                try:
                    ## Start the PyTorch job for distributed training
                    job_name = "{{workflow.name}}" + (f"-{retries:03d}" if retries else "")
                    deploy.run_pytorch_job(
                        # owning_workflow setups it up so that when the pipeline is deleted,
                        # the training job is cleaned up
                        owning_workflow=OwningWorkFlow(
                            name="{{workflow.name}}", uid="{{workflow.uid}}"
                        ),
                        # These place holders for namespace and job name are
                        # filled in by Kubeflow when the pipeline runs.
                        namespace="{{workflow.namespace}}",
                        pytorch_job_name=job_name,
                        # Shared volumes used by the training script
                        pvcs=[
                            PvcMount(
                                pvc_name=(shared_pvc_name),
                                mount_path="/workspace",
                            )
                        ],
                        # The command to run in each worker
                        # This almost always starts with "torch.distributed.run" for DDP
                        command=[
                            "python",
                            "-m",
                            "torch.distributed.run",
                            "/workspace/mnist.py",
                            "--root_dir=/workspace",
                            "--data_dir=/workspace",
                            "--model=/workspace/mnist_model.pt",
                            "--batch_size=672",
                            "--evaluation_metrics=/workspace/metrics.json",
                            "--checkpoint",
                            "--max_epochs=15",
                        ],
                        # Number of workers
                        num_workers=3,
                        # Number of GPUs per worker (OK to leave this at 1)
                        gpus_per_worker=1,
                        # The base image used for the worker pods
                        worker_image=worker_image,
                    )
                    break
                except RuntimeError as e:
                    print(f"THE JOB FAILED BECAUSE OF ERROR {e}")

            # Return test metrics from the trial
            # Kubeflow can use this information to compare trial runs
            with open("/workspace/metrics.json") as f:
                jsonstr = f.read()
                print(jsonstr)
                trial_metrics = json.loads(jsonstr)

            metrics = {
                "metrics": [
                    {"name": "f1", "numberValue": trial_metrics["test_f1"], "format": "RAW"},
                    {
                        "name": "accuracy",
                        "numberValue": trial_metrics["test_acc"],
                        "format": "PERCENTAGE",
                    },
                ]
            }

            out_tuple = namedtuple("EvaluationOutput", ["mlpipeline_metrics"])
            return out_tuple(json.dumps(metrics))

        import argparse
        _parser = argparse.ArgumentParser(prog='Train and test model', description='')
        _parser.add_argument("--shared-pvc-name", dest="shared_pvc_name", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--worker-image", dest="worker_image", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = train_and_test_model(**_parsed_args)

        _output_serializers = [
            str,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      image: quay.io/ntlawrence/mnist-dist-pytorch:1.0.6
      volumeMounts:
      - {mountPath: /workspace, name: create-shared-volume-for-training}
    inputs:
      parameters:
      - {name: create-shared-volume-for-training-name}
    outputs:
      artifacts:
      - {name: mlpipeline-metrics, path: /tmp/outputs/mlpipeline_metrics/data}
    nodeSelector: {ai.accelerator: V100}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: Train and Test Model,
        pipelines.kubeflow.org/component_spec: '{"implementation": {"container": {"args":
          ["--shared-pvc-name", {"inputValue": "shared_pvc_name"}, "--worker-image",
          {"inputValue": "worker_image"}, "----output-paths", {"outputPath": "mlpipeline_metrics"}],
          "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" >
          \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def train_and_test_model(\n    shared_pvc_name,
          worker_image\n):\n\n    import json\n    import distributed_kf_tools.deploy
          as deploy\n    from distributed_kf_tools.template import OwningWorkFlow,
          PvcMount\n    from collections import namedtuple\n\n    for retries in range(5):\n        try:\n            ##
          Start the PyTorch job for distributed training\n            job_name = \"{{workflow.name}}\"
          + (f\"-{retries:03d}\" if retries else \"\")\n            deploy.run_pytorch_job(\n                #
          owning_workflow setups it up so that when the pipeline is deleted,\n                #
          the training job is cleaned up\n                owning_workflow=OwningWorkFlow(\n                    name=\"{{workflow.name}}\",
          uid=\"{{workflow.uid}}\"\n                ),\n                # These place
          holders for namespace and job name are\n                # filled in by Kubeflow
          when the pipeline runs.\n                namespace=\"{{workflow.namespace}}\",\n                pytorch_job_name=job_name,\n                #
          Shared volumes used by the training script\n                pvcs=[\n                    PvcMount(\n                        pvc_name=(shared_pvc_name),\n                        mount_path=\"/workspace\",\n                    )\n                ],\n                #
          The command to run in each worker\n                # This almost always
          starts with \"torch.distributed.run\" for DDP\n                command=[\n                    \"python\",\n                    \"-m\",\n                    \"torch.distributed.run\",\n                    \"/workspace/mnist.py\",\n                    \"--root_dir=/workspace\",\n                    \"--data_dir=/workspace\",\n                    \"--model=/workspace/mnist_model.pt\",\n                    \"--batch_size=672\",\n                    \"--evaluation_metrics=/workspace/metrics.json\",\n                    \"--checkpoint\",\n                    \"--max_epochs=15\",\n                ],\n                #
          Number of workers\n                num_workers=3,\n                # Number
          of GPUs per worker (OK to leave this at 1)\n                gpus_per_worker=1,\n                #
          The base image used for the worker pods\n                worker_image=worker_image,\n            )\n            break\n        except
          RuntimeError as e:\n            print(f\"THE JOB FAILED BECAUSE OF ERROR
          {e}\")\n\n    # Return test metrics from the trial\n    # Kubeflow can use
          this information to compare trial runs\n    with open(\"/workspace/metrics.json\")
          as f:\n        jsonstr = f.read()\n        print(jsonstr)\n        trial_metrics
          = json.loads(jsonstr)\n\n    metrics = {\n        \"metrics\": [\n            {\"name\":
          \"f1\", \"numberValue\": trial_metrics[\"test_f1\"], \"format\": \"RAW\"},\n            {\n                \"name\":
          \"accuracy\",\n                \"numberValue\": trial_metrics[\"test_acc\"],\n                \"format\":
          \"PERCENTAGE\",\n            },\n        ]\n    }\n\n    out_tuple = namedtuple(\"EvaluationOutput\",
          [\"mlpipeline_metrics\"])\n    return out_tuple(json.dumps(metrics))\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Train and test model'',
          description='''')\n_parser.add_argument(\"--shared-pvc-name\", dest=\"shared_pvc_name\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--worker-image\",
          dest=\"worker_image\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = train_and_test_model(**_parsed_args)\n\n_output_serializers
          = [\n    str,\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "quay.io/ntlawrence/mnist-dist-pytorch:1.0.6"}}, "inputs": [{"name":
          "shared_pvc_name", "type": "String"}, {"name": "worker_image", "type": "String"}],
          "name": "Train and test model", "outputs": [{"name": "mlpipeline_metrics",
          "type": "Metrics"}]}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"shared_pvc_name":
          "{{inputs.parameters.create-shared-volume-for-training-name}}", "worker_image":
          "quay.io/ntlawrence/mnist-dist-pytorch:1.0.6"}', pipelines.kubeflow.org/max_cache_staleness: P0D}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
    volumes:
    - name: create-shared-volume-for-training
      persistentVolumeClaim: {claimName: '{{inputs.parameters.create-shared-volume-for-training-name}}'}
  arguments:
    parameters:
    - {name: notebook_pvc_name}
  serviceAccountName: pipeline-runner
