{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "841ff471-215d-4984-a0f5-b88fd13a5b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4038e527-8023-445d-849d-123cae61e46c",
   "metadata": {},
   "source": [
    "# Distributed training example using PyTorch\n",
    "\n",
    "This notebook trains, evaluates and deploys a classifier for handwritten digits (Using the MNIST dataset). The training is distributed across multiple GPUs using DDP.\n",
    "\n",
    "\n",
    "Author: Nick Lawrence ntl@us.ibm.com\n",
    "License: [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "## Base container Image\n",
    "The components of this pipeline use a a container image that has been built for this example. The Dockerfile is included in the [container_image](./container_image) directory. \n",
    "\n",
    "The container image does *not* include a notebook server, meaning you cannot use the base image for a notebook server. A new container image was created so that PyTorch 2.0 and Python 3.10 could be used in the example. These packages were not available when the notebook server images were built. The image also includes pytorch-lightning, which makes implementing a model for distributed training simpler.\n",
    "\n",
    "The image also has installed the pytorch_distributed_kf_tools, which simplifies the creation and deployment of the PyTorch Job. The source code for this package is in [../../distributed_kf_tools](../../distributed_kf_tools).\n",
    "\n",
    "The base image does not include the python code for the PyTorch model. This is cloned from github into a pvc, you can find the source [in the ./src directory](./src)\n",
    "\n",
    "## Workspace\n",
    "In addition to using parameters to pass information between components, a shared workspace is mounted to many of the components.\n",
    "\n",
    "The workspace is used to:\n",
    "* Store python implementation of the model and training code (after downloading from github)\n",
    "* Store training and test data for the MNIST dataset (downloaded from pytroch)\n",
    "* Store checkpoints created during the training process\n",
    "* Store tensorboard logs that are created during training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd2949a-1204-45aa-bbe7-aa354eafcbb1",
   "metadata": {},
   "source": [
    "## Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3deb0460-f169-4bed-8d16-cba226a651bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp.components import InputPath, OutputPath\n",
    "import kfp.dsl as dsl\n",
    "from kfp.dsl import PipelineConf, data_passing_methods\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import NamedTuple, Dict\n",
    "\n",
    "BASE_IMAGE = \"quay.io/ntlawrence/pytorch:1.0.1\"\n",
    "COMPONENT_CATALOG_FOLDER = f\"{os.getenv('HOME')}/components\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d438e6a8-4fcc-461e-a9ca-1db72364a1e2",
   "metadata": {},
   "source": [
    "## Node Selectors\n",
    "\n",
    "An important observation of GPUs on Power 9 nodes is that each node has only one type of GPU. These constants are used to ensure that pipeline tasks or inference servers run on the correct type of node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b424167f-b102-4a60-885c-ab439b97ef95",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_NODE_LABEL = \"nvidia.com/gpu.product\"\n",
    "INFERENCE_GPU_PRODUCT = \"Tesla-V100-SXM2-32GB\"\n",
    "TRAINING_GPU_PRODUCT = \"Tesla-V100-SXM2-32GB\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa02571-71b6-458f-9339-69ca4ba49b5e",
   "metadata": {},
   "source": [
    "# Define the component that prepares the workspace\n",
    "\n",
    "This component prepares the shared workspace by:\n",
    "* downloading the training and test data\n",
    "* cloning the github repo that contains the python code for model and training\n",
    "* creating the directory for the tensorboard logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b552cf95-1480-4b5e-a3d7-88ce6153b8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_workspace(\n",
    "    github_repo: str = \"https://github.com/ntl-ibm/kubeflow-ppc64le-examples.git\",\n",
    "    branch: str = \"pytorch_example_improvements\",\n",
    "    workspace_mount_point: str = \"/workspace\",\n",
    ") -> NamedTuple(\n",
    "    \"CreatedPaths\", [(\"data\", str), (\"code\", str), (\"tensorboard_logs\", str)]\n",
    "):\n",
    "    \"\"\"\n",
    "    Prepares the workspace with a directory structure\n",
    "\n",
    "    params:\n",
    "        github_repo - repo to clone\n",
    "        branch - branch or tag from the repo to clone\n",
    "\n",
    "    returns:\n",
    "        Tuple of paths that can be referenced by later components (The mount point is not included in the path)\n",
    "          data - where the data has been downloaded\n",
    "          code - where the repo has been cloned into\n",
    "          tensorboard_logs - empty directory for storing tensorboard logs\n",
    "    \"\"\"\n",
    "\n",
    "    import subprocess\n",
    "    import os\n",
    "    from collections import namedtuple\n",
    "\n",
    "    from torchvision.datasets import MNIST\n",
    "\n",
    "    # Download data\n",
    "    _ = MNIST(f\"{workspace_mount_point}/data\", download=True, train=True)\n",
    "\n",
    "    # Clone git repo\n",
    "    subprocess.run(\n",
    "        f\"git clone {github_repo}  {workspace_mount_point}/code -b {branch}\",\n",
    "        shell=True,\n",
    "        check=True,\n",
    "    )\n",
    "\n",
    "    os.mkdir(f\"{workspace_mount_point}/tensorboard_logs\")\n",
    "\n",
    "    out_tuple = namedtuple(\"CreatedPaths\", [\"data\", \"code\", \"tensorboard_logs\"])\n",
    "\n",
    "    # These are the paths withing the PVC that caller cares about\n",
    "    return out_tuple(\"data\", \"code\", \"tensorboard_logs\")\n",
    "\n",
    "\n",
    "prepare_workspace_comp = kfp.components.create_component_from_func(\n",
    "    prepare_workspace, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd55a9f-6362-4b9c-94b6-86e45ee6641b",
   "metadata": {},
   "source": [
    "# Define a component to create a tensorboard\n",
    "\n",
    "Our training code logs data to a tensorboard, which graphs training metrics in real time as the training progresses.\n",
    "\n",
    "The \"Visualizations\" tab of the component provides a link to the TensorBoard service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "243b27f6-70e4-4e39-aa35-938aa65461ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIGURE_TENSORBOARD_COMPONENT = f\"{os.getenv('HOME')}/kubeflow-ppc64le-examples/configure_tensorboard_component/configure_tensorboard_component.yaml\"\n",
    "\n",
    "configure_tensorboard_comp = kfp.components.load_component_from_file(\n",
    "    CONFIGURE_TENSORBOARD_COMPONENT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a834c6-abb8-43b1-80f0-39283c03f128",
   "metadata": {},
   "source": [
    "# Define a component to train the model\n",
    "\n",
    "This example uses a pytorch job with DDP to perform distributed training.\n",
    "\n",
    "The component creats a pytorch job that manages the workers performing distributed training. A diagram of this setup is shown here. The tasks in the kubeflow pipeline are on the right, the PyTorchJob custom resource is in the middle, and the worker jobs are shown on the left.\n",
    "\n",
    "\n",
    "![Diagram of pytorch job](./pytorchjob.jpg)\n",
    "\n",
    "The component uses a run_pytorch_job python method to work with the pytorch job, the python package is built into the container image, with the source code being available [here](../distributed_kf_tools).\n",
    "The run_pytorch_job method performs these basic tasks:\n",
    "* Construct pytorch job template\n",
    "* Create pytorch job resource\n",
    "* Wait until the resource indicates that training is complete\n",
    "* Report errors if training fails\n",
    "* Terminate training if the pipeline is terminated.\n",
    "* Stream training logs to the train_model task's logs.\n",
    "\n",
    "Our training reoutine includes checkpointing. Checkpoints are written to the workspace at each epoch, if an error occurs, training is resumed from the previous checkpoint. The training component will fail if more than 5 retries occur.\n",
    "\n",
    "The pytorch job is cleaned up at the end of training.\n",
    "\n",
    "Metrics are logged to the tensorboard, and can be viewed in real time.\n",
    "\n",
    "The workspace pvc name is passed as a parameter, so that the PyTorchJob can mount that PVC on each worker. The path to the best model's checkpoint is returned as an output parameter from the train_model task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87c120ff-ef8a-4b09-b823-ae743bb867c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    workspace_pvc_name: str,\n",
    "    worker_image: str,\n",
    "    tensorboard_path: str,\n",
    "    data_path: str,\n",
    "    working_dir: str,\n",
    "    num_workers: int = 3,\n",
    "    batch_size: int = 672,\n",
    "    max_epochs: int = 50,\n",
    "    node_selector: Dict[str, str] = None,\n",
    ") -> NamedTuple(\"ModelCkptPath\", [(\"best\", str)]):\n",
    "\n",
    "    import distributed_kf_tools.deploy as deploy\n",
    "    from distributed_kf_tools.template import OwningWorkFlow, PvcMount\n",
    "    import subprocess\n",
    "    from collections import namedtuple\n",
    "\n",
    "    workspace_mount_point = \"/workspace\"\n",
    "\n",
    "    success = False\n",
    "    for retries in range(5):\n",
    "        try:\n",
    "            ## Start the PyTorch job for distributed training\n",
    "            job_name = \"{{workflow.name}}\" + (f\"-{retries:03d}\" if retries else \"\")\n",
    "            deploy.run_pytorch_job(\n",
    "                # owning_workflow setups it up so that when the pipeline is deleted,\n",
    "                # the training job is cleaned up\n",
    "                owning_workflow=OwningWorkFlow(\n",
    "                    name=\"{{workflow.name}}\", uid=\"{{workflow.uid}}\"\n",
    "                ),\n",
    "                # These place holders for namespace and job name are\n",
    "                # filled in by Kubeflow when the pipeline runs.\n",
    "                namespace=\"{{workflow.namespace}}\",\n",
    "                pytorch_job_name=job_name,\n",
    "                # Shared volumes used by the training script\n",
    "                pvcs=[\n",
    "                    PvcMount(\n",
    "                        pvc_name=(workspace_pvc_name),\n",
    "                        mount_path=workspace_mount_point,\n",
    "                    )\n",
    "                ],\n",
    "                working_dir=f\"{workspace_mount_point}/{working_dir}\",\n",
    "                # The command to run in each worker\n",
    "                # This starts with \"torch.distributed.run\" for DDP\n",
    "                # The output model path needs to be stored on the pvc,\n",
    "                # The output path for this POD points the tmp directory of THIS pod,\n",
    "                # Storing it in the temp directory of the PyTorch worker pod doesn't work,\n",
    "                # because this pod doesn't have access to that.\n",
    "                command=[\n",
    "                    \"python\",\n",
    "                    \"-m\",\n",
    "                    \"torch.distributed.run\",\n",
    "                    \"train.py\",\n",
    "                    \"--root_dir=./work\",\n",
    "                    f\"--data_dir={workspace_mount_point}/{data_path}\",\n",
    "                    f\"--model_ckpt={workspace_mount_point}/models/best.ckpt\",\n",
    "                    f\"--batch_size={batch_size}\",\n",
    "                    \"--checkpoint\",\n",
    "                    \"--early_stopping\",\n",
    "                    f\"--max_epochs={max_epochs}\",\n",
    "                    \"--pytorchjob\",\n",
    "                    f\"--tensorboard={workspace_mount_point}/{tensorboard_path}\",\n",
    "                ],\n",
    "                # Number of workers (pods)\n",
    "                num_workers=num_workers,\n",
    "                # Number of GPUs per worker (OK to leave this at 1)\n",
    "                gpus_per_worker=1,\n",
    "                # The base image used for the worker pods\n",
    "                worker_image=worker_image,\n",
    "                node_selector=node_selector,\n",
    "            )\n",
    "\n",
    "            success = True\n",
    "            break\n",
    "        except RuntimeError as e:\n",
    "            print(f\"THE JOB FAILED BECAUSE OF ERROR {e}\")\n",
    "\n",
    "    if not success:\n",
    "        raise RuntimeError(\"Training has failed\")\n",
    "\n",
    "    out_tuple = namedtuple(\"ModelCkptPath\", [\"best\"])\n",
    "    return out_tuple(\"models/best.ckpt\")\n",
    "\n",
    "\n",
    "train_model_comp = kfp.components.create_component_from_func(\n",
    "    train_model,\n",
    "    base_image=BASE_IMAGE,\n",
    "    packages_to_install=[\n",
    "        \"git+https://github.com/ntl-ibm/kubeflow-ppc64le-examples@3.0.0#subdirectory=distributed_training/distributed_kf_tools\"\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d50c9cc-2d7b-4744-aff9-cf3db1c7e633",
   "metadata": {},
   "source": [
    "# Define a component to test the model\n",
    "\n",
    "This runs the python test script.\n",
    "\n",
    "The metrics from the testing are returned as an output parameter (mlpipeline_metrics). This will allow the metrics to appear on the experiment page. This figure shows the acc (accuracy) and F1 metrics as columns in the experiment runs window.\n",
    "\n",
    "![metrics](./experiment_metrics.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "576c315b-0d8b-4077-88f6-e915326a47c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(\n",
    "    data_path: str,\n",
    "    working_dir: str,\n",
    "    model_path: str,\n",
    "    workspace_mount_point: str = \"/workspace\",\n",
    "    batch_size: int = 224,\n",
    ") -> NamedTuple(\"EvaluationOutput\", [(\"mlpipeline_metrics\", \"Metrics\")]):\n",
    "    import json\n",
    "    from collections import namedtuple\n",
    "    import subprocess\n",
    "\n",
    "    subprocess.run(\n",
    "        [\n",
    "            \"python\",\n",
    "            \"test.py\",\n",
    "            \"--root_dir=/tmp\",\n",
    "            f\"--data_dir={workspace_mount_point}/{data_path}\",\n",
    "            f\"--model_ckpt={workspace_mount_point}/{model_path}\",\n",
    "            \"--batch_size=672\",\n",
    "            \"--metrics_json=/tmp/metrics.json\",\n",
    "        ],\n",
    "        cwd=f\"{workspace_mount_point}/{working_dir}\",\n",
    "        check=True,\n",
    "    )\n",
    "\n",
    "    with open(\"/tmp/metrics.json\", \"r\") as f:\n",
    "        metrics = json.load(f)\n",
    "\n",
    "    metrics = {\n",
    "        \"metrics\": [\n",
    "            {\"name\": \"acc\", \"numberValue\": metrics[\"test/acc\"], \"format\": \"RAW\"},\n",
    "            {\"name\": \"F1\", \"numberValue\": metrics[\"test/F1\"], \"format\": \"RAW\"},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    out_tuple = namedtuple(\"EvaluationOutput\", [\"mlpipeline_metrics\"])\n",
    "    return out_tuple(json.dumps(metrics))\n",
    "\n",
    "\n",
    "test_model_comp = kfp.components.create_component_from_func(\n",
    "    test_model, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b108724-94a2-4ca2-a300-03df642cc2f8",
   "metadata": {},
   "source": [
    "# Define a component to convert the model to ONNX\n",
    "\n",
    "ONNX is a standard formant for Neural Networks. In this pipeline, we convert to ONNX because we have an inference server (Triton) that can handle ONNX models. It's easier to convert to ONNX and use Triton than to build a PyTorch Inference Service.\n",
    "\n",
    "When a pytorch model is converted to ONNX, the conversion involves both static analysis of the python code, and also tracing of a reference data set through the model.  The mechanism to do this is part of pytorch, and can be accomplished with a simple function call.\n",
    "\n",
    "As with the training and test process, we've contained that logic in a python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f9f51c7-a9bb-44f0-9347-763b7971db7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_onnx(\n",
    "    onnx_model: OutputPath(str),\n",
    "    data_path: str,\n",
    "    working_dir: str,\n",
    "    model_path: str,\n",
    "    workspace_mount_point: str = \"/workspace\",\n",
    "):\n",
    "    import shutil\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    import subprocess\n",
    "\n",
    "    subprocess.run(\n",
    "        [\n",
    "            \"python\",\n",
    "            \"convert.py\",\n",
    "            \"--root_dir=/tmp\",\n",
    "            f\"--data_dir={workspace_mount_point}/{data_path}\",\n",
    "            f\"--model_ckpt={workspace_mount_point}/{model_path}\",\n",
    "            \"--onnx=/tmp/model.onnx\",\n",
    "        ],\n",
    "        cwd=f\"{workspace_mount_point}/{working_dir}\",\n",
    "        check=True,\n",
    "    )\n",
    "\n",
    "    directory = str(Path(onnx_model).parent.absolute())\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    shutil.copy(\"/tmp/model.onnx\", onnx_model)\n",
    "\n",
    "\n",
    "convert_to_onnx_comp = kfp.components.create_component_from_func(\n",
    "    convert_to_onnx, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba28815-9785-405e-88fb-ea02edefd9ce",
   "metadata": {},
   "source": [
    "# Define a component to upload the model to MinIO.\n",
    "\n",
    "Storing models in object storage is very common, MinIO is an Object Store that comes with KubeFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da15a36f-cf9a-4887-8c53-60a7a53f619f",
   "metadata": {},
   "outputs": [],
   "source": [
    "UPLOAD_MODEL_COMPONENT = (\n",
    "    f\"{COMPONENT_CATALOG_FOLDER}/model-building/upload-model/component.yaml\"\n",
    ")\n",
    "\n",
    "upload_model_comp = kfp.components.load_component_from_file(UPLOAD_MODEL_COMPONENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fb44b0-83fc-4a84-b496-2217788556e9",
   "metadata": {},
   "source": [
    "# Define a component to depoly the Inference Service\n",
    "\n",
    "We use Triton in this example. This example is NOT setup to use a GPU for inferencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "203646a8-3015-4728-9d2f-101b58b29a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPLOY_INFERENCE_SERVICE_COMPONENT = f\"{os.getenv('HOME')}/kubeflow-ppc64le-examples/deploy_triton_inference_service_component/deploy_triton_inference_service_component.yaml\"\n",
    "\n",
    "deploy_inference_service_comp = kfp.components.load_component_from_file(\n",
    "    DEPLOY_INFERENCE_SERVICE_COMPONENT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fa62e7-2685-44cb-b0f0-214925abc60b",
   "metadata": {},
   "source": [
    "# Define the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a3a5224-30e5-4c65-9b81-745835047d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_NAME = \"Mnist-HW\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31a8707d-77db-47b6-b029-1a3ba605edf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=PIPELINE_NAME,\n",
    "    description=\"An example pipeline that trains using distributed pytorch\",\n",
    ")\n",
    "def mnist_pipeline(\n",
    "    github_repo: str = \"https://github.com/ntl-ibm/kubeflow-ppc64le-examples.git\",\n",
    "    branch: str = \"3.0.0\",\n",
    "    context: str = \"distributed_training/pytorch/mnist/src\",\n",
    "):\n",
    "\n",
    "    workspace_volume_volop = dsl.VolumeOp(\n",
    "        name=\"Create workspace for training\",\n",
    "        resource_name=\"shared-workspace-pvc\",\n",
    "        modes=dsl.VOLUME_MODE_RWM,\n",
    "        size=\"4Gi\",\n",
    "        set_owner_reference=True,\n",
    "    )\n",
    "\n",
    "    prepare_workspace_task = prepare_workspace_comp(\n",
    "        github_repo, branch, workspace_mount_point=\"/workspace\"\n",
    "    )\n",
    "    prepare_workspace_task.add_pvolumes({\"/workspace\": workspace_volume_volop.volume})\n",
    "\n",
    "    configure_tensorboard_task = configure_tensorboard_comp(\n",
    "        pipeline_name=PIPELINE_NAME,\n",
    "        pvc_name=workspace_volume_volop.volume.persistent_volume_claim.claim_name,\n",
    "        pvc_path=prepare_workspace_task.outputs[\"tensorboard_logs\"],\n",
    "    )\n",
    "\n",
    "    train_model_task = train_model_comp(\n",
    "        workspace_pvc_name=workspace_volume_volop.volume.persistent_volume_claim.claim_name,\n",
    "        worker_image=BASE_IMAGE,\n",
    "        tensorboard_path=prepare_workspace_task.outputs[\"tensorboard_logs\"],\n",
    "        data_path=prepare_workspace_task.outputs[\"data\"],\n",
    "        working_dir=f\"{prepare_workspace_task.outputs['code']}/{context}\",\n",
    "        node_selector={GPU_NODE_LABEL: TRAINING_GPU_PRODUCT},\n",
    "    )\n",
    "    train_model_task.after(configure_tensorboard_task)\n",
    "\n",
    "    test_model_task = test_model_comp(\n",
    "        data_path=prepare_workspace_task.outputs[\"data\"],\n",
    "        working_dir=f\"{prepare_workspace_task.outputs['code']}/{context}\",\n",
    "        model_path=train_model_task.outputs[\"best\"],\n",
    "        workspace_mount_point=\"/workspace\",\n",
    "    )\n",
    "    test_model_task.add_pvolumes({\"/workspace\": workspace_volume_volop.volume})\n",
    "\n",
    "    convert_to_onnx_task = convert_to_onnx_comp(\n",
    "        data_path=prepare_workspace_task.outputs[\"data\"],\n",
    "        working_dir=f\"{prepare_workspace_task.outputs['code']}/{context}\",\n",
    "        model_path=train_model_task.outputs[\"best\"],\n",
    "        workspace_mount_point=\"/workspace\",\n",
    "    )\n",
    "    convert_to_onnx_task.add_pvolumes({\"/workspace\": workspace_volume_volop.volume})\n",
    "\n",
    "    model_name = \"mnist\"\n",
    "    minio_url = \"minio-service.kubeflow:9000\"\n",
    "\n",
    "    upload_model_task = upload_model_comp(\n",
    "        convert_to_onnx_task.outputs[\"onnx_model\"],\n",
    "        minio_url=minio_url,\n",
    "        export_bucket=\"{{workflow.namespace}}-models\",\n",
    "        model_format=\"onnx\",\n",
    "        model_name=model_name,\n",
    "        model_version=1,\n",
    "    )\n",
    "\n",
    "    deploy_model_task = deploy_inference_service_comp(\n",
    "        name=model_name,\n",
    "        rm_existing=True,\n",
    "        storage_uri=\"s3://{{workflow.namespace}}-models/onnx/\",\n",
    "        minio_url=minio_url,\n",
    "        concurrency_target=4,  # soft limit, may be exceeded for short periods of time\n",
    "        predictor_min_replicas=0,  # min_replicas supports scale to 0\n",
    "        predictor_max_replicas=4,  # We don't want to scale till we consume all the available GPUs\n",
    "        predictor_protocol=\"v2\",\n",
    "    )\n",
    "    deploy_model_task.after(upload_model_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b121e2-6498-45a1-8391-d1936c532f1b",
   "metadata": {},
   "source": [
    "# Configure Pipeline\n",
    "\n",
    "The first transformer disables caching for our pipeline. Kubeflow caches tasks based on input and output parameters. Because we are using shared storage, the input parameters are the same as previous runs, however the data on the shared storage is most likely different.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a36cd67f-8f7d-4186-85f8-a09935fcd297",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_conf = kfp.dsl.PipelineConf()\n",
    "\n",
    "# Disable Caching\n",
    "def disable_cache_transformer(op: dsl.ContainerOp):\n",
    "    if isinstance(op, dsl.ContainerOp):\n",
    "        op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    else:\n",
    "        op.add_pod_annotation(\n",
    "            name=\"pipelines.kubeflow.org/max_cache_staleness\", value=\"P0D\"\n",
    "        )\n",
    "    return op\n",
    "\n",
    "\n",
    "pipeline_conf.add_op_transformer(disable_cache_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc1de34-2104-4cfe-a923-aa3177d549fa",
   "metadata": {},
   "source": [
    "# Compile, upload and run pipeline\n",
    "\n",
    "This creates a run of the pipeline within an experiment.\n",
    "\n",
    "Once the pipeline is uploaded, you can run it from the Kubeflow UI with parameters. Here we run the pipeline programatically, so othat we can wait for it to finish before running other cells to demo inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d6ca018-cbbc-4a36-9135-6b2ecc9d48d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pipeline(pipeline_name: str):\n",
    "    \"\"\"Delete's a pipeline with the specified name\"\"\"\n",
    "\n",
    "    client = kfp.Client()\n",
    "    existing_pipelines = client.list_pipelines(page_size=999).pipelines\n",
    "    matches = (\n",
    "        [ep.id for ep in existing_pipelines if ep.name == pipeline_name]\n",
    "        if existing_pipelines\n",
    "        else []\n",
    "    )\n",
    "    for id in matches:\n",
    "        client.delete_pipeline(id)\n",
    "\n",
    "\n",
    "def get_experiment_id(experiment_name: str) -> str:\n",
    "    \"\"\"Returns the id for the experiment, creating the experiment if needed\"\"\"\n",
    "    client = kfp.Client()\n",
    "    existing_experiments = client.list_experiments(page_size=999).experiments\n",
    "    matches = (\n",
    "        [ex.id for ex in existing_experiments if ex.name == experiment_name]\n",
    "        if existing_experiments\n",
    "        else []\n",
    "    )\n",
    "\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "\n",
    "    exp = client.create_experiment(experiment_name)\n",
    "    return exp.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c00906ed-c38f-4159-bac8-ee8dd1b50689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=/pipeline/#/pipelines/details/2fae5709-8a35-4693-9c9b-8581c669812a>Pipeline details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/3aa37a92-6b7f-43e9-bb66-3f0e71d337bf\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PIPELINE_NAME = \"train-mnist-pytorch-model\"\n",
    "\n",
    "client = kfp.Client()\n",
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=mnist_pipeline,\n",
    "    package_path=f\"{PIPELINE_NAME}.yaml\",\n",
    "    pipeline_conf=pipeline_conf,\n",
    ")\n",
    "\n",
    "delete_pipeline(PIPELINE_NAME)\n",
    "uploaded_pipeline = client.upload_pipeline(f\"{PIPELINE_NAME}.yaml\", PIPELINE_NAME)\n",
    "run = client.run_pipeline(\n",
    "    experiment_id=get_experiment_id(\"mnist-pytorch-exp\"),\n",
    "    job_name=\"mnist-pytorch\",\n",
    "    pipeline_id=uploaded_pipeline.id,\n",
    "    params={},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2961511-7b46-47ac-9763-da77e043ade4",
   "metadata": {},
   "source": [
    "# Wait for pipeline completion\n",
    "\n",
    "The complention report includes both the status of the run, and also metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9bd57c2-35d1-4f73-b5c1-7ff08fb3ad21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'Succeeded',\n",
       " 'error': None,\n",
       " 'time': '0:11:38',\n",
       " 'metrics': [{'format': 'RAW',\n",
       "   'name': 'acc',\n",
       "   'node_id': 'mnist-hw-jtl64-3979504309',\n",
       "   'number_value': 0.9797000288963318},\n",
       "  {'format': 'RAW',\n",
       "   'name': 'F1',\n",
       "   'node_id': 'mnist-hw-jtl64-3979504309',\n",
       "   'number_value': 0.9795413017272949}]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TWENTY_MIN = 20 * 60\n",
    "result = client.wait_for_run_completion(run.id, timeout=TWENTY_MIN)\n",
    "{\n",
    "    \"status\": result.run.status,\n",
    "    \"error\": result.run.error,\n",
    "    \"time\": str(result.run.finished_at - result.run.created_at),\n",
    "    \"metrics\": result.run.metrics,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a9a86b-848d-491c-a4d2-2751e6065a8b",
   "metadata": {},
   "source": [
    "# Make a prediction\n",
    "\n",
    "This section uses the inference service to make a prediction.\n",
    "\n",
    "Our inference service is setup to use a REST protocol, this is not as fast as a GRPC protocol, but it is easier to work with since the input and output data is JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6484eff0-bb2d-4c9b-84a3-43efb55c7b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a0f497-7702-480a-bb08-9a20d979a8bb",
   "metadata": {},
   "source": [
    "## Load image\n",
    "\n",
    "Here we display the image, and convert to an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e370e91c-2c01-4634-8abf-383554287d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABq0lEQVR4nGWSPWtUURCGnzkfe/feu3uVdbcQFgMRQUmlYJfaFCK2YlBrsUkhFmlFtBER0kjSpLMRBMXSH2Anoo1CkCgoyeIu2Q/3wzsWN5tkT6aaw8PMeeedgSAEjDvM9xOjObg4H05EnRmFNc4WSYVIAmYs4KtJDEA2wxzgHZAYauVQCTB394MOBxtN8H4GZMjVd7m2/upgo+hzNOYf/dL22tLc6g9tHuv5oPXvRYO0fEvHEXEA68/OVtOTnNf+7brBzKqtYuHGyx39lGBsOGhNuDdQ/XoxMzZAGaxttycD7d7EUwpoQidXfd1TXYbaLHOU03NV7OL73d610KEIAA9ZZ+/59JmACAKcgBRDY3W4vW+R6RuvUlYSOtgeJdNql35SmRT9xFghWVmOja9hMdLrPpx6KwYnLOxupuABtzLQKwcXkoKpXN57Q2SrUF/8M15P3KFMEXOm9RbALTz+rh8Bb6EoT/r4b42nW93o0tKF8as7o1OtowOmbI50mA9150mTDFxxCAKimKh0f/76l8+/17fyGWtEAFeG0xAHe5yGR8T4Yydw8HUUkv/uS3eJ50exBAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with Image.open(\"3.jpg\") as i:\n",
    "    i.show()\n",
    "    image_array = np.asarray(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018040d5-f3e7-4fef-a725-03da69af6694",
   "metadata": {},
   "source": [
    "## Shape\n",
    "\n",
    "The shape of the loaded image is 28x28, however our model is built with a channels dimension, even though we only have a single channel.\n",
    "\n",
    "We also need to add a batch dimension to get the shape to be (1,1,28,28)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee64f75e-ff7e-4a0f-be0a-16efa8f1c0dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c70b231-ebc1-4dc3-a6b4-7ec36c2cf127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# channels dimension is required, even though the shape is always (1,28,28)\n",
    "tensor = np.expand_dims(image_array, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4615c807-ccd8-4b85-acdd-58b9625555e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 28, 28)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch dimension required\n",
    "tensor = np.expand_dims(tensor, 0)\n",
    "tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35f91c9-2229-4c08-84ce-7e53ac773646",
   "metadata": {},
   "source": [
    "## Get service details\n",
    "\n",
    "We can verify the inference service has been created and is ready with an OC command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ee5a553-a37f-419b-9a34-5e7a1819d14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME    URL                                                     READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION             AGE\n",
      "mnist   http://mnist-ntl-us-ibm-com.apps.openshift.sopenshift   True           100                              mnist-predictor-default-00001   43s\n"
     ]
    }
   ],
   "source": [
    "!oc get inferenceservice mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e70f87-1464-4729-a087-cc0567f06c6b",
   "metadata": {},
   "source": [
    "### url\n",
    "\n",
    "With a little code, we can get the URL for the inference API (v2 format in this example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "011f068f-9264-48f3-a82f-99af8dcbd392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import yaml\n",
    "import requests\n",
    "\n",
    "p = subprocess.run(\n",
    "    \"oc get inferenceservice mnist -o yaml\", shell=True, stdout=subprocess.PIPE\n",
    ")\n",
    "\n",
    "desc = yaml.safe_load(p.stdout)\n",
    "url = desc[\"status\"][\"address\"][\"url\"]\n",
    "predictor = desc[\"status\"][\"components\"][\"predictor\"][\"address\"][\"url\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685c6b8e-5ec5-4661-9558-9cbe2cf0f947",
   "metadata": {},
   "source": [
    "## Model details\n",
    "\n",
    "Trition offers an extension to tell us about the model, specifically the input names and data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e50b53bd-438f-449b-a684-250c7ae718dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'mnist',\n",
       " 'versions': ['1'],\n",
       " 'platform': 'onnxruntime_onnx',\n",
       " 'inputs': [{'name': 'input.1', 'datatype': 'FP32', 'shape': [1, 1, 28, 28]}],\n",
       " 'outputs': [{'name': '18', 'datatype': 'FP32', 'shape': [1, 10]}]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(f\"{predictor}/v2/models/mnist/versions/1\").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f344c53-de94-460b-92cd-bbdf6db383c1",
   "metadata": {},
   "source": [
    "## Make request\n",
    "\n",
    "Once we know the name, shape and datatype of the input, we can make an inference request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0991d23-3172-4852-b186-fb9f05a7448c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"input.1\",\n",
    "            \"shape\": list(tensor.shape),\n",
    "            \"datatype\": \"FP32\",\n",
    "            \"data\": tensor.flatten().tolist(),\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "res = requests.post(url, headers={}, data=json.dumps(payload))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44460a25-9018-40e1-a7b9-8bb176c4ff7d",
   "metadata": {},
   "source": [
    "## The highest score is the winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6f7118b-ce92-42df-976a-590424a6fa50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(res.json()[\"outputs\"][0][\"data\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
