{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "841ff471-215d-4984-a0f5-b88fd13a5b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4038e527-8023-445d-849d-123cae61e46c",
   "metadata": {},
   "source": [
    "# Distributed training example using PyTorch\n",
    "\n",
    "This notebook trains and evaluates a classifier of handwritten digits (Using the MNIST dataset). The training is distributed across multiple GPUs.\n",
    "\n",
    "There are important features and differences in this flow than other examples in this repo, such as the MPI/Tensorflow example.\n",
    "\n",
    "Author: Nick Lawrence ntl@us.ibm.com\n",
    "License: [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "## Container Image\n",
    "The components of this pipeline use a custom built container image as their base image. The Dockerfile is included in GitHub. The container image does *not* include a notebook server, meaning you cannot use it to launch a notebook. The notebook container images from https://quay.io/repository/ibm/kubeflow-notebook-image-ppc64le have a wide range of packages installed in them, and these can be used to start an interactive notebook server. The base image used in this example has installed a much smaller set of newer packages, including Python 3.10 and PyTorch 1.13 from RocketCE. Also included is pytorch-lightning, which makes it easier to code up models for distributed training.\n",
    "\n",
    "Since your notebook image and pipeline images are not the same you may be able to run code in the pipeline that does not run interactivly in the notebook. And you may be able to run code interactivly that does not run in the pipeline.\n",
    "\n",
    "The custom image also includes the pytorch_distributed_kf_tools, which simplifies the creation and deployment of the PyTorch Job. The source code for this package is in the GitHub Repo.\n",
    "\n",
    "You can see how the custom image was built and that packages that are included by looking at the Dockerfile.\n",
    "\n",
    "## Model Training Script\n",
    "When using distributed training with GPUs, most PyTorch models are trained within a script that is called from the command line, as opposed to a cell in a interactive notebook.\n",
    "\n",
    "The script to run the model is stored in the GitHub Repo.\n",
    "\n",
    "The Kubeflow train_and_test_model component will create a PyTorch job that runs the script across a pre-defined number of worker pods. The component will then monitor the job and wait for the job's completion.\n",
    "\n",
    "This script is downloaded by the pipeline, rather than being built into the container image. This allows the script to be changed without rebuilding the container image.\n",
    "\n",
    "## Shared Storage\n",
    "The Kubeflow component needs to make the Python script and training data available to the PytorchJob, and it needs to be able to obtain the trained model after training is completed.\n",
    "\n",
    "The pipeline will create a Volume as the first step in the pipeline.\n",
    "The pipeline will copy the training data and training script onto this storage so that the workers can access it.\n",
    "After training, the pipeline will copy the model from the shared storage to storage that is accessible outside the pipeline. (The notebook's volume in this example).\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd2949a-1204-45aa-bbe7-aa354eafcbb1",
   "metadata": {},
   "source": [
    "## Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3deb0460-f169-4bed-8d16-cba226a651bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp.components import InputPath, OutputPath\n",
    "import kfp.dsl as dsl\n",
    "from kfp.dsl import PipelineConf, data_passing_methods\n",
    "from kubernetes.client import (\n",
    "    V1Volume,\n",
    "    V1PersistentVolumeClaimVolumeSource,\n",
    "    V1VolumeMount,\n",
    ")\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import List, NamedTuple\n",
    "\n",
    "BASE_IMAGE = \"quay.io/ntlawrence/mnist-dist-pytorch:1.0.6\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea276b5e",
   "metadata": {},
   "source": [
    "# Notebook Volume\n",
    "Set the value in this next cell to your noteboook's volume. Make sure that the volume was created Read Write Many."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23092186",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_PVC_NAME = \"my-notebook-datavol-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb5ba09-6f1b-4f3a-96f3-69c2448fb995",
   "metadata": {},
   "source": [
    "# Prepare shared storage\n",
    "\n",
    "This cell downloads 3 things to the shared storage\n",
    "* Training data\n",
    "* Test data\n",
    "* Model training script\n",
    "\n",
    "The assumption in the function is that the shared volume has been mounted to the path \"/workspace\". This mount is defined by the pipeline definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b552cf95-1480-4b5e-a3d7-88ce6153b8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_shared_storage():\n",
    "    from torchvision.datasets import MNIST\n",
    "    import urllib\n",
    "\n",
    "    # Download training data\n",
    "    _ = MNIST(\"/workspace\", download=True, train=True)\n",
    "    _ = MNIST(\"/workspace\", download=True, train=False)\n",
    "\n",
    "    # Download python model training script\n",
    "    r = urllib.request.urlretrieve(\n",
    "        \"https://raw.githubusercontent.com/ntl-ibm/kubeflow-ppc64le-examples/2.0.3/distributed_training/pytorch/mnist/mnist.py\",\n",
    "        \"/workspace/mnist.py\",\n",
    "    )\n",
    "\n",
    "\n",
    "prepare_shared_storage_comp = kfp.components.create_component_from_func(\n",
    "    prepare_shared_storage, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d8c99d-d8ed-400d-ae8c-4587772c9307",
   "metadata": {},
   "source": [
    "# Train and Test the model\n",
    "For this example, both training and evaluating the model is part of the same script.\n",
    "\n",
    "The training data and training script has been downloaded to shared storage (assumed to be mounted to /workspace).\n",
    "\n",
    "The pipeline component needs to create a PyTorchJob resource and monitor the resource. Only the creation of the resource and monitoring happens in this component, the actual training happens in independent worker pods.\n",
    "\n",
    "Creating and monitoring the resource is complex. The pytorch_distrbuted_kf_tools package (actually from this repo) is built into the base container image and allows us to do that with a single function call.\n",
    "\n",
    "The training script could have been included in the container image, which avoids the need to download in. However downloading has the advantage of being able to pickup versioned changes from Git, without rebuilding the container image.\n",
    "\n",
    "The code is written to retry the creation of the pytorch job up to 5 times if an error occurrs. The model is designed to use checkpoints; this is how we might design the training of a large model so that it can continue without starting over if a problem occurs in the middle.\n",
    "\n",
    "The metrics returned from the training component are shown by Kubeflow in the Kubeflow Experiment so that pipeline runs can be compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87c120ff-ef8a-4b09-b823-ae743bb867c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_model(\n",
    "    shared_pvc_name: str, worker_image: str\n",
    ") -> NamedTuple(\"EvaluationOutput\", [(\"mlpipeline_metrics\", \"Metrics\")]):\n",
    "\n",
    "    import json\n",
    "    import distributed_kf_tools.deploy as deploy\n",
    "    from distributed_kf_tools.template import OwningWorkFlow, PvcMount\n",
    "    from collections import namedtuple\n",
    "\n",
    "    for retries in range(5):\n",
    "        try:\n",
    "            ## Start the PyTorch job for distributed training\n",
    "            job_name = \"{{workflow.name}}\" + (f\"-{retries:03d}\" if retries else \"\")\n",
    "            deploy.run_pytorch_job(\n",
    "                # owning_workflow setups it up so that when the pipeline is deleted,\n",
    "                # the training job is cleaned up\n",
    "                owning_workflow=OwningWorkFlow(\n",
    "                    name=\"{{workflow.name}}\", uid=\"{{workflow.uid}}\"\n",
    "                ),\n",
    "                # These place holders for namespace and job name are\n",
    "                # filled in by Kubeflow when the pipeline runs.\n",
    "                namespace=\"{{workflow.namespace}}\",\n",
    "                pytorch_job_name=job_name,\n",
    "                # Shared volumes used by the training script\n",
    "                pvcs=[\n",
    "                    PvcMount(\n",
    "                        pvc_name=(shared_pvc_name),\n",
    "                        mount_path=\"/workspace\",\n",
    "                    )\n",
    "                ],\n",
    "                # The command to run in each worker\n",
    "                # This almost always starts with \"torch.distributed.run\" for DDP\n",
    "                command=[\n",
    "                    \"python\",\n",
    "                    \"-m\",\n",
    "                    \"torch.distributed.run\",\n",
    "                    \"/workspace/mnist.py\",\n",
    "                    \"--root_dir=/workspace\",\n",
    "                    \"--data_dir=/workspace\",\n",
    "                    \"--model=/workspace/mnist_model.pt\",\n",
    "                    \"--batch_size=672\",\n",
    "                    \"--evaluation_metrics=/workspace/metrics.json\",\n",
    "                    \"--checkpoint\",\n",
    "                    \"--max_epochs=15\",\n",
    "                ],\n",
    "                # Number of workers\n",
    "                num_workers=3,\n",
    "                # Number of GPUs per worker (OK to leave this at 1)\n",
    "                gpus_per_worker=1,\n",
    "                # The base image used for the worker pods\n",
    "                worker_image=worker_image,\n",
    "            )\n",
    "            break\n",
    "        except RuntimeError as e:\n",
    "            print(f\"THE JOB FAILED BECAUSE OF ERROR {e}\")\n",
    "\n",
    "    # Return test metrics from the trial\n",
    "    # Kubeflow can use this information to compare trial runs\n",
    "    with open(\"/workspace/metrics.json\") as f:\n",
    "        jsonstr = f.read()\n",
    "        print(jsonstr)\n",
    "        trial_metrics = json.loads(jsonstr)\n",
    "\n",
    "    metrics = {\n",
    "        \"metrics\": [\n",
    "            {\"name\": \"f1\", \"numberValue\": trial_metrics[\"test_f1\"], \"format\": \"RAW\"},\n",
    "            {\n",
    "                \"name\": \"accuracy\",\n",
    "                \"numberValue\": trial_metrics[\"test_acc\"],\n",
    "                \"format\": \"PERCENTAGE\",\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    out_tuple = namedtuple(\"EvaluationOutput\", [\"mlpipeline_metrics\"])\n",
    "    return out_tuple(json.dumps(metrics))\n",
    "\n",
    "\n",
    "train_and_test_model_comp = kfp.components.create_component_from_func(\n",
    "    train_and_test_model, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2103ed-2ad7-4156-b17c-abe961261592",
   "metadata": {},
   "source": [
    "# Copy data\n",
    "This component can be used to copy a file or directory from one path to another.\n",
    "\n",
    "We use this to copy the model from the shared PVC to the PVC used by the notebook server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3dc156a-34eb-4186-81e5-b42e14d89791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_data(source: str, dest: str):\n",
    "    import os\n",
    "    import shutil\n",
    "\n",
    "    # Make target directories if needed\n",
    "    parent_dirs = os.path.basename(dest)\n",
    "    if not os.path.exists(parent_dirs):\n",
    "        os.makedirs(parent_dirs)\n",
    "\n",
    "    if os.path.isdir(source):\n",
    "        shutil.copytree(source, dest)\n",
    "    else:\n",
    "        shutil.copyfile(source, dest)\n",
    "\n",
    "\n",
    "copy_data_comp = kfp.components.create_component_from_func(\n",
    "    copy_data, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95a249d-a6bf-495d-af62-9d85a9e56d4b",
   "metadata": {},
   "source": [
    "# Define the pipeline\n",
    "\n",
    "\n",
    "The pipeline has four tasks in it:\n",
    "* Create shared volume\n",
    "* Prepare shared volume\n",
    "   - Download training data\n",
    "   - Download training script\n",
    "* Train model\n",
    "   - Metrics are output to a visualization\n",
    "* Copy trained model to the notebook PVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31a8707d-77db-47b6-b029-1a3ba605edf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"Handwritten digit classification\",\n",
    "    description=\"An example pipeline that trains using distributed pytorch\",\n",
    ")\n",
    "def mnist_pipeline(notebook_pvc_name: str):\n",
    "\n",
    "    create_shared_volume_volop = dsl.VolumeOp(\n",
    "        name=\"Create shared volume for training\",\n",
    "        resource_name=\"shared-pvc\",\n",
    "        modes=dsl.VOLUME_MODE_RWM,\n",
    "        size=\"4Gi\",\n",
    "        set_owner_reference=True,\n",
    "    )\n",
    "\n",
    "    prepare_shared_storage_task = prepare_shared_storage_comp()\n",
    "    prepare_shared_storage_task.add_pvolumes(\n",
    "        {\"/workspace\": create_shared_volume_volop.volume}\n",
    "    )\n",
    "\n",
    "    train_model_task = train_and_test_model_comp(\n",
    "        create_shared_volume_volop.volume.persistent_volume_claim.claim_name,\n",
    "        worker_image=BASE_IMAGE,\n",
    "    )\n",
    "    train_model_task.add_pvolumes({\"/workspace\": create_shared_volume_volop.volume})\n",
    "    train_model_task.after(prepare_shared_storage_task)\n",
    "    train_model_task.set_display_name(\"Train and Test Model\")\n",
    "\n",
    "    copy_model_task = copy_data_comp(\n",
    "        \"/workspace/mnist_model.pt\", \"/target/mnist_model.pt\"\n",
    "    )\n",
    "    copy_model_task.add_pvolumes({\"/workspace\": create_shared_volume_volop.volume})\n",
    "    copy_model_task.add_volume(\n",
    "        V1Volume(\n",
    "            name=notebook_pvc_name,\n",
    "            persistent_volume_claim=V1PersistentVolumeClaimVolumeSource(\n",
    "                notebook_pvc_name\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    copy_model_task.add_volume_mount(\n",
    "        V1VolumeMount(name=notebook_pvc_name, mount_path=\"/target\")\n",
    "    )\n",
    "    copy_model_task.set_display_name(f\"Copy Model to target PVC\")\n",
    "    copy_model_task.after(train_model_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb9970a3-f083-49b1-a7ab-5d999215adf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_NAME = \"MNIST HW Classification Pipeline\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b121e2-6498-45a1-8391-d1936c532f1b",
   "metadata": {},
   "source": [
    "# Configure Pipeline\n",
    "\n",
    "The first transformer disables caching for our pipeline. Kubeflow caches tasks based on input and output parameters. Because we are using shared storage, the input parameters are the same as previous runs, however the data on the shared storage is most likely different.\n",
    "\n",
    "The second transformer adds a node constraint to all tasks. This is only needed in the IBM Lab. We currently have a few machines that are older Power 8 hardware in the cluster. Python 3.10 from RocketCE has been optimized for Power 9 and Power 10. We'll force all out pods to run on the newer AC922's with newer hardware. (This isn't an issue for environments that have all AC922s or newer.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a36cd67f-8f7d-4186-85f8-a09935fcd297",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_conf = kfp.dsl.PipelineConf()\n",
    "\n",
    "# Disable Caching\n",
    "def disable_cache_transformer(op: dsl.ContainerOp):\n",
    "    if isinstance(op, dsl.ContainerOp):\n",
    "        op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    else:\n",
    "        op.add_pod_annotation(\n",
    "            name=\"pipelines.kubeflow.org/max_cache_staleness\", value=\"P0D\"\n",
    "        )\n",
    "    return op\n",
    "\n",
    "\n",
    "pipeline_conf.add_op_transformer(disable_cache_transformer)\n",
    "\n",
    "# This transformer is only Relevant inside an IBM lab that has both P8 and P9 machines\n",
    "# (Assumptioin is that the P9 machines have the ai.accelerator label on them)\n",
    "def run_on_power_9_transformer(op: dsl.ContainerOp):\n",
    "    if isinstance(op, dsl.ContainerOp):\n",
    "        op.add_node_selector_constraint(\"ai.accelerator\", \"V100\")\n",
    "\n",
    "\n",
    "pipeline_conf.add_op_transformer(run_on_power_9_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc1de34-2104-4cfe-a923-aa3177d549fa",
   "metadata": {},
   "source": [
    "# Compile, upload and run pipeline\n",
    "\n",
    "This creates a run of the pipeline within an experiment.\n",
    "\n",
    "The parameter to the pipeline is the name of the PVC to copy the model onto. *This PVC must have an access mode of ReadWriteMany!*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d6ca018-cbbc-4a36-9135-6b2ecc9d48d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pipeline(pipeline_name: str):\n",
    "    \"\"\"Delete's a pipeline with the specified name\"\"\"\n",
    "\n",
    "    client = kfp.Client()\n",
    "    existing_pipelines = client.list_pipelines(page_size=999).pipelines\n",
    "    matches = (\n",
    "        [ep.id for ep in existing_pipelines if ep.name == pipeline_name]\n",
    "        if existing_pipelines\n",
    "        else []\n",
    "    )\n",
    "    for id in matches:\n",
    "        client.delete_pipeline(id)\n",
    "\n",
    "\n",
    "def get_experiment_id(experiment_name: str) -> str:\n",
    "    \"\"\"Returns the id for the experiment, creating the experiment if needed\"\"\"\n",
    "    client = kfp.Client()\n",
    "    existing_experiments = client.list_experiments(page_size=999).experiments\n",
    "    matches = (\n",
    "        [ex.id for ex in existing_experiments if ex.name == experiment_name]\n",
    "        if existing_experiments\n",
    "        else []\n",
    "    )\n",
    "\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "\n",
    "    exp = client.create_experiment(experiment_name)\n",
    "    return exp.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c00906ed-c38f-4159-bac8-ee8dd1b50689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=/pipeline/#/pipelines/details/2bb4814f-4641-477b-99bf-4c910f579e6b>Pipeline details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/3d9a6216-820d-4535-b15a-31d91b9db09e\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "client = kfp.Client()\n",
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=mnist_pipeline,\n",
    "    package_path=f\"{PIPELINE_NAME}.yaml\",\n",
    "    pipeline_conf=pipeline_conf,\n",
    ")\n",
    "\n",
    "delete_pipeline(PIPELINE_NAME)\n",
    "uploaded_pipeline = client.upload_pipeline(f\"{PIPELINE_NAME}.yaml\", PIPELINE_NAME)\n",
    "run = client.run_pipeline(\n",
    "    experiment_id=get_experiment_id(\"mnist-pytorch-exp\"),\n",
    "    job_name=\"mnist-pytorch\",\n",
    "    pipeline_id=uploaded_pipeline.id,\n",
    "    params={\"notebook_pvc_name\": NOTEBOOK_PVC_NAME},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9bd57c2-35d1-4f73-b5c1-7ff08fb3ad21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'Error',\n",
       " 'error': None,\n",
       " 'time': '0:05:06',\n",
       " 'metrics': [{'format': 'PERCENTAGE',\n",
       "   'name': 'accuracy',\n",
       "   'node_id': 'handwritten-digit-classification-4js95-1989843049',\n",
       "   'number_value': 0.965506911277771},\n",
       "  {'format': 'RAW',\n",
       "   'name': 'f1',\n",
       "   'node_id': 'handwritten-digit-classification-4js95-1989843049',\n",
       "   'number_value': 0.96537184715271}]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TWENTY_MIN = 20 * 60\n",
    "result = client.wait_for_run_completion(run.id, timeout=TWENTY_MIN)\n",
    "{\n",
    "    \"status\": result.run.status,\n",
    "    \"error\": result.run.error,\n",
    "    \"time\": str(result.run.finished_at - result.run.created_at),\n",
    "    \"metrics\": result.run.metrics,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc42bfe-2cab-44c6-94e2-3b95b2abf02c",
   "metadata": {},
   "source": [
    "# Check output\n",
    "\n",
    "After the pipeine completes, you should see the model in the root directory of the pvc defined by the NOTEBOOK_PVC_NAME variable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
