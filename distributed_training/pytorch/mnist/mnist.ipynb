{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "841ff471-215d-4984-a0f5-b88fd13a5b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4038e527-8023-445d-849d-123cae61e46c",
   "metadata": {},
   "source": [
    "# Distributed training example using PyTorch\n",
    "\n",
    "This notebook trains, evaluates and deploys a classifier for handwritten digits (Using the MNIST dataset). The training is distributed across multiple GPUs using DDP.\n",
    "\n",
    "\n",
    "Author: Nick Lawrence ntl@us.ibm.com\n",
    "License: [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "## Base container Image\n",
    "The components of this pipeline use a a container image that has been built for this example. The Dockerfile is included in the [container_image](../../pytorch_model_frameworks/mnist/container_image) directory. \n",
    "\n",
    "The container image does *not* include a notebook server, meaning you cannot use the base image to launch a notebook. A new container image was created so that PyTorch 2.0 and Python 3.10 could be used in the example. These packages were not available when the notebook server's image was built. The image also includes pytorch-lightning, which makes designing a model simpler.\n",
    "\n",
    "\n",
    "The disadvantage of using different images for pipeline steps is that it is more difficult to test code from within the notebook server itself, since not all packages are available.\n",
    "\n",
    "The custom image also has installed the pytorch_distributed_kf_tools, which simplifies the creation and deployment of the PyTorch Job. The source code for this package is [here](../../distributed_kf_tools)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4fdace-d301-424f-8fed-fd9b8f33553c",
   "metadata": {},
   "source": [
    "# Workspace\n",
    "\n",
    "The pipeline allocates a shared PVC (the workspace) that is used to:\n",
    "* Download python code for the model and training from github\n",
    "* Load training and test data for the MNIST dataset\n",
    "* Store checkpoints created during the training process\n",
    "* Store tensorboard logs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd2949a-1204-45aa-bbe7-aa354eafcbb1",
   "metadata": {},
   "source": [
    "## Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3deb0460-f169-4bed-8d16-cba226a651bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp.components import InputPath, OutputPath\n",
    "import kfp.dsl as dsl\n",
    "from kfp.dsl import PipelineConf, data_passing_methods\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import NamedTuple\n",
    "\n",
    "BASE_IMAGE = \"quay.io/ntlawrence/pytorch@sha256:75b0d6804e480a864ce2cc0e14be9b0bd993cc341dee346d29031defdba2f06d\"\n",
    "COMPONENT_CATALOG_FOLDER = f\"{os.getenv('HOME')}/components\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa02571-71b6-458f-9339-69ca4ba49b5e",
   "metadata": {},
   "source": [
    "# Prepare workspace component\n",
    "\n",
    "This component performs several functions:\n",
    "* download the training and test data to the workspace\n",
    "* clone the python code for model and training from git\n",
    "* create the directory for the tensorboard logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b552cf95-1480-4b5e-a3d7-88ce6153b8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_workspace():\n",
    "    from torchvision.datasets import MNIST\n",
    "    import subprocess\n",
    "    import os\n",
    "\n",
    "    # Download data\n",
    "    _ = MNIST(\"/workspace/data\", download=True, train=True)\n",
    "\n",
    "    # Clone git repo\n",
    "    subprocess.run(\n",
    "        \"git clone https://github.com/ntl-ibm/kubeflow-ppc64le-examples.git  /workspace/kubeflow-ppc64le-examples -b pytorch_example_improvements\",\n",
    "        shell=True,\n",
    "    )\n",
    "\n",
    "    os.mkdir(\"/workspace/tensorboard\")\n",
    "\n",
    "\n",
    "prepare_workspace_comp = kfp.components.create_component_from_func(\n",
    "    prepare_workspace, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "243b27f6-70e4-4e39-aa35-938aa65461ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_tensorboard(\n",
    "    mlpipeline_ui_metadata_path: OutputPath(),\n",
    "    pvc_name: str,\n",
    "    pvc_path: str = \"\",\n",
    "    tensorboard_name: str = \"\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Monitors a training job based on Tensorboard logs.\n",
    "    Logs are expected to be written to the specified subpath of the pvc\n",
    "    \"\"\"\n",
    "    from collections import namedtuple\n",
    "    import json\n",
    "    from kubernetes import client, config, watch\n",
    "    import logging\n",
    "    import sys\n",
    "    import os\n",
    "    import yaml\n",
    "    import textwrap\n",
    "    import json\n",
    "    import http\n",
    "\n",
    "    logging.basicConfig(\n",
    "        stream=sys.stdout,\n",
    "        level=logging.INFO,\n",
    "        format=\"%(levelname)s %(asctime)s: %(message)s\",\n",
    "    )\n",
    "    logger = logging.getLogger()\n",
    "\n",
    "    if not tensorboard_name:\n",
    "        tensorboard_name = \"{{workflow.name}}\"\n",
    "\n",
    "    namespace = \"{{workflow.namespace}}\"\n",
    "\n",
    "    config.load_incluster_config()\n",
    "    api_client = client.ApiClient()\n",
    "    apps_api = client.AppsV1Api(api_client)\n",
    "    custom_object_api = client.CustomObjectsApi(api_client)\n",
    "\n",
    "    # Delete possible existing tensorboard\n",
    "    try:\n",
    "        custom_object_api.delete_namespaced_custom_object(\n",
    "            group=\"tensorboard.kubeflow.org\",\n",
    "            version=\"v1alpha1\",\n",
    "            plural=\"tensorboards\",\n",
    "            namespace=namespace,\n",
    "            name=tensorboard_name,\n",
    "            body=client.V1DeleteOptions(),\n",
    "        )\n",
    "    except client.exceptions.ApiException as e:\n",
    "        if e.status != http.HTTPStatus.NOT_FOUND:\n",
    "            raise\n",
    "\n",
    "    tensorboard_spec = textwrap.dedent(\n",
    "        f\"\"\"\\\n",
    "            apiVersion: tensorboard.kubeflow.org/v1alpha1\n",
    "            kind: Tensorboard\n",
    "            metadata:\n",
    "              name: \"{tensorboard_name}\"\n",
    "              namespace: \"{namespace}\"\n",
    "              ownerReferences:\n",
    "                - apiVersion: v1\n",
    "                  kind: Workflow\n",
    "                  name: \"{{workflow.name}}\"\n",
    "                  uid: \"{{workflow.uid}}\"\n",
    "            spec:\n",
    "              logspath: \"pvc://{pvc_name}/{pvc_path}\"\n",
    "            \"\"\"\n",
    "    )\n",
    "\n",
    "    logger.info(tensorboard_spec)\n",
    "\n",
    "    custom_object_api.create_namespaced_custom_object(\n",
    "        group=\"tensorboard.kubeflow.org\",\n",
    "        version=\"v1alpha1\",\n",
    "        plural=\"tensorboards\",\n",
    "        namespace=namespace,\n",
    "        body=yaml.safe_load(tensorboard_spec),\n",
    "        pretty=True,\n",
    "    )\n",
    "\n",
    "    tensorboard_watch = watch.Watch()\n",
    "    try:\n",
    "        for tensorboard_event in tensorboard_watch.stream(\n",
    "            custom_object_api.list_namespaced_custom_object,\n",
    "            group=\"tensorboard.kubeflow.org\",\n",
    "            version=\"v1alpha1\",\n",
    "            plural=\"tensorboards\",\n",
    "            namespace=namespace,\n",
    "            field_selector=f\"metadata.name={tensorboard_name}\",\n",
    "            timeout_seconds=0,\n",
    "        ):\n",
    "\n",
    "            logger.info(f\"tensorboard_event: {json.dumps(tensorboard_event, indent=2)}\")\n",
    "\n",
    "            if tensorboard_event[\"type\"] == \"DELETED\":\n",
    "                raise RuntimeError(\"The tensorboard was deleted!\")\n",
    "\n",
    "            tensorboard = tensorboard_event[\"object\"]\n",
    "\n",
    "            if \"status\" not in tensorboard:\n",
    "                continue\n",
    "\n",
    "            deployment_state = \"Progressing\"\n",
    "            if \"conditions\" in tensorboard[\"status\"]:\n",
    "                deployment_state = tensorboard[\"status\"][\"conditions\"][-1][\n",
    "                    \"deploymentState\"\n",
    "                ]\n",
    "\n",
    "            if deployment_state == \"Progressing\":\n",
    "                logger.info(\"Tensorboard deployment is progressing...\")\n",
    "            elif deployment_state == \"Available\":\n",
    "                logger.info(\"Tensorboard deployment is Available.\")\n",
    "                break\n",
    "            elif deployment_state == \"ReplicaFailure\":\n",
    "                raise RuntimeError(\n",
    "                    \"Tensorboard deployment failed with a ReplicaFailure!\"\n",
    "                )\n",
    "            else:\n",
    "                raise RuntimeError(f\"Unknown deployment state: {deployment_state}\")\n",
    "    finally:\n",
    "        tensorboard_watch.stop()\n",
    "\n",
    "    button_style = (\n",
    "        \"align-items: center; \"\n",
    "        \"appearance: none; \"\n",
    "        \"background-color: rgb(26, 115, 232); \"\n",
    "        \"border: 0px none rgb(255, 255, 255); \"\n",
    "        \"border-radius: 3px; \"\n",
    "        \"box-sizing: border-box; \"\n",
    "        \"color: rgb(255, 255, 255); \"\n",
    "        \"cursor: pointer; \"\n",
    "        \"display: inline-flex; \"\n",
    "        \"font-family: 'Google Sans', 'Helvetica Neue', sans-serif; \"\n",
    "        \"font-size: 14px; \"\n",
    "        \"font-stretch: 100%; \"\n",
    "        \"font-style: normal; font-weight: 700; \"\n",
    "        \"justify-content: center; \"\n",
    "        \"letter-spacing: normal; \"\n",
    "        \"line-height: 24.5px; \"\n",
    "        \"margin: 0px 10px 2px 0px; \"\n",
    "        \"min-height: 25px; \"\n",
    "        \"min-width: 64px; \"\n",
    "        \"padding: 2px 6px 2px 6px; \"\n",
    "        \"position: relative; \"\n",
    "        \"tab-size: 4; \"\n",
    "        \"text-align: center; \"\n",
    "        \"text-indent: 0px; \"\n",
    "        \"text-rendering: auto; \"\n",
    "        \"text-shadow: none; \"\n",
    "        \"text-size-adjust: 100%; \"\n",
    "        \"text-transform: none; \"\n",
    "        \"user-select: none; \"\n",
    "        \"vertical-align: middle; \"\n",
    "        \"word-spacing: 0px; \"\n",
    "        \"writing-mode: horizontal-tb;\"\n",
    "    )\n",
    "\n",
    "    # See: https://github.com/kubeflow/kubeflow/blob/master/components/crud-web-apps/tensorboards/frontend/src/app/pages/index/index.component.ts\n",
    "    # window.open(`/tensorboard/${tensorboard.namespace}/${tensorboard.name}/`);\n",
    "    ui_address = f\"/tensorboard/{namespace}/{tensorboard_name}/#scalars\"\n",
    "\n",
    "    markdown = textwrap.dedent(\n",
    "        f\"\"\"\\\n",
    "        # Tensorboard\n",
    "        - <a href=\"{ui_address}\" style=\"{button_style}\" target=\"_blank\">Connect</a>\n",
    "        - <a href=\"/_/tensorboards/\" style=\"{button_style}\" target=\"_blank\">Manage all</a>\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    markdown_output = {\n",
    "        \"type\": \"markdown\",\n",
    "        \"storage\": \"inline\",\n",
    "        \"source\": markdown,\n",
    "    }\n",
    "\n",
    "    ui_metadata = {\"outputs\": [markdown_output]}\n",
    "    with open(mlpipeline_ui_metadata_path, \"w\") as metadata_file:\n",
    "        json.dump(ui_metadata, metadata_file)\n",
    "\n",
    "    logging.info(\"Finished.\")\n",
    "\n",
    "\n",
    "configure_tensorboard_comp = kfp.components.create_component_from_func(\n",
    "    func=configure_tensorboard,\n",
    "    base_image=BASE_IMAGE,\n",
    "    packages_to_install=[\"kubernetes\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87c120ff-ef8a-4b09-b823-ae743bb867c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(shared_pvc_name: str, worker_image: str):\n",
    "\n",
    "    import distributed_kf_tools.deploy as deploy\n",
    "    from distributed_kf_tools.template import OwningWorkFlow, PvcMount\n",
    "    import subprocess\n",
    "\n",
    "    for retries in range(5):\n",
    "        try:\n",
    "            ## Start the PyTorch job for distributed training\n",
    "            job_name = \"{{workflow.name}}\" + (f\"-{retries:03d}\" if retries else \"\")\n",
    "            deploy.run_pytorch_job(\n",
    "                # owning_workflow setups it up so that when the pipeline is deleted,\n",
    "                # the training job is cleaned up\n",
    "                owning_workflow=OwningWorkFlow(\n",
    "                    name=\"{{workflow.name}}\", uid=\"{{workflow.uid}}\"\n",
    "                ),\n",
    "                # These place holders for namespace and job name are\n",
    "                # filled in by Kubeflow when the pipeline runs.\n",
    "                namespace=\"{{workflow.namespace}}\",\n",
    "                pytorch_job_name=job_name,\n",
    "                # Shared volumes used by the training script\n",
    "                pvcs=[\n",
    "                    PvcMount(\n",
    "                        pvc_name=(shared_pvc_name),\n",
    "                        mount_path=\"/workspace\",\n",
    "                    )\n",
    "                ],\n",
    "                working_dir=\"/workspace/kubeflow-ppc64le-examples/distributed_training/pytorch_model_frameworks/mnist/src/\",\n",
    "                # The command to run in each worker\n",
    "                # This starts with \"torch.distributed.run\" for DDP\n",
    "                # The output model path needs to be stored on the pvc,\n",
    "                # The output path for this POD points the tmp directory of THIS pod,\n",
    "                # Storing it in the temp directory of the PyTorch worker pod doesn't work,\n",
    "                # because this pod doesn't have access to that.\n",
    "                command=[\n",
    "                    \"python\",\n",
    "                    \"-m\",\n",
    "                    \"torch.distributed.run\",\n",
    "                    \"train.py\",\n",
    "                    \"--root_dir=./work\",\n",
    "                    \"--data_dir=/workspace/data\",\n",
    "                    \"--model_ckpt=/workspace/models/best.ckpt\",\n",
    "                    \"--batch_size=672\",\n",
    "                    \"--checkpoint\",\n",
    "                    \"--early_stopping\",\n",
    "                    \"--max_epochs=50\",\n",
    "                    \"--pytorchjob\",\n",
    "                    \"--tensorboard=/workspace/tensorboard\",\n",
    "                ],\n",
    "                # Number of workers (pods)\n",
    "                num_workers=3,\n",
    "                # Number of GPUs per worker (OK to leave this at 1)\n",
    "                gpus_per_worker=1,\n",
    "                # The base image used for the worker pods\n",
    "                worker_image=worker_image,\n",
    "            )\n",
    "\n",
    "            break\n",
    "        except RuntimeError as e:\n",
    "            print(f\"THE JOB FAILED BECAUSE OF ERROR {e}\")\n",
    "\n",
    "\n",
    "train_model_comp = kfp.components.create_component_from_func(\n",
    "    train_model, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "576c315b-0d8b-4077-88f6-e915326a47c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model() -> NamedTuple(\"EvaluationOutput\", [(\"mlpipeline_metrics\", \"Metrics\")]):\n",
    "    import json\n",
    "    from collections import namedtuple\n",
    "    import subprocess\n",
    "\n",
    "    subprocess.run(\n",
    "        [\n",
    "            \"python\",\n",
    "            \"test.py\",\n",
    "            \"--root_dir=/tmp\",\n",
    "            \"--data_dir=/workspace/data\",\n",
    "            \"--model_ckpt=/workspace/models/best.ckpt\",\n",
    "            \"--batch_size=672\",\n",
    "            \"--metrics_json=/tmp/metrics.json\",\n",
    "        ],\n",
    "        cwd=\"/workspace/kubeflow-ppc64le-examples/distributed_training/pytorch_model_frameworks/mnist/src/\",\n",
    "        check=True,\n",
    "    )\n",
    "\n",
    "    with open(\"/tmp/metrics.json\", \"r\") as f:\n",
    "        metrics = json.load(f)\n",
    "\n",
    "    metrics = {\n",
    "        \"metrics\": [\n",
    "            {\"name\": \"acc\", \"numberValue\": metrics[\"test/acc\"], \"format\": \"RAW\"},\n",
    "            {\"name\": \"F1\", \"numberValue\": metrics[\"test/F1\"], \"format\": \"RAW\"},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    out_tuple = namedtuple(\"EvaluationOutput\", [\"mlpipeline_metrics\"])\n",
    "    return out_tuple(json.dumps(metrics))\n",
    "\n",
    "\n",
    "test_model_comp = kfp.components.create_component_from_func(\n",
    "    test_model, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f9f51c7-a9bb-44f0-9347-763b7971db7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_onnx(onnx_model: OutputPath(str)):\n",
    "    import shutil\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    import subprocess\n",
    "\n",
    "    subprocess.run(\n",
    "        [\n",
    "            \"python\",\n",
    "            \"convert.py\",\n",
    "            \"--root_dir=/tmp\",\n",
    "            \"--data_dir=/workspace/data\",\n",
    "            \"--model_ckpt=/workspace/models/best.ckpt\",\n",
    "            \"--onnx=/tmp/model.onnx\",\n",
    "        ],\n",
    "        cwd=\"/workspace/kubeflow-ppc64le-examples/distributed_training/pytorch_model_frameworks/mnist/src/\",\n",
    "        check=True,\n",
    "    )\n",
    "\n",
    "    directory = str(Path(onnx_model).parent.absolute())\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    shutil.copy(\"/tmp/model.onnx\", onnx_model)\n",
    "\n",
    "\n",
    "convert_to_onnx_comp = kfp.components.create_component_from_func(\n",
    "    convert_to_onnx, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da15a36f-cf9a-4887-8c53-60a7a53f619f",
   "metadata": {},
   "outputs": [],
   "source": [
    "UPLOAD_MODEL_COMPONENT = (\n",
    "    f\"{COMPONENT_CATALOG_FOLDER}/model-building/upload-model/component.yaml\"\n",
    ")\n",
    "\n",
    "upload_model_comp = kfp.components.load_component_from_file(UPLOAD_MODEL_COMPONENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "203646a8-3015-4728-9d2f-101b58b29a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPLOY_INFERENCE_SERVICE_COMPONENT = f\"{os.getenv('HOME')}/kubeflow-ppc64le-examples/deploy_triton_inference_service_component/deploy_triton_inference_service_component.yaml\"\n",
    "\n",
    "deploy_inference_service_comp = kfp.components.load_component_from_file(\n",
    "    DEPLOY_INFERENCE_SERVICE_COMPONENT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31a8707d-77db-47b6-b029-1a3ba605edf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"Handwritten digit classification\",\n",
    "    description=\"An example pipeline that trains using distributed pytorch\",\n",
    ")\n",
    "def mnist_pipeline():\n",
    "\n",
    "    workspace_volume_volop = dsl.VolumeOp(\n",
    "        name=\"Create workspace for training\",\n",
    "        resource_name=\"shared-workspace-pvc\",\n",
    "        modes=dsl.VOLUME_MODE_RWM,\n",
    "        size=\"4Gi\",\n",
    "        set_owner_reference=True,\n",
    "    )\n",
    "\n",
    "    prepare_workspace_task = prepare_workspace_comp()\n",
    "    prepare_workspace_task.add_pvolumes({\"/workspace\": workspace_volume_volop.volume})\n",
    "\n",
    "    configure_tensorboard_task = configure_tensorboard_comp(\n",
    "        pvc_name=workspace_volume_volop.volume.persistent_volume_claim.claim_name,\n",
    "        pvc_path=\"tensorboard\",\n",
    "        tensorboard_name=\"tb-{{workflow.name}}\",\n",
    "    )\n",
    "    configure_tensorboard_task.after(prepare_workspace_task)\n",
    "\n",
    "    train_model_task = train_model_comp(\n",
    "        workspace_volume_volop.volume.persistent_volume_claim.claim_name,\n",
    "        worker_image=BASE_IMAGE,\n",
    "    )\n",
    "    train_model_task.after(prepare_workspace_task)\n",
    "    train_model_task.after(configure_tensorboard_task)\n",
    "    train_model_task.add_pvolumes({\"/workspace\": workspace_volume_volop.volume})\n",
    "\n",
    "    test_model_task = test_model_comp()\n",
    "    test_model_task.after(train_model_task)\n",
    "    test_model_task.add_pvolumes({\"/workspace\": workspace_volume_volop.volume})\n",
    "\n",
    "    convert_to_onnx_task = convert_to_onnx_comp()\n",
    "    convert_to_onnx_task.after(test_model_task)\n",
    "    convert_to_onnx_task.add_pvolumes({\"/workspace\": workspace_volume_volop.volume})\n",
    "\n",
    "    model_name = \"mnist\"\n",
    "    minio_url = \"minio-service.kubeflow:9000\"\n",
    "\n",
    "    upload_model_task = upload_model_comp(\n",
    "        convert_to_onnx_task.outputs[\"onnx_model\"],\n",
    "        minio_url=minio_url,\n",
    "        export_bucket=\"{{workflow.namespace}}-models\",\n",
    "        model_format=\"onnx\",\n",
    "        model_name=model_name,\n",
    "        model_version=1,\n",
    "    )\n",
    "\n",
    "    deploy_model_task = deploy_inference_service_comp(\n",
    "        name=model_name,\n",
    "        rm_existing=True,\n",
    "        storage_uri=\"s3://{{workflow.namespace}}-models/onnx/\",\n",
    "        minio_url=minio_url,\n",
    "        concurrency_target=4,  # soft limit, may be exceeded for short periods of time\n",
    "        predictor_min_replicas=0,  # min_replicas supports scale to 0\n",
    "        predictor_max_replicas=4,  # We don't want to scale till we consume all the available GPUs\n",
    "        predictor_protocol=\"v2\",\n",
    "        # Uncomment these next two lines to use the GPU runtime and allocate GPUs\n",
    "        # triton_runtime_version=\"21.08-py3-gpu\",\n",
    "        # predictor_gpu_allocation=1,  # gpu per replica\n",
    "    )\n",
    "    deploy_model_task.after(upload_model_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb9970a3-f083-49b1-a7ab-5d999215adf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_NAME = \"MNIST HW Classification Pipeline\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b121e2-6498-45a1-8391-d1936c532f1b",
   "metadata": {},
   "source": [
    "# Configure Pipeline\n",
    "\n",
    "The first transformer disables caching for our pipeline. Kubeflow caches tasks based on input and output parameters. Because we are using shared storage, the input parameters are the same as previous runs, however the data on the shared storage is most likely different.\n",
    "\n",
    "The second transformer adds a node constraint to all tasks. This is only needed in the IBM Lab. We currently have a few machines that are older Power 8 hardware in the cluster. Python 3.10 from RocketCE has been optimized for Power 9 and Power 10. We'll force all out pods to run on the newer AC922's with newer hardware. (This isn't an issue for environments that have all AC922s or newer.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a36cd67f-8f7d-4186-85f8-a09935fcd297",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_conf = kfp.dsl.PipelineConf()\n",
    "\n",
    "# Disable Caching\n",
    "def disable_cache_transformer(op: dsl.ContainerOp):\n",
    "    if isinstance(op, dsl.ContainerOp):\n",
    "        op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    else:\n",
    "        op.add_pod_annotation(\n",
    "            name=\"pipelines.kubeflow.org/max_cache_staleness\", value=\"P0D\"\n",
    "        )\n",
    "    return op\n",
    "\n",
    "\n",
    "pipeline_conf.add_op_transformer(disable_cache_transformer)\n",
    "\n",
    "# This transformer is only Relevant inside an IBM lab that has both P8 and P9 machines\n",
    "# (Assumptioin is that the P9 machines have the ai.accelerator label on them)\n",
    "def run_on_power_9_transformer(op: dsl.ContainerOp):\n",
    "    if isinstance(op, dsl.ContainerOp):\n",
    "        op.add_node_selector_constraint(\"ai.accelerator\", \"V100\")\n",
    "\n",
    "\n",
    "pipeline_conf.add_op_transformer(run_on_power_9_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc1de34-2104-4cfe-a923-aa3177d549fa",
   "metadata": {},
   "source": [
    "# Compile, upload and run pipeline\n",
    "\n",
    "This creates a run of the pipeline within an experiment.\n",
    "\n",
    "The parameter to the pipeline is the name of the PVC to copy the model onto. *This PVC must have an access mode of ReadWriteMany!*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d6ca018-cbbc-4a36-9135-6b2ecc9d48d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pipeline(pipeline_name: str):\n",
    "    \"\"\"Delete's a pipeline with the specified name\"\"\"\n",
    "\n",
    "    client = kfp.Client()\n",
    "    existing_pipelines = client.list_pipelines(page_size=999).pipelines\n",
    "    matches = (\n",
    "        [ep.id for ep in existing_pipelines if ep.name == pipeline_name]\n",
    "        if existing_pipelines\n",
    "        else []\n",
    "    )\n",
    "    for id in matches:\n",
    "        client.delete_pipeline(id)\n",
    "\n",
    "\n",
    "def get_experiment_id(experiment_name: str) -> str:\n",
    "    \"\"\"Returns the id for the experiment, creating the experiment if needed\"\"\"\n",
    "    client = kfp.Client()\n",
    "    existing_experiments = client.list_experiments(page_size=999).experiments\n",
    "    matches = (\n",
    "        [ex.id for ex in existing_experiments if ex.name == experiment_name]\n",
    "        if existing_experiments\n",
    "        else []\n",
    "    )\n",
    "\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "\n",
    "    exp = client.create_experiment(experiment_name)\n",
    "    return exp.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c00906ed-c38f-4159-bac8-ee8dd1b50689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=/pipeline/#/pipelines/details/5264800e-355c-49b9-b6ea-407ea99b6a61>Pipeline details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/a45932f6-10f1-4f9f-aad9-6f0dcb9ea1ab\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PIPELINE_NAME = \"train-mnist-pytorch-model\"\n",
    "\n",
    "client = kfp.Client()\n",
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=mnist_pipeline,\n",
    "    package_path=f\"{PIPELINE_NAME}.yaml\",\n",
    "    pipeline_conf=pipeline_conf,\n",
    ")\n",
    "\n",
    "delete_pipeline(PIPELINE_NAME)\n",
    "uploaded_pipeline = client.upload_pipeline(f\"{PIPELINE_NAME}.yaml\", PIPELINE_NAME)\n",
    "run = client.run_pipeline(\n",
    "    experiment_id=get_experiment_id(\"mnist-pytorch-exp\"),\n",
    "    job_name=\"mnist-pytorch\",\n",
    "    pipeline_id=uploaded_pipeline.id,\n",
    "    params={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9bd57c2-35d1-4f73-b5c1-7ff08fb3ad21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'Succeeded',\n",
       " 'error': None,\n",
       " 'time': '0:08:09',\n",
       " 'metrics': [{'format': 'RAW',\n",
       "   'name': 'acc',\n",
       "   'node_id': 'handwritten-digit-classification-9kgcn-4184176891',\n",
       "   'number_value': 0.9537000060081482},\n",
       "  {'format': 'RAW',\n",
       "   'name': 'F1',\n",
       "   'node_id': 'handwritten-digit-classification-9kgcn-4184176891',\n",
       "   'number_value': 0.9531377553939819}]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TWENTY_MIN = 20 * 60\n",
    "result = client.wait_for_run_completion(run.id, timeout=TWENTY_MIN)\n",
    "{\n",
    "    \"status\": result.run.status,\n",
    "    \"error\": result.run.error,\n",
    "    \"time\": str(result.run.finished_at - result.run.created_at),\n",
    "    \"metrics\": result.run.metrics,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a9a86b-848d-491c-a4d2-2751e6065a8b",
   "metadata": {},
   "source": [
    "# Make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6484eff0-bb2d-4c9b-84a3-43efb55c7b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e370e91c-2c01-4634-8abf-383554287d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABq0lEQVR4nGWSPWtUURCGnzkfe/feu3uVdbcQFgMRQUmlYJfaFCK2YlBrsUkhFmlFtBER0kjSpLMRBMXSH2Anoo1CkCgoyeIu2Q/3wzsWN5tkT6aaw8PMeeedgSAEjDvM9xOjObg4H05EnRmFNc4WSYVIAmYs4KtJDEA2wxzgHZAYauVQCTB394MOBxtN8H4GZMjVd7m2/upgo+hzNOYf/dL22tLc6g9tHuv5oPXvRYO0fEvHEXEA68/OVtOTnNf+7brBzKqtYuHGyx39lGBsOGhNuDdQ/XoxMzZAGaxttycD7d7EUwpoQidXfd1TXYbaLHOU03NV7OL73d610KEIAA9ZZ+/59JmACAKcgBRDY3W4vW+R6RuvUlYSOtgeJdNql35SmRT9xFghWVmOja9hMdLrPpx6KwYnLOxupuABtzLQKwcXkoKpXN57Q2SrUF/8M15P3KFMEXOm9RbALTz+rh8Bb6EoT/r4b42nW93o0tKF8as7o1OtowOmbI50mA9150mTDFxxCAKimKh0f/76l8+/17fyGWtEAFeG0xAHe5yGR8T4Yydw8HUUkv/uS3eJ50exBAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with Image.open(\"3.jpg\") as i:\n",
    "    i.show()\n",
    "    image_array = np.asarray(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ee64f75e-ff7e-4a0f-be0a-16efa8f1c0dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8c70b231-ebc1-4dc3-a6b4-7ec36c2cf127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# channels dimension is required, even though the shape is always (1,28,28)\n",
    "tensor = np.expand_dims(image_array, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4615c807-ccd8-4b85-acdd-58b9625555e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 28, 28)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch dimension required\n",
    "tensor = np.expand_dims(tensor, 0)\n",
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4ee5a553-a37f-419b-9a34-5e7a1819d14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME    URL                                                          READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION             AGE\n",
      "mnist   http://mnist-ntl-us-ibm-com.apps.openshift.ibmscoutsandbox   True           100                              mnist-predictor-default-00001   71m\n"
     ]
    }
   ],
   "source": [
    "!oc get inferenceservice mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "011f068f-9264-48f3-a82f-99af8dcbd392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import yaml\n",
    "\n",
    "p = subprocess.run(\n",
    "    \"oc get inferenceservice mnist -o yaml\", shell=True, stdout=subprocess.PIPE\n",
    ")\n",
    "\n",
    "desc = yaml.safe_load(p.stdout)\n",
    "url = desc[\"status\"][\"address\"][\"url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e50b53bd-438f-449b-a684-250c7ae718dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'mnist',\n",
       " 'versions': ['1'],\n",
       " 'platform': 'onnxruntime_onnx',\n",
       " 'inputs': [{'name': 'input.1', 'datatype': 'FP32', 'shape': [1, 1, 28, 28]}],\n",
       " 'outputs': [{'name': '18', 'datatype': 'FP32', 'shape': [1, 10]}]}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(\n",
    "    \"http://mnist.ntl-us-ibm-com.svc.cluster.local/v2/models/mnist/versions/1\"\n",
    ").json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f0991d23-3172-4852-b186-fb9f05a7448c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"input.1\",\n",
    "            \"shape\": list(tensor.shape),\n",
    "            \"datatype\": \"FP32\",\n",
    "            \"data\": tensor.flatten().tolist(),\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "res = requests.post(url, headers={}, data=json.dumps(payload))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a6f7118b-ce92-42df-976a-590424a6fa50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(res.json()[\"outputs\"][0][\"data\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
