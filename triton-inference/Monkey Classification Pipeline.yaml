apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: end-to-end-monkey-species-classification-pipeline-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.18, pipelines.kubeflow.org/pipeline_compilation_time: '2023-01-28T01:00:11.421772',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "An example pipeline that
      performs an image classification and determines different monkey species", "inputs":
      [{"default": "monkey-classification", "name": "model_name", "optional": true,
      "type": "String"}, {"default": "1", "name": "model_version", "optional": true,
      "type": "Integer"}, {"default": "minio-service.kubeflow:9000", "name": "minio_url",
      "optional": true, "type": "String"}], "name": "End-to-end monkey species classification
      pipeline"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.18}
spec:
  entrypoint: end-to-end-monkey-species-classification-pipeline
  templates:
  - name: convert-model-to-onnx
    container:
      args: [--model-dir, /tmp/inputs/model_dir/data, --onnx-model-dir, /tmp/outputs/onnx_model_dir/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def convert_model_to_onnx(
            model_dir,
            onnx_model_dir
        ):
            """Converts a model to ONNX format. Supported input formats: Keras."""

            import logging
            import onnx
            import sys
            import tensorflow as tf
            import tf2onnx

            logging.basicConfig(
                stream=sys.stdout,
                level=logging.INFO,
                format='%(levelname)s %(asctime)s: %(message)s'
            )
            logger = logging.getLogger()

            logger.info(f"Loading model from '{model_dir}'...")
            keras_model = tf.keras.models.load_model(model_dir)

            logger.info("Converting model to ONNX...")
            converted_model, _ = tf2onnx.convert.from_keras(keras_model)

            logger.info(f"Saving ONNX model to '{onnx_model_dir}'...")
            onnx.save_model(converted_model, onnx_model_dir)

            logger.info("Finished.")

        import argparse
        _parser = argparse.ArgumentParser(prog='Convert model to onnx', description='Converts a model to ONNX format. Supported input formats: Keras.')
        _parser.add_argument("--model-dir", dest="model_dir", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--onnx-model-dir", dest="onnx_model_dir", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = convert_model_to_onnx(**_parsed_args)
      image: quay.io/ibm/kubeflow-notebook-image-ppc64le:latest
    inputs:
      artifacts:
      - {name: train-model-model_dir, path: /tmp/inputs/model_dir/data}
    outputs:
      artifacts:
      - {name: convert-model-to-onnx-onnx_model_dir, path: /tmp/outputs/onnx_model_dir/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Converts
          a model to ONNX format. Supported input formats: Keras.", "implementation":
          {"container": {"args": ["--model-dir", {"inputPath": "model_dir"}, "--onnx-model-dir",
          {"outputPath": "onnx_model_dir"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef convert_model_to_onnx(\n    model_dir,\n    onnx_model_dir\n):\n    \"\"\"Converts
          a model to ONNX format. Supported input formats: Keras.\"\"\"\n\n    import
          logging\n    import onnx\n    import sys\n    import tensorflow as tf\n    import
          tf2onnx\n\n    logging.basicConfig(\n        stream=sys.stdout,\n        level=logging.INFO,\n        format=''%(levelname)s
          %(asctime)s: %(message)s''\n    )\n    logger = logging.getLogger()\n\n    logger.info(f\"Loading
          model from ''{model_dir}''...\")\n    keras_model = tf.keras.models.load_model(model_dir)\n\n    logger.info(\"Converting
          model to ONNX...\")\n    converted_model, _ = tf2onnx.convert.from_keras(keras_model)\n\n    logger.info(f\"Saving
          ONNX model to ''{onnx_model_dir}''...\")\n    onnx.save_model(converted_model,
          onnx_model_dir)\n\n    logger.info(\"Finished.\")\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Convert model to onnx'', description=''Converts
          a model to ONNX format. Supported input formats: Keras.'')\n_parser.add_argument(\"--model-dir\",
          dest=\"model_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--onnx-model-dir\",
          dest=\"onnx_model_dir\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = convert_model_to_onnx(**_parsed_args)\n"], "image": "quay.io/ibm/kubeflow-notebook-image-ppc64le:latest"}},
          "inputs": [{"name": "model_dir", "type": "String"}], "name": "Convert model
          to onnx", "outputs": [{"name": "onnx_model_dir", "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "d7a79d9da2486fe842613ed02ddceeb6255f0e7a2b52f62c9fe13e81e761a939", "url":
          "/home/jovyan/components/model-building/convert-to-onnx/component.yaml"}'}
  - name: deploy-inference-service
    container:
      args:
      - --name
      - '{{inputs.parameters.model_name}}'
      - --storage-uri
      - s3://{{workflow.namespace}}-models/onnx/
      - --minio-url
      - '{{inputs.parameters.minio_url}}'
      - --rm-existing
      - "True"
      - --minio-credential-secret
      - mlpipeline-minio-artifact
      - --concurrency-target
      - '4'
      - --predictor-min-replicas
      - '1'
      - --predictor-max-replicas
      - '4'
      - --predictor-gpu-allocation
      - '1'
      - --predictor-protocol
      - grpc-v2
      - --triton-runtime-version
      - 21.08-py3-gpu
      - --transformer-specification
      - '{"image": "quay.io/ntlawrence/monkeytransform:latest", "labels": "{{inputs.parameters.load-test-train-dataset-for-tf-class_names}}",
        "maxReplicas": 4, "minReplicas": 1}'
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def deploy_inference_service(\n    name,\n    storage_uri,\n    minio_url,\n\
        \    rm_existing = False,\n    minio_credential_secret=\"mlpipeline-minio-artifact\"\
        ,\n    concurrency_target = None,\n    predictor_min_replicas = None,\n  \
        \  predictor_max_replicas = None,\n    predictor_gpu_allocation = 0,\n   \
        \ predictor_protocol = \"v2\",  # or grpc-v2\n    triton_runtime_version =\
        \ \"22.03-py3\",\n    transformer_specification = None,\n):\n    import os\n\
        \    import subprocess\n    import yaml\n    import base64\n\n    # https://kserve.github.io/website/modelserving/storage/s3/s3/#predict-on-a-inferenceservice-with-a-saved-model-on-s3\n\
        \    # https://kserve.github.io/website/reference/api/\n    # https://kserve.github.io/website/modelserving/autoscaling/autoscaling/\n\
        \    # https://kserve.github.io/website/modelserving/v1beta1/triton/torchscript/#inference-with-http-endpoint\n\
        \    # https://github.com/kserve/kserve/blob/master/docs/samples/multimodelserving/triton/README.md\n\
        \    # https://kserve.github.io/website/modelserving/v1beta1/triton/torchscript/#inference-with-grpc-endpoint\n\
        \n    # It happens that the credentials for the minio user name and password\
        \ are already in a secret\n    # This just loads them so that we can create\
        \ our own secret to store the S3 connection information\n    # for the Inference\
        \ service\n    r = subprocess.run(\n        [\"kubectl\", \"get\", \"secret\"\
        , minio_credential_secret, \"-oyaml\"],\n        stdout=subprocess.PIPE,\n\
        \        check=True,\n        text=True,\n    )\n    secret = yaml.safe_load(r.stdout)\n\
        \n    s3_credentials_spec = f\"\"\"\n    apiVersion: v1\n    kind: Secret\n\
        \    metadata:\n      name: minio-credentials\n      annotations:\n      \
        \  serving.kserve.io/s3-endpoint: {minio_url} \n        serving.kserve.io/s3-usehttps:\
        \ \"0\"\n        serving.kserve.io/s3-region: \"us-west1\"\n        serving.kserve.io/s3-useanoncredential:\
        \ \"false\"\n    type: Opaque\n    data:\n      AWS_ACCESS_KEY_ID: {secret['data']['accesskey']}\n\
        \      AWS_SECRET_ACCESS_KEY: {secret['data']['secretkey']}\n    \"\"\"\n\n\
        \    print(s3_credentials_spec)\n    subprocess.run(\n        [\"kubectl\"\
        , \"apply\", \"-f\", \"-\"],\n        input=s3_credentials_spec,\n       \
        \ check=True,\n        text=True,\n    )\n\n    sa_spec = \"\"\"\n    apiVersion:\
        \ v1\n    kind: ServiceAccount\n    metadata:\n      name: kserve-inference-sa\n\
        \    secrets:\n    - name: minio-credentials\n    \"\"\"\n\n    print(sa_spec)\n\
        \    subprocess.run(\n        [\"kubectl\", \"apply\", \"-f\", \"-\"], input=sa_spec,\
        \ check=True, text=True\n    )\n\n    ### Remove Existing\n    if rm_existing:\n\
        \        subprocess.run([\"kubectl\", \"delete\", \"inferenceservice\", name],\
        \ check=False)\n\n    ####### Inference Service template #######\n    min_t_replicas\
        \ = (\n        (\"minReplicas: \" + transformer_specification[\"min_replicas\"\
        ])\n        if \"min_replicas\" in transformer_specification\n        else\
        \ \"\"\n    )\n    max_t_replicas = (\n        (\"maxReplicas: \" + transformer_specification[\"\
        max_replicas\"])\n        if \"min_replicas\" in transformer_specification\n\
        \        else \"\"\n    )\n\n    # There is a bug in the pipeline that prevents\
        \ passing\n    # json serialized strings in some instances, workaround\n \
        \   # was to use base64 encoded json strings\n    if \"labels\" in transformer_specification:\n\
        \        labels = base64.b64decode(transformer_specification[\"labels\"]).decode(\"\
        utf-8\")\n    else:\n        labels = \"\"\n\n    if transformer_specification:\n\
        \        transform_spec = f\"\"\"\n      transformer:\n        {min_t_replicas}\n\
        \        {max_t_replicas}\n        serviceAccountName: kserve-inference-sa\n\
        \        containers:\n        - image: \"{transformer_specification[\"image\"\
        ]}\n          name: \"{name}-transformer\"\n          command: {transformer_specification.get(\"\
        command\", '[\"python\", \"transform.py\"]')}\n          args: [\"--protocol={predictor_protocol}\"\
        ]\n          env:\n            - name: STORAGE_URI\n              value: {storage_uri}\n\
        \            - name: CLASS_LABELS\n              value: |\n              \
        \      {labels}\n          \"\"\"\n    else:\n        transform_spec = \"\"\
        \n\n    gpu_resources = (\n        f\"nvidia.com/gpu: {predictor_gpu_allocation}\"\
        \n        if predictor_gpu_allocation\n        else \"\"\n    )\n\n    min_p_replicas\
        \ = (\n        f\"minReplicas: {predictor_min_replicas}\"\n        if predictor_min_replicas\
        \ is not None\n        else \"\"\n    )\n    max_p_replicas = (\n        f\"\
        maxReplicas: {predictor_max_replicas}\"\n        if predictor_max_replicas\
        \ is not None\n        else \"\"\n    )\n\n    predictor_port_spec = (\n \
        \       '[{\"containerPort\": 9000, \"name\": \"h2c\", \"protocol\": \"TCP\"\
        }]'\n        if predictor_protocol == \"grpc-v2\"\n        else \"\"\n   \
        \ )\n\n    if concurrency_target:\n        autoscaling_target = f\"\"\"\n\
        \        autoscaling.knative.dev/target: \"{concurrency_target}\"\n      \
        \  autoscaling.knative.dev/metric: \"concurrency\"\n        \"\"\"\n    else:\n\
        \        autoscaling_target = \"\"\n\n    service_spec = f\"\"\"\n    apiVersion:\
        \ serving.kserve.io/v1beta1\n    kind: InferenceService\n    metadata:\n \
        \     name: {name}\n      annotations:\n        sidecar.istio.io/inject: \"\
        false\"\n        # https://knative.dev/docs/serving/autoscaling/concurrency/#soft-limit\n\
        \        {autoscaling_target}\n    spec:\n      {transform_spec}\n\n     \
        \ predictor:\n        {min_p_replicas}\n        {max_p_replicas}\n       \
        \ serviceAccountName: kserve-inference-sa\n        triton:\n          runtimeVersion:\
        \ {triton_runtime_version}\n          args: [ \"--strict-model-config=false\"\
        ]\n          storageUri: {storage_uri}\n          ports: {predictor_port_spec}\n\
        \          env:\n          - name: OMP_NUM_THREADS\n            value: \"\
        1\"\n          resources:\n            limits:\n               {gpu_resources}\n\
        \    \"\"\"\n\n    print(service_spec)\n    subprocess.run(\n        [\"kubectl\"\
        , \"apply\", \"-f\", \"-\"], input=service_spec, check=True, text=True\n \
        \   )\n\n    print(\"Waiting for inference service to become available\")\n\
        \    subprocess.run(\n        [\n            \"kubectl\",\n            \"\
        wait\",\n            \"--for=condition=Ready\",\n            f\"inferenceservice/{name}\"\
        ,\n            \"--timeout=600s\",\n        ],\n        check=True,\n    )\n\
        \ndef _deserialize_bool(s) -> bool:\n    from distutils.util import strtobool\n\
        \    return strtobool(s) == 1\n\nimport json\nimport argparse\n_parser = argparse.ArgumentParser(prog='Deploy\
        \ inference service', description='')\n_parser.add_argument(\"--name\", dest=\"\
        name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --storage-uri\", dest=\"storage_uri\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--minio-url\", dest=\"minio_url\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--rm-existing\", dest=\"\
        rm_existing\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--minio-credential-secret\", dest=\"minio_credential_secret\"\
        , type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --concurrency-target\", dest=\"concurrency_target\", type=int, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictor-min-replicas\"\
        , dest=\"predictor_min_replicas\", type=int, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--predictor-max-replicas\", dest=\"predictor_max_replicas\"\
        , type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --predictor-gpu-allocation\", dest=\"predictor_gpu_allocation\", type=int,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictor-protocol\"\
        , dest=\"predictor_protocol\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--triton-runtime-version\", dest=\"triton_runtime_version\"\
        , type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --transformer-specification\", dest=\"transformer_specification\", type=json.loads,\
        \ required=False, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
        \n_outputs = deploy_inference_service(**_parsed_args)\n"
      image: quay.io/ibm/kubeflow-notebook-image-ppc64le:elyra3.13.0-py3.8-tf2.9.2-pt1.12.1-v0
    inputs:
      parameters:
      - {name: load-test-train-dataset-for-tf-class_names}
      - {name: minio_url}
      - {name: model_name}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--name", {"inputValue": "name"}, "--storage-uri", {"inputValue":
          "storage_uri"}, "--minio-url", {"inputValue": "minio_url"}, {"if": {"cond":
          {"isPresent": "rm_existing"}, "then": ["--rm-existing", {"inputValue": "rm_existing"}]}},
          {"if": {"cond": {"isPresent": "minio_credential_secret"}, "then": ["--minio-credential-secret",
          {"inputValue": "minio_credential_secret"}]}}, {"if": {"cond": {"isPresent":
          "concurrency_target"}, "then": ["--concurrency-target", {"inputValue": "concurrency_target"}]}},
          {"if": {"cond": {"isPresent": "predictor_min_replicas"}, "then": ["--predictor-min-replicas",
          {"inputValue": "predictor_min_replicas"}]}}, {"if": {"cond": {"isPresent":
          "predictor_max_replicas"}, "then": ["--predictor-max-replicas", {"inputValue":
          "predictor_max_replicas"}]}}, {"if": {"cond": {"isPresent": "predictor_gpu_allocation"},
          "then": ["--predictor-gpu-allocation", {"inputValue": "predictor_gpu_allocation"}]}},
          {"if": {"cond": {"isPresent": "predictor_protocol"}, "then": ["--predictor-protocol",
          {"inputValue": "predictor_protocol"}]}}, {"if": {"cond": {"isPresent": "triton_runtime_version"},
          "then": ["--triton-runtime-version", {"inputValue": "triton_runtime_version"}]}},
          {"if": {"cond": {"isPresent": "transformer_specification"}, "then": ["--transformer-specification",
          {"inputValue": "transformer_specification"}]}}], "command": ["sh", "-ec",
          "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def deploy_inference_service(\n    name,\n    storage_uri,\n    minio_url,\n    rm_existing
          = False,\n    minio_credential_secret=\"mlpipeline-minio-artifact\",\n    concurrency_target
          = None,\n    predictor_min_replicas = None,\n    predictor_max_replicas
          = None,\n    predictor_gpu_allocation = 0,\n    predictor_protocol = \"v2\",  #
          or grpc-v2\n    triton_runtime_version = \"22.03-py3\",\n    transformer_specification
          = None,\n):\n    import os\n    import subprocess\n    import yaml\n    import
          base64\n\n    # https://kserve.github.io/website/modelserving/storage/s3/s3/#predict-on-a-inferenceservice-with-a-saved-model-on-s3\n    #
          https://kserve.github.io/website/reference/api/\n    # https://kserve.github.io/website/modelserving/autoscaling/autoscaling/\n    #
          https://kserve.github.io/website/modelserving/v1beta1/triton/torchscript/#inference-with-http-endpoint\n    #
          https://github.com/kserve/kserve/blob/master/docs/samples/multimodelserving/triton/README.md\n    #
          https://kserve.github.io/website/modelserving/v1beta1/triton/torchscript/#inference-with-grpc-endpoint\n\n    #
          It happens that the credentials for the minio user name and password are
          already in a secret\n    # This just loads them so that we can create our
          own secret to store the S3 connection information\n    # for the Inference
          service\n    r = subprocess.run(\n        [\"kubectl\", \"get\", \"secret\",
          minio_credential_secret, \"-oyaml\"],\n        stdout=subprocess.PIPE,\n        check=True,\n        text=True,\n    )\n    secret
          = yaml.safe_load(r.stdout)\n\n    s3_credentials_spec = f\"\"\"\n    apiVersion:
          v1\n    kind: Secret\n    metadata:\n      name: minio-credentials\n      annotations:\n        serving.kserve.io/s3-endpoint:
          {minio_url} \n        serving.kserve.io/s3-usehttps: \"0\"\n        serving.kserve.io/s3-region:
          \"us-west1\"\n        serving.kserve.io/s3-useanoncredential: \"false\"\n    type:
          Opaque\n    data:\n      AWS_ACCESS_KEY_ID: {secret[''data''][''accesskey'']}\n      AWS_SECRET_ACCESS_KEY:
          {secret[''data''][''secretkey'']}\n    \"\"\"\n\n    print(s3_credentials_spec)\n    subprocess.run(\n        [\"kubectl\",
          \"apply\", \"-f\", \"-\"],\n        input=s3_credentials_spec,\n        check=True,\n        text=True,\n    )\n\n    sa_spec
          = \"\"\"\n    apiVersion: v1\n    kind: ServiceAccount\n    metadata:\n      name:
          kserve-inference-sa\n    secrets:\n    - name: minio-credentials\n    \"\"\"\n\n    print(sa_spec)\n    subprocess.run(\n        [\"kubectl\",
          \"apply\", \"-f\", \"-\"], input=sa_spec, check=True, text=True\n    )\n\n    ###
          Remove Existing\n    if rm_existing:\n        subprocess.run([\"kubectl\",
          \"delete\", \"inferenceservice\", name], check=False)\n\n    ####### Inference
          Service template #######\n    min_t_replicas = (\n        (\"minReplicas:
          \" + transformer_specification[\"min_replicas\"])\n        if \"min_replicas\"
          in transformer_specification\n        else \"\"\n    )\n    max_t_replicas
          = (\n        (\"maxReplicas: \" + transformer_specification[\"max_replicas\"])\n        if
          \"min_replicas\" in transformer_specification\n        else \"\"\n    )\n\n    #
          There is a bug in the pipeline that prevents passing\n    # json serialized
          strings in some instances, workaround\n    # was to use base64 encoded json
          strings\n    if \"labels\" in transformer_specification:\n        labels
          = base64.b64decode(transformer_specification[\"labels\"]).decode(\"utf-8\")\n    else:\n        labels
          = \"\"\n\n    if transformer_specification:\n        transform_spec = f\"\"\"\n      transformer:\n        {min_t_replicas}\n        {max_t_replicas}\n        serviceAccountName:
          kserve-inference-sa\n        containers:\n        - image: \"{transformer_specification[\"image\"]}\n          name:
          \"{name}-transformer\"\n          command: {transformer_specification.get(\"command\",
          ''[\"python\", \"transform.py\"]'')}\n          args: [\"--protocol={predictor_protocol}\"]\n          env:\n            -
          name: STORAGE_URI\n              value: {storage_uri}\n            - name:
          CLASS_LABELS\n              value: |\n                    {labels}\n          \"\"\"\n    else:\n        transform_spec
          = \"\"\n\n    gpu_resources = (\n        f\"nvidia.com/gpu: {predictor_gpu_allocation}\"\n        if
          predictor_gpu_allocation\n        else \"\"\n    )\n\n    min_p_replicas
          = (\n        f\"minReplicas: {predictor_min_replicas}\"\n        if predictor_min_replicas
          is not None\n        else \"\"\n    )\n    max_p_replicas = (\n        f\"maxReplicas:
          {predictor_max_replicas}\"\n        if predictor_max_replicas is not None\n        else
          \"\"\n    )\n\n    predictor_port_spec = (\n        ''[{\"containerPort\":
          9000, \"name\": \"h2c\", \"protocol\": \"TCP\"}]''\n        if predictor_protocol
          == \"grpc-v2\"\n        else \"\"\n    )\n\n    if concurrency_target:\n        autoscaling_target
          = f\"\"\"\n        autoscaling.knative.dev/target: \"{concurrency_target}\"\n        autoscaling.knative.dev/metric:
          \"concurrency\"\n        \"\"\"\n    else:\n        autoscaling_target =
          \"\"\n\n    service_spec = f\"\"\"\n    apiVersion: serving.kserve.io/v1beta1\n    kind:
          InferenceService\n    metadata:\n      name: {name}\n      annotations:\n        sidecar.istio.io/inject:
          \"false\"\n        # https://knative.dev/docs/serving/autoscaling/concurrency/#soft-limit\n        {autoscaling_target}\n    spec:\n      {transform_spec}\n\n      predictor:\n        {min_p_replicas}\n        {max_p_replicas}\n        serviceAccountName:
          kserve-inference-sa\n        triton:\n          runtimeVersion: {triton_runtime_version}\n          args:
          [ \"--strict-model-config=false\"]\n          storageUri: {storage_uri}\n          ports:
          {predictor_port_spec}\n          env:\n          - name: OMP_NUM_THREADS\n            value:
          \"1\"\n          resources:\n            limits:\n               {gpu_resources}\n    \"\"\"\n\n    print(service_spec)\n    subprocess.run(\n        [\"kubectl\",
          \"apply\", \"-f\", \"-\"], input=service_spec, check=True, text=True\n    )\n\n    print(\"Waiting
          for inference service to become available\")\n    subprocess.run(\n        [\n            \"kubectl\",\n            \"wait\",\n            \"--for=condition=Ready\",\n            f\"inferenceservice/{name}\",\n            \"--timeout=600s\",\n        ],\n        check=True,\n    )\n\ndef
          _deserialize_bool(s) -> bool:\n    from distutils.util import strtobool\n    return
          strtobool(s) == 1\n\nimport json\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Deploy
          inference service'', description='''')\n_parser.add_argument(\"--name\",
          dest=\"name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--storage-uri\",
          dest=\"storage_uri\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--minio-url\",
          dest=\"minio_url\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--rm-existing\",
          dest=\"rm_existing\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--minio-credential-secret\",
          dest=\"minio_credential_secret\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--concurrency-target\",
          dest=\"concurrency_target\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictor-min-replicas\",
          dest=\"predictor_min_replicas\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictor-max-replicas\",
          dest=\"predictor_max_replicas\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictor-gpu-allocation\",
          dest=\"predictor_gpu_allocation\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictor-protocol\",
          dest=\"predictor_protocol\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--triton-runtime-version\",
          dest=\"triton_runtime_version\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--transformer-specification\",
          dest=\"transformer_specification\", type=json.loads, required=False, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = deploy_inference_service(**_parsed_args)\n"],
          "image": "quay.io/ibm/kubeflow-notebook-image-ppc64le:elyra3.13.0-py3.8-tf2.9.2-pt1.12.1-v0"}},
          "inputs": [{"name": "name", "type": "String"}, {"name": "storage_uri", "type":
          "String"}, {"name": "minio_url", "type": "String"}, {"default": "False",
          "name": "rm_existing", "optional": true, "type": "Boolean"}, {"default":
          "mlpipeline-minio-artifact", "name": "minio_credential_secret", "optional":
          true}, {"name": "concurrency_target", "optional": true, "type": "Integer"},
          {"name": "predictor_min_replicas", "optional": true, "type": "Integer"},
          {"name": "predictor_max_replicas", "optional": true, "type": "Integer"},
          {"default": "0", "name": "predictor_gpu_allocation", "optional": true, "type":
          "Integer"}, {"default": "v2", "name": "predictor_protocol", "optional":
          true, "type": "String"}, {"default": "22.03-py3", "name": "triton_runtime_version",
          "optional": true, "type": "String"}, {"name": "transformer_specification",
          "optional": true, "type": "typing.Dict[str, typing.Union[str, int]]"}],
          "name": "Deploy inference service"}', pipelines.kubeflow.org/component_ref: '{"digest":
          "f21ccbc1efb65dae24ea3beba13c1f5716ffd67f0d2f235ec0ce6cdd6f293a1e", "url":
          "/home/jovyan/kubeflow-ppc64le-examples/deploy_triton_inference_service_component/deploy_triton_inference_service_component.yaml"}',
        pipelines.kubeflow.org/arguments.parameters: '{"concurrency_target": "4",
          "minio_credential_secret": "mlpipeline-minio-artifact", "minio_url": "{{inputs.parameters.minio_url}}",
          "name": "{{inputs.parameters.model_name}}", "predictor_gpu_allocation":
          "1", "predictor_max_replicas": "4", "predictor_min_replicas": "1", "predictor_protocol":
          "grpc-v2", "rm_existing": "True", "storage_uri": "s3://{{workflow.namespace}}-models/onnx/",
          "transformer_specification": "{\"image\": \"quay.io/ntlawrence/monkeytransform:latest\",
          \"labels\": \"{{inputs.parameters.load-test-train-dataset-for-tf-class_names}}\",
          \"maxReplicas\": 4, \"minReplicas\": 1}", "triton_runtime_version": "21.08-py3-gpu"}',
        pipelines.kubeflow.org/max_cache_staleness: P0D}
  - name: end-to-end-monkey-species-classification-pipeline
    inputs:
      parameters:
      - {name: minio_url}
      - {name: model_name}
      - {name: model_version}
    dag:
      tasks:
      - name: convert-model-to-onnx
        template: convert-model-to-onnx
        dependencies: [train-model]
        arguments:
          artifacts:
          - {name: train-model-model_dir, from: '{{tasks.train-model.outputs.artifacts.train-model-model_dir}}'}
      - name: deploy-inference-service
        template: deploy-inference-service
        dependencies: [load-test-train-dataset-for-tf, upload-model]
        arguments:
          parameters:
          - {name: load-test-train-dataset-for-tf-class_names, value: '{{tasks.load-test-train-dataset-for-tf.outputs.parameters.load-test-train-dataset-for-tf-class_names}}'}
          - {name: minio_url, value: '{{inputs.parameters.minio_url}}'}
          - {name: model_name, value: '{{inputs.parameters.model_name}}'}
      - name: evaluate-model
        template: evaluate-model
        dependencies: [load-test-train-dataset-for-tf, train-model]
        arguments:
          artifacts:
          - {name: load-test-train-dataset-for-tf-test_dataset_dir, from: '{{tasks.load-test-train-dataset-for-tf.outputs.artifacts.load-test-train-dataset-for-tf-test_dataset_dir}}'}
          - {name: train-model-model_dir, from: '{{tasks.train-model.outputs.artifacts.train-model-model_dir}}'}
      - {name: load-test-train-dataset-for-tf, template: load-test-train-dataset-for-tf}
      - name: train-model
        template: train-model
        dependencies: [load-test-train-dataset-for-tf]
        arguments:
          artifacts:
          - {name: load-test-train-dataset-for-tf-train_dataset_dir, from: '{{tasks.load-test-train-dataset-for-tf.outputs.artifacts.load-test-train-dataset-for-tf-train_dataset_dir}}'}
          - {name: load-test-train-dataset-for-tf-validation_dataset_dir, from: '{{tasks.load-test-train-dataset-for-tf.outputs.artifacts.load-test-train-dataset-for-tf-validation_dataset_dir}}'}
      - name: upload-model
        template: upload-model
        dependencies: [convert-model-to-onnx]
        arguments:
          parameters:
          - {name: model_name, value: '{{inputs.parameters.model_name}}'}
          - {name: model_version, value: '{{inputs.parameters.model_version}}'}
          artifacts:
          - {name: convert-model-to-onnx-onnx_model_dir, from: '{{tasks.convert-model-to-onnx.outputs.artifacts.convert-model-to-onnx-onnx_model_dir}}'}
  - name: evaluate-model
    container:
      args: [--test-dataset-dir, /tmp/inputs/test_dataset_dir/data, --model-dir, /tmp/inputs/model_dir/data,
        --batch-size, '20', '----output-paths', /tmp/outputs/mlpipeline_metrics/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def evaluate_model(
            test_dataset_dir, model_dir, batch_size = 20
        ):
            """Loads a saved model from file and uses a pre-downloaded dataset for evaluation.
            Model metrics are persisted to `{metrics_path}` for Kubeflow Pipelines metadata."""

            from collections import namedtuple
            import json
            import tensorflow as tf

            test_dataset = tf.data.experimental.load(test_dataset_dir)
            model = tf.keras.models.load_model(model_dir)
            (loss, accuracy) = model.evaluate(test_dataset)

            print((loss, accuracy))

            metrics = {
                "metrics": [
                    {"name": "loss", "numberValue": str(loss), "format": "RAW"},
                    {"name": "accuracy", "numberValue": str(accuracy), "format": "PERCENTAGE"},
                ]
            }

            out_tuple = namedtuple("EvaluationOutput", ["mlpipeline_metrics"])

            json_metrics = json.dumps(metrics)
            print(json_metrics)
            return out_tuple(json_metrics)

        import argparse
        _parser = argparse.ArgumentParser(prog='Evaluate model', description='Loads a saved model from file and uses a pre-downloaded dataset for evaluation.')
        _parser.add_argument("--test-dataset-dir", dest="test_dataset_dir", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--model-dir", dest="model_dir", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--batch-size", dest="batch_size", type=int, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = evaluate_model(**_parsed_args)

        _output_serializers = [
            str,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      image: quay.io/ibm/kubeflow-notebook-image-ppc64le:elyra3.14.1-py3.9-tf2.9.2-pt1.12.1-v0
    inputs:
      artifacts:
      - {name: train-model-model_dir, path: /tmp/inputs/model_dir/data}
      - {name: load-test-train-dataset-for-tf-test_dataset_dir, path: /tmp/inputs/test_dataset_dir/data}
    outputs:
      artifacts:
      - {name: mlpipeline-metrics, path: /tmp/outputs/mlpipeline_metrics/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Loads
          a saved model from file and uses a pre-downloaded dataset for evaluation.",
          "implementation": {"container": {"args": ["--test-dataset-dir", {"inputPath":
          "test_dataset_dir"}, "--model-dir", {"inputPath": "model_dir"}, {"if": {"cond":
          {"isPresent": "batch_size"}, "then": ["--batch-size", {"inputValue": "batch_size"}]}},
          "----output-paths", {"outputPath": "mlpipeline_metrics"}], "command": ["sh",
          "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def evaluate_model(\n    test_dataset_dir,
          model_dir, batch_size = 20\n):\n    \"\"\"Loads a saved model from file
          and uses a pre-downloaded dataset for evaluation.\n    Model metrics are
          persisted to `{metrics_path}` for Kubeflow Pipelines metadata.\"\"\"\n\n    from
          collections import namedtuple\n    import json\n    import tensorflow as
          tf\n\n    test_dataset = tf.data.experimental.load(test_dataset_dir)\n    model
          = tf.keras.models.load_model(model_dir)\n    (loss, accuracy) = model.evaluate(test_dataset)\n\n    print((loss,
          accuracy))\n\n    metrics = {\n        \"metrics\": [\n            {\"name\":
          \"loss\", \"numberValue\": str(loss), \"format\": \"RAW\"},\n            {\"name\":
          \"accuracy\", \"numberValue\": str(accuracy), \"format\": \"PERCENTAGE\"},\n        ]\n    }\n\n    out_tuple
          = namedtuple(\"EvaluationOutput\", [\"mlpipeline_metrics\"])\n\n    json_metrics
          = json.dumps(metrics)\n    print(json_metrics)\n    return out_tuple(json_metrics)\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Evaluate model'', description=''Loads
          a saved model from file and uses a pre-downloaded dataset for evaluation.'')\n_parser.add_argument(\"--test-dataset-dir\",
          dest=\"test_dataset_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-dir\",
          dest=\"model_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--batch-size\",
          dest=\"batch_size\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = evaluate_model(**_parsed_args)\n\n_output_serializers
          = [\n    str,\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "quay.io/ibm/kubeflow-notebook-image-ppc64le:elyra3.14.1-py3.9-tf2.9.2-pt1.12.1-v0"}},
          "inputs": [{"name": "test_dataset_dir", "type": "String"}, {"name": "model_dir",
          "type": "String"}, {"default": "20", "name": "batch_size", "optional": true,
          "type": "Integer"}], "name": "Evaluate model", "outputs": [{"name": "mlpipeline_metrics",
          "type": "Metrics"}]}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"batch_size":
          "20"}'}
  - name: load-test-train-dataset-for-tf
    container:
      args: [--dataset-url, Lehrig/Monkey-Species-Collection, --dataset-configuration,
        downsized, --train-dataset-dir, /tmp/outputs/train_dataset_dir/data, --validation-dataset-dir,
        /tmp/outputs/validation_dataset_dir/data, --test-dataset-dir, /tmp/outputs/test_dataset_dir/data,
        '----output-paths', /tmp/outputs/class_names/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def load_test_train_dataset_for_tf(
            train_dataset_dir,
            validation_dataset_dir,
            test_dataset_dir,
            dataset_url="Lehrig/Monkey-Species-Collection",
            dataset_configuration="downsized",
        ):
            import os

            import datasets
            import json
            import numpy as np
            import tensorflow as tf
            import base64

            def process(examples):
                examples["image"] = [np.array(img) for img in examples["image"]]

                examples["label"] = [ONE_HOT_MATRIX[label] for label in examples["label"]]

                return examples

            def save_as_tf_dataset(dataset, directory):
                if not os.path.exists(directory):
                    os.makedirs(directory)
                tf.data.experimental.save(
                    dataset.to_tf_dataset(
                        batch_size=16, columns=["image"], label_cols=["label"]
                    ),
                    directory,
                )

            dataset = datasets.load_dataset(dataset_url, dataset_configuration)

            CLASSES = dataset["train"].features["label"].names
            ONE_HOT_MATRIX = np.identity(len(CLASSES))

            dataset = dataset.shuffle(seed=42)

            # The input dataset is known to have images of shape (224, 224, 3)
            # The images are all of the same size, and so they can be batched
            # together. The model will have a resizing layer so that inference will
            # resize the input images if necessary, but there is an assumption
            # that images in the batch are all the same size for both
            # training and inferencing. We assume that is true with the
            # hugging face data and don't resize here.
            dataset = dataset.map(
                process,
                batched=True,
                batch_size=16,
                features=datasets.Features(
                    {
                        "image": datasets.Array3D(dtype="uint8", shape=(224, 224, 3)),
                        "label": datasets.Sequence(
                            feature=datasets.Value(dtype="int32"), length=len(CLASSES)
                        ),
                    }
                ),
                num_proc=8,
            )

            dev_test_dataset = dataset["test"].train_test_split(test_size=0.5, shuffle=False)

            save_as_tf_dataset(dataset["train"], train_dataset_dir)
            save_as_tf_dataset(dev_test_dataset["train"], validation_dataset_dir)
            save_as_tf_dataset(dev_test_dataset["test"], test_dataset_dir)

            # return as b64 encoded json, there is a bug when serializing
            # pipeline parameters that prevents us from returning simple
            # json
            return (base64.b64encode(json.dumps(CLASSES).encode("utf-8")).decode('utf-8'),)

        def _serialize_str(str_value: str) -> str:
            if not isinstance(str_value, str):
                raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                    str(str_value), str(type(str_value))))
            return str_value

        import argparse
        _parser = argparse.ArgumentParser(prog='Load test train dataset for tf', description='')
        _parser.add_argument("--dataset-url", dest="dataset_url", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--dataset-configuration", dest="dataset_configuration", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--train-dataset-dir", dest="train_dataset_dir", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--validation-dataset-dir", dest="validation_dataset_dir", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--test-dataset-dir", dest="test_dataset_dir", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = load_test_train_dataset_for_tf(**_parsed_args)

        _output_serializers = [
            _serialize_str,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      image: quay.io/ibm/kubeflow-notebook-image-ppc64le:elyra3.14.1-py3.9-tf2.9.2-pt1.12.1-v0
    outputs:
      parameters:
      - name: load-test-train-dataset-for-tf-class_names
        valueFrom: {path: /tmp/outputs/class_names/data}
      artifacts:
      - {name: load-test-train-dataset-for-tf-class_names, path: /tmp/outputs/class_names/data}
      - {name: load-test-train-dataset-for-tf-test_dataset_dir, path: /tmp/outputs/test_dataset_dir/data}
      - {name: load-test-train-dataset-for-tf-train_dataset_dir, path: /tmp/outputs/train_dataset_dir/data}
      - {name: load-test-train-dataset-for-tf-validation_dataset_dir, path: /tmp/outputs/validation_dataset_dir/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [{"if": {"cond": {"isPresent": "dataset_url"}, "then": ["--dataset-url",
          {"inputValue": "dataset_url"}]}}, {"if": {"cond": {"isPresent": "dataset_configuration"},
          "then": ["--dataset-configuration", {"inputValue": "dataset_configuration"}]}},
          "--train-dataset-dir", {"outputPath": "train_dataset_dir"}, "--validation-dataset-dir",
          {"outputPath": "validation_dataset_dir"}, "--test-dataset-dir", {"outputPath":
          "test_dataset_dir"}, "----output-paths", {"outputPath": "class_names"}],
          "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" >
          \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef load_test_train_dataset_for_tf(\n    train_dataset_dir,\n    validation_dataset_dir,\n    test_dataset_dir,\n    dataset_url=\"Lehrig/Monkey-Species-Collection\",\n    dataset_configuration=\"downsized\",\n):\n    import
          os\n\n    import datasets\n    import json\n    import numpy as np\n    import
          tensorflow as tf\n    import base64\n\n    def process(examples):\n        examples[\"image\"]
          = [np.array(img) for img in examples[\"image\"]]\n\n        examples[\"label\"]
          = [ONE_HOT_MATRIX[label] for label in examples[\"label\"]]\n\n        return
          examples\n\n    def save_as_tf_dataset(dataset, directory):\n        if
          not os.path.exists(directory):\n            os.makedirs(directory)\n        tf.data.experimental.save(\n            dataset.to_tf_dataset(\n                batch_size=16,
          columns=[\"image\"], label_cols=[\"label\"]\n            ),\n            directory,\n        )\n\n    dataset
          = datasets.load_dataset(dataset_url, dataset_configuration)\n\n    CLASSES
          = dataset[\"train\"].features[\"label\"].names\n    ONE_HOT_MATRIX = np.identity(len(CLASSES))\n\n    dataset
          = dataset.shuffle(seed=42)\n\n    # The input dataset is known to have images
          of shape (224, 224, 3)\n    # The images are all of the same size, and so
          they can be batched\n    # together. The model will have a resizing layer
          so that inference will\n    # resize the input images if necessary, but
          there is an assumption\n    # that images in the batch are all the same
          size for both\n    # training and inferencing. We assume that is true with
          the\n    # hugging face data and don''t resize here.\n    dataset = dataset.map(\n        process,\n        batched=True,\n        batch_size=16,\n        features=datasets.Features(\n            {\n                \"image\":
          datasets.Array3D(dtype=\"uint8\", shape=(224, 224, 3)),\n                \"label\":
          datasets.Sequence(\n                    feature=datasets.Value(dtype=\"int32\"),
          length=len(CLASSES)\n                ),\n            }\n        ),\n        num_proc=8,\n    )\n\n    dev_test_dataset
          = dataset[\"test\"].train_test_split(test_size=0.5, shuffle=False)\n\n    save_as_tf_dataset(dataset[\"train\"],
          train_dataset_dir)\n    save_as_tf_dataset(dev_test_dataset[\"train\"],
          validation_dataset_dir)\n    save_as_tf_dataset(dev_test_dataset[\"test\"],
          test_dataset_dir)\n\n    # return as b64 encoded json, there is a bug when
          serializing\n    # pipeline parameters that prevents us from returning simple\n    #
          json\n    return (base64.b64encode(json.dumps(CLASSES).encode(\"utf-8\")).decode(''utf-8''),)\n\ndef
          _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,
          str):\n        raise TypeError(''Value \"{}\" has type \"{}\" instead of
          str.''.format(\n            str(str_value), str(type(str_value))))\n    return
          str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Load
          test train dataset for tf'', description='''')\n_parser.add_argument(\"--dataset-url\",
          dest=\"dataset_url\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dataset-configuration\",
          dest=\"dataset_configuration\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--train-dataset-dir\",
          dest=\"train_dataset_dir\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--validation-dataset-dir\",
          dest=\"validation_dataset_dir\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--test-dataset-dir\",
          dest=\"test_dataset_dir\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\", dest=\"_output_paths\",
          type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = load_test_train_dataset_for_tf(**_parsed_args)\n\n_output_serializers
          = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "quay.io/ibm/kubeflow-notebook-image-ppc64le:elyra3.14.1-py3.9-tf2.9.2-pt1.12.1-v0"}},
          "inputs": [{"default": "Lehrig/Monkey-Species-Collection", "name": "dataset_url",
          "optional": true}, {"default": "downsized", "name": "dataset_configuration",
          "optional": true}], "name": "Load test train dataset for tf", "outputs":
          [{"name": "train_dataset_dir", "type": "String"}, {"name": "validation_dataset_dir",
          "type": "String"}, {"name": "test_dataset_dir", "type": "String"}, {"name":
          "class_names", "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"dataset_configuration": "downsized",
          "dataset_url": "Lehrig/Monkey-Species-Collection"}'}
  - name: train-model
    container:
      args: [--train-dataset-dir, /tmp/inputs/train_dataset_dir/data, --validation-dataset-dir,
        /tmp/inputs/validation_dataset_dir/data, --epochs, '100', --batch-size, '32',
        --model-dir, /tmp/outputs/model_dir/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def train_model(
            train_dataset_dir,
            validation_dataset_dir,
            model_dir,
            epochs = 100,
            batch_size = 32,
        ):
            """Uses transfer learning on a prepared dataset. Once trained, the model is persisted to `model_dir`."""
            import os
            import tensorflow as tf
            from tensorflow.keras import Sequential
            from tensorflow.keras.applications import InceptionV3
            from tensorflow.keras.layers import (
                BatchNormalization,
                Dense,
                Dropout,
                GlobalAveragePooling2D,
                Resizing,
                Rescaling,
                RandomContrast,
                RandomBrightness,
                RandomRotation,
                Input,
            )

            from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

            def build_model():

                backbone = InceptionV3(include_top=False, weights="imagenet")

                for layer in backbone.layers:
                    layer.trainable = False

                model = Sequential()
                model.add(Input(shape=(None, None, 3), name="image", dtype=tf.uint8))
                model.add(Resizing(height=224, width=224, interpolation="nearest"))
                model.add(Rescaling(scale=1.0 / 255))
                model.add(RandomContrast(factor=0.2, seed=42))
                model.add(RandomBrightness(factor=0.2, value_range=(0.0, 1.0), seed=42))
                model.add(RandomRotation(factor=0.2, seed=42))
                model.add(backbone)
                model.add(GlobalAveragePooling2D())
                model.add(Dense(128, activation="relu"))
                model.add(BatchNormalization())
                model.add(Dropout(0.3))
                model.add(Dense(64, activation="relu"))
                model.add(BatchNormalization())
                model.add(Dropout(0.3))
                model.add(Dense(10, activation="softmax", name="scores"))

                return model

            callbacks = [
                EarlyStopping(monitor="val_loss", patience=20, verbose=0, mode="min"),
                ReduceLROnPlateau(
                    monitor="val_loss",
                    factor=0.1,
                    patience=7,
                    verbose=1,
                    min_delta=0.0001,
                    mode="min",
                ),
            ]

            # Datasets were saved in batches so no need to batch again
            train_dataset = (
                tf.data.experimental.load(train_dataset_dir).cache().prefetch(tf.data.AUTOTUNE)
            )

            validation_dataset = (
                tf.data.experimental.load(validation_dataset_dir)
                .cache()
                .prefetch(tf.data.AUTOTUNE)
            )

            model = build_model()

            model.compile(
                optimizer="adam",
                loss="categorical_crossentropy",
                metrics=["categorical_accuracy"],
            )

            hist = model.fit(
                train_dataset,
                validation_data=validation_dataset,
                epochs=epochs,
                callbacks=callbacks,
            )

            print("Model train history:")
            print(hist.history)

            if not os.path.exists(model_dir):
                os.makedirs(model_dir)
            model.save(model_dir)
            print(f"Model saved to: {model_dir}")

        import argparse
        _parser = argparse.ArgumentParser(prog='Train model', description='Uses transfer learning on a prepared dataset. Once trained, the model is persisted to `model_dir`.')
        _parser.add_argument("--train-dataset-dir", dest="train_dataset_dir", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--validation-dataset-dir", dest="validation_dataset_dir", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--epochs", dest="epochs", type=int, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--batch-size", dest="batch_size", type=int, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--model-dir", dest="model_dir", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = train_model(**_parsed_args)
      image: quay.io/ibm/kubeflow-notebook-image-ppc64le:elyra3.14.1-py3.9-tf2.9.2-pt1.12.1-v0
      resources:
        limits: {nvidia.com/gpu: 1}
    inputs:
      artifacts:
      - {name: load-test-train-dataset-for-tf-train_dataset_dir, path: /tmp/inputs/train_dataset_dir/data}
      - {name: load-test-train-dataset-for-tf-validation_dataset_dir, path: /tmp/inputs/validation_dataset_dir/data}
    outputs:
      artifacts:
      - {name: train-model-model_dir, path: /tmp/outputs/model_dir/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Uses
          transfer learning on a prepared dataset. Once trained, the model is persisted
          to `model_dir`.", "implementation": {"container": {"args": ["--train-dataset-dir",
          {"inputPath": "train_dataset_dir"}, "--validation-dataset-dir", {"inputPath":
          "validation_dataset_dir"}, {"if": {"cond": {"isPresent": "epochs"}, "then":
          ["--epochs", {"inputValue": "epochs"}]}}, {"if": {"cond": {"isPresent":
          "batch_size"}, "then": ["--batch-size", {"inputValue": "batch_size"}]}},
          "--model-dir", {"outputPath": "model_dir"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef train_model(\n    train_dataset_dir,\n    validation_dataset_dir,\n    model_dir,\n    epochs
          = 100,\n    batch_size = 32,\n):\n    \"\"\"Uses transfer learning on a
          prepared dataset. Once trained, the model is persisted to `model_dir`.\"\"\"\n    import
          os\n    import tensorflow as tf\n    from tensorflow.keras import Sequential\n    from
          tensorflow.keras.applications import InceptionV3\n    from tensorflow.keras.layers
          import (\n        BatchNormalization,\n        Dense,\n        Dropout,\n        GlobalAveragePooling2D,\n        Resizing,\n        Rescaling,\n        RandomContrast,\n        RandomBrightness,\n        RandomRotation,\n        Input,\n    )\n\n    from
          tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\n    def
          build_model():\n\n        backbone = InceptionV3(include_top=False, weights=\"imagenet\")\n\n        for
          layer in backbone.layers:\n            layer.trainable = False\n\n        model
          = Sequential()\n        model.add(Input(shape=(None, None, 3), name=\"image\",
          dtype=tf.uint8))\n        model.add(Resizing(height=224, width=224, interpolation=\"nearest\"))\n        model.add(Rescaling(scale=1.0
          / 255))\n        model.add(RandomContrast(factor=0.2, seed=42))\n        model.add(RandomBrightness(factor=0.2,
          value_range=(0.0, 1.0), seed=42))\n        model.add(RandomRotation(factor=0.2,
          seed=42))\n        model.add(backbone)\n        model.add(GlobalAveragePooling2D())\n        model.add(Dense(128,
          activation=\"relu\"))\n        model.add(BatchNormalization())\n        model.add(Dropout(0.3))\n        model.add(Dense(64,
          activation=\"relu\"))\n        model.add(BatchNormalization())\n        model.add(Dropout(0.3))\n        model.add(Dense(10,
          activation=\"softmax\", name=\"scores\"))\n\n        return model\n\n    callbacks
          = [\n        EarlyStopping(monitor=\"val_loss\", patience=20, verbose=0,
          mode=\"min\"),\n        ReduceLROnPlateau(\n            monitor=\"val_loss\",\n            factor=0.1,\n            patience=7,\n            verbose=1,\n            min_delta=0.0001,\n            mode=\"min\",\n        ),\n    ]\n\n    #
          Datasets were saved in batches so no need to batch again\n    train_dataset
          = (\n        tf.data.experimental.load(train_dataset_dir).cache().prefetch(tf.data.AUTOTUNE)\n    )\n\n    validation_dataset
          = (\n        tf.data.experimental.load(validation_dataset_dir)\n        .cache()\n        .prefetch(tf.data.AUTOTUNE)\n    )\n\n    model
          = build_model()\n\n    model.compile(\n        optimizer=\"adam\",\n        loss=\"categorical_crossentropy\",\n        metrics=[\"categorical_accuracy\"],\n    )\n\n    hist
          = model.fit(\n        train_dataset,\n        validation_data=validation_dataset,\n        epochs=epochs,\n        callbacks=callbacks,\n    )\n\n    print(\"Model
          train history:\")\n    print(hist.history)\n\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n    model.save(model_dir)\n    print(f\"Model
          saved to: {model_dir}\")\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Train
          model'', description=''Uses transfer learning on a prepared dataset. Once
          trained, the model is persisted to `model_dir`.'')\n_parser.add_argument(\"--train-dataset-dir\",
          dest=\"train_dataset_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--validation-dataset-dir\",
          dest=\"validation_dataset_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--epochs\",
          dest=\"epochs\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--batch-size\",
          dest=\"batch_size\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-dir\",
          dest=\"model_dir\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = train_model(**_parsed_args)\n"], "image": "quay.io/ibm/kubeflow-notebook-image-ppc64le:elyra3.14.1-py3.9-tf2.9.2-pt1.12.1-v0"}},
          "inputs": [{"name": "train_dataset_dir", "type": "String"}, {"name": "validation_dataset_dir",
          "type": "String"}, {"default": "100", "name": "epochs", "optional": true,
          "type": "Integer"}, {"default": "32", "name": "batch_size", "optional":
          true, "type": "Integer"}], "name": "Train model", "outputs": [{"name": "model_dir",
          "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"batch_size":
          "32", "epochs": "100"}'}
  - name: upload-model
    container:
      args: [--model, /tmp/inputs/model/data, --minio-url, 'minio-service.kubeflow:9000',
        --minio-secret, mlpipeline-minio-artifact, --export-bucket, '{{workflow.namespace}}-models',
        --model-name, '{{inputs.parameters.model_name}}', --model-version, '{{inputs.parameters.model_version}}',
        --model-format, onnx, '----output-paths', /tmp/outputs/s3_address/data, /tmp/outputs/triton_s3_address/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def upload_model(
            model,
            model_config = None,
            minio_url = "minio-service.kubeflow:9000",
            minio_secret = "mlpipeline-minio-artifact",
            export_bucket = "models",
            model_name = "my-model",
            model_version = 1,
            model_format = "onnx",
        ):
            """Uploads a model file to MinIO artifact store."""

            from collections import namedtuple
            from kubernetes import client, config
            import logging
            from minio import Minio
            import sys

            logging.basicConfig(
                stream=sys.stdout,
                level=logging.INFO,
                format="%(levelname)s %(asctime)s: %(message)s",
            )
            logger = logging.getLogger()

            def get_minio_client(minio_secret):
                import base64
                from kubernetes.client.rest import ApiException

                def get_current_namespace():
                    SA_NAMESPACE = "/var/run/secrets/kubernetes.io/serviceaccount/namespace"
                    with open(SA_NAMESPACE) as f:
                        return f.read()

                def decode(text):
                    return base64.b64decode(text).decode("utf-8")

                config.load_incluster_config()
                api_client = client.ApiClient()

                try:
                    secret = client.CoreV1Api(api_client).read_namespaced_secret(
                        minio_secret, get_current_namespace()
                    )

                    minio_user = decode(secret.data["accesskey"])
                    minio_pass = decode(secret.data["secretkey"])

                    return Minio(
                        minio_url, access_key=minio_user, secret_key=minio_pass, secure=False
                    )
                except ApiException as e:
                    if e.status == 404:
                        logger.error(
                            "Failed to get secret 'mlpipeline-minio-artifact', which is needed for communicating with MinIO!"
                        )
                    raise Exception(e)

            logger.info(f"Establishing MinIO connection to '{minio_url}'...")
            minio_client = get_minio_client(minio_secret)

            # Create export bucket if it does not yet exist
            response = minio_client.list_buckets()
            export_bucket_exists = False
            for bucket in response:
                if bucket.name == export_bucket:
                    export_bucket_exists = True

            if not export_bucket_exists:
                logger.info(f"Creating bucket '{export_bucket}'...")
                minio_client.make_bucket(bucket_name=export_bucket)

            model_path = f"{model_name}/{model_version}/model.{model_format}"
            s3_address = f"s3://{minio_url}/{export_bucket}/{model_format}"
            triton_s3_address = f"{s3_address}/{model_path}"

            logger.info(f"Saving onnx file to MinIO (s3 address: {s3_address})...")
            minio_client.fput_object(
                bucket_name=export_bucket,  # bucket name in Minio
                object_name=f"{model_format}/{model_path}",  # file name in bucket of Minio / for Triton name MUST be model.onnx!
                file_path=model,  # file path / name in local system
            )

            if model_config:
                logger.info("Saving model config for triton to MinIO")
                # The config is above the version in the directory tree
                # https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_repository.md#repository-layout
                model_config_path = f"{model_name}/config.pbtxt"
                minio_client.fput_object(
                    bucket_name=export_bucket,  # bucket name in Minio
                    object_name=f"{model_format}/{model_config_path}",
                    file_path=model_config,  # file path / name in local system
                )

            logger.info("Finished.")
            out_tuple = namedtuple("UploadOutput", ["s3_address", "triton_s3_address"])
            return out_tuple(s3_address, triton_s3_address)

        def _serialize_str(str_value: str) -> str:
            if not isinstance(str_value, str):
                raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                    str(str_value), str(type(str_value))))
            return str_value

        import argparse
        _parser = argparse.ArgumentParser(prog='Upload model', description='Uploads a model file to MinIO artifact store.')
        _parser.add_argument("--model", dest="model", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--model-config", dest="model_config", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--minio-url", dest="minio_url", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--minio-secret", dest="minio_secret", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--export-bucket", dest="export_bucket", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--model-name", dest="model_name", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--model-version", dest="model_version", type=int, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--model-format", dest="model_format", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=2)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = upload_model(**_parsed_args)

        _output_serializers = [
            _serialize_str,
            _serialize_str,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      image: quay.io/ibm/kubeflow-notebook-image-ppc64le:elyra3.13.0-py3.8-tf2.9.2-pt1.12.1-v0
    inputs:
      parameters:
      - {name: model_name}
      - {name: model_version}
      artifacts:
      - {name: convert-model-to-onnx-onnx_model_dir, path: /tmp/inputs/model/data}
    outputs:
      artifacts:
      - {name: upload-model-s3_address, path: /tmp/outputs/s3_address/data}
      - {name: upload-model-triton_s3_address, path: /tmp/outputs/triton_s3_address/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Uploads
          a model file to MinIO artifact store.", "implementation": {"container":
          {"args": ["--model", {"inputPath": "model"}, {"if": {"cond": {"isPresent":
          "model_config"}, "then": ["--model-config", {"inputPath": "model_config"}]}},
          {"if": {"cond": {"isPresent": "minio_url"}, "then": ["--minio-url", {"inputValue":
          "minio_url"}]}}, {"if": {"cond": {"isPresent": "minio_secret"}, "then":
          ["--minio-secret", {"inputValue": "minio_secret"}]}}, {"if": {"cond": {"isPresent":
          "export_bucket"}, "then": ["--export-bucket", {"inputValue": "export_bucket"}]}},
          {"if": {"cond": {"isPresent": "model_name"}, "then": ["--model-name", {"inputValue":
          "model_name"}]}}, {"if": {"cond": {"isPresent": "model_version"}, "then":
          ["--model-version", {"inputValue": "model_version"}]}}, {"if": {"cond":
          {"isPresent": "model_format"}, "then": ["--model-format", {"inputValue":
          "model_format"}]}}, "----output-paths", {"outputPath": "s3_address"}, {"outputPath":
          "triton_s3_address"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def upload_model(\n    model,\n    model_config = None,\n    minio_url
          = \"minio-service.kubeflow:9000\",\n    minio_secret = \"mlpipeline-minio-artifact\",\n    export_bucket
          = \"models\",\n    model_name = \"my-model\",\n    model_version = 1,\n    model_format
          = \"onnx\",\n):\n    \"\"\"Uploads a model file to MinIO artifact store.\"\"\"\n\n    from
          collections import namedtuple\n    from kubernetes import client, config\n    import
          logging\n    from minio import Minio\n    import sys\n\n    logging.basicConfig(\n        stream=sys.stdout,\n        level=logging.INFO,\n        format=\"%(levelname)s
          %(asctime)s: %(message)s\",\n    )\n    logger = logging.getLogger()\n\n    def
          get_minio_client(minio_secret):\n        import base64\n        from kubernetes.client.rest
          import ApiException\n\n        def get_current_namespace():\n            SA_NAMESPACE
          = \"/var/run/secrets/kubernetes.io/serviceaccount/namespace\"\n            with
          open(SA_NAMESPACE) as f:\n                return f.read()\n\n        def
          decode(text):\n            return base64.b64decode(text).decode(\"utf-8\")\n\n        config.load_incluster_config()\n        api_client
          = client.ApiClient()\n\n        try:\n            secret = client.CoreV1Api(api_client).read_namespaced_secret(\n                minio_secret,
          get_current_namespace()\n            )\n\n            minio_user = decode(secret.data[\"accesskey\"])\n            minio_pass
          = decode(secret.data[\"secretkey\"])\n\n            return Minio(\n                minio_url,
          access_key=minio_user, secret_key=minio_pass, secure=False\n            )\n        except
          ApiException as e:\n            if e.status == 404:\n                logger.error(\n                    \"Failed
          to get secret ''mlpipeline-minio-artifact'', which is needed for communicating
          with MinIO!\"\n                )\n            raise Exception(e)\n\n    logger.info(f\"Establishing
          MinIO connection to ''{minio_url}''...\")\n    minio_client = get_minio_client(minio_secret)\n\n    #
          Create export bucket if it does not yet exist\n    response = minio_client.list_buckets()\n    export_bucket_exists
          = False\n    for bucket in response:\n        if bucket.name == export_bucket:\n            export_bucket_exists
          = True\n\n    if not export_bucket_exists:\n        logger.info(f\"Creating
          bucket ''{export_bucket}''...\")\n        minio_client.make_bucket(bucket_name=export_bucket)\n\n    model_path
          = f\"{model_name}/{model_version}/model.{model_format}\"\n    s3_address
          = f\"s3://{minio_url}/{export_bucket}/{model_format}\"\n    triton_s3_address
          = f\"{s3_address}/{model_path}\"\n\n    logger.info(f\"Saving onnx file
          to MinIO (s3 address: {s3_address})...\")\n    minio_client.fput_object(\n        bucket_name=export_bucket,  #
          bucket name in Minio\n        object_name=f\"{model_format}/{model_path}\",  #
          file name in bucket of Minio / for Triton name MUST be model.onnx!\n        file_path=model,  #
          file path / name in local system\n    )\n\n    if model_config:\n        logger.info(\"Saving
          model config for triton to MinIO\")\n        # The config is above the version
          in the directory tree\n        # https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_repository.md#repository-layout\n        model_config_path
          = f\"{model_name}/config.pbtxt\"\n        minio_client.fput_object(\n            bucket_name=export_bucket,  #
          bucket name in Minio\n            object_name=f\"{model_format}/{model_config_path}\",\n            file_path=model_config,  #
          file path / name in local system\n        )\n\n    logger.info(\"Finished.\")\n    out_tuple
          = namedtuple(\"UploadOutput\", [\"s3_address\", \"triton_s3_address\"])\n    return
          out_tuple(s3_address, triton_s3_address)\n\ndef _serialize_str(str_value:
          str) -> str:\n    if not isinstance(str_value, str):\n        raise TypeError(''Value
          \"{}\" has type \"{}\" instead of str.''.format(\n            str(str_value),
          str(type(str_value))))\n    return str_value\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Upload model'', description=''Uploads a
          model file to MinIO artifact store.'')\n_parser.add_argument(\"--model\",
          dest=\"model\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-config\",
          dest=\"model_config\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--minio-url\",
          dest=\"minio_url\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--minio-secret\",
          dest=\"minio_secret\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--export-bucket\",
          dest=\"export_bucket\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-name\",
          dest=\"model_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-version\",
          dest=\"model_version\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-format\",
          dest=\"model_format\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=2)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = upload_model(**_parsed_args)\n\n_output_serializers
          = [\n    _serialize_str,\n    _serialize_str,\n\n]\n\nimport os\nfor idx,
          output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "quay.io/ibm/kubeflow-notebook-image-ppc64le:elyra3.13.0-py3.8-tf2.9.2-pt1.12.1-v0"}},
          "inputs": [{"name": "model", "type": "String"}, {"name": "model_config",
          "optional": true, "type": "String"}, {"default": "minio-service.kubeflow:9000",
          "name": "minio_url", "optional": true, "type": "String"}, {"default": "mlpipeline-minio-artifact",
          "name": "minio_secret", "optional": true, "type": "String"}, {"default":
          "models", "name": "export_bucket", "optional": true, "type": "String"},
          {"default": "my-model", "name": "model_name", "optional": true, "type":
          "String"}, {"default": "1", "name": "model_version", "optional": true, "type":
          "Integer"}, {"default": "onnx", "name": "model_format", "optional": true,
          "type": "String"}], "name": "Upload model", "outputs": [{"name": "s3_address",
          "type": "String"}, {"name": "triton_s3_address", "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "6da225cd49bc4c67005599df5513903975e1aa6837d1a1e41c503d7b793253c6", "url":
          "/home/jovyan/components/model-building/upload-model/component.yaml"}',
        pipelines.kubeflow.org/arguments.parameters: '{"export_bucket": "{{workflow.namespace}}-models",
          "minio_secret": "mlpipeline-minio-artifact", "minio_url": "minio-service.kubeflow:9000",
          "model_format": "onnx", "model_name": "{{inputs.parameters.model_name}}",
          "model_version": "{{inputs.parameters.model_version}}"}'}
  arguments:
    parameters:
    - {name: model_name, value: monkey-classification}
    - {name: model_version, value: '1'}
    - {name: minio_url, value: 'minio-service.kubeflow:9000'}
  serviceAccountName: pipeline-runner
