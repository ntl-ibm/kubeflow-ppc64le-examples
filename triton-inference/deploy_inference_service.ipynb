{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e373bc98-4651-464c-81c2-4017c73c9601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lab_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext lab_black\n"
     ]
    }
   ],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81816b35-ffc3-4798-b7c3-643317205029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.components import create_component_from_func, InputPath\n",
    "from typing import Dict, Union\n",
    "BASE_IMAGE = (\n",
    "    \"quay.io/ibm/kubeflow-notebook-image-ppc64le:elyra3.13.0-py3.8-tf2.9.2-pt1.12.1-v0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "009a7699-4124-4713-b3e9-a581ff986c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_inference_service(\n",
    "    name: str,\n",
    "    storage_uri: str,\n",
    "    minio_url: str,\n",
    "    class_labels: InputPath(str),\n",
    "    transformer_image: str = \"quay.io/ntlawrence/monkeytransform:latest\",\n",
    "    rm_existing: bool = False,\n",
    "    minio_credential_secret=\"mlpipeline-minio-artifact\",\n",
    "    min_replicas: int = None,\n",
    "    max_replicas: int = None,\n",
    "    pred_gpus: int = 0,\n",
    "    concurrency_target: int = 1,\n",
    "    triton_runtime_version: str = \"22.03-py3\",\n",
    "    transformer_specification: Dict[str, Union[str, int]] = None,\n",
    "):\n",
    "    import os\n",
    "    import subprocess\n",
    "    import yaml\n",
    "\n",
    "    # https://kserve.github.io/website/modelserving/storage/s3/s3/#predict-on-a-inferenceservice-with-a-saved-model-on-s3\n",
    "    # https://kserve.github.io/website/reference/api/\n",
    "    # https://kserve.github.io/website/modelserving/autoscaling/autoscaling/\n",
    "    # https://kserve.github.io/website/modelserving/v1beta1/triton/torchscript/#inference-with-http-endpoint\n",
    "    # https://github.com/kserve/kserve/blob/master/docs/samples/multimodelserving/triton/README.md\n",
    "    # https://kserve.github.io/website/modelserving/v1beta1/triton/torchscript/#inference-with-grpc-endpoint\n",
    "\n",
    "    # It happens that the credentials for the minio user name and password are already in a secret\n",
    "    # This just loads them so that we can create our own secret to store the S3 connection information\n",
    "    # for the Inference service\n",
    "    r = subprocess.run(\n",
    "        [\"kubectl\", \"get\", \"secret\", minio_credential_secret, \"-oyaml\"],\n",
    "        stdout=subprocess.PIPE,\n",
    "        check=True,\n",
    "        text=True,\n",
    "    )\n",
    "    secret = yaml.safe_load(r.stdout)\n",
    "\n",
    "    s3_credentials_spec = f\"\"\"\n",
    "    apiVersion: v1\n",
    "    kind: Secret\n",
    "    metadata:\n",
    "      name: minio-credentials\n",
    "      annotations:\n",
    "        serving.kserve.io/s3-endpoint: {minio_url} \n",
    "        serving.kserve.io/s3-usehttps: \"0\"\n",
    "        serving.kserve.io/s3-region: \"us-west1\"\n",
    "        serving.kserve.io/s3-useanoncredential: \"false\"\n",
    "    type: Opaque\n",
    "    data:\n",
    "      AWS_ACCESS_KEY_ID: {secret['data']['accesskey']}\n",
    "      AWS_SECRET_ACCESS_KEY: {secret['data']['secretkey']}\n",
    "    \"\"\"\n",
    "\n",
    "    print(s3_credentials_spec)\n",
    "    subprocess.run(\n",
    "        [\"kubectl\", \"apply\", \"-f\", \"-\"],\n",
    "        input=s3_credentials_spec,\n",
    "        check=True,\n",
    "        text=True,\n",
    "    )\n",
    "\n",
    "    sa_spec = \"\"\"\n",
    "    apiVersion: v1\n",
    "    kind: ServiceAccount\n",
    "    metadata:\n",
    "      name: kserve-inference-sa\n",
    "    secrets:\n",
    "    - name: minio-credentials\n",
    "    \"\"\"\n",
    "\n",
    "    print(sa_spec)\n",
    "    subprocess.run(\n",
    "        [\"kubectl\", \"apply\", \"-f\", \"-\"], input=sa_spec, check=True, text=True\n",
    "    )\n",
    "\n",
    "    if rm_existing:\n",
    "        subprocess.run([\"kubectl\", \"delete\", \"inferenceservice\", name], check=False)\n",
    "\n",
    "    with open(class_labels, \"r\") as clf:\n",
    "        labels_json = clf.read()\n",
    "\n",
    "    gpu_resources = f\"nvidia.com/gpu: {pred_gpus}\" if pred_gpus else \"\"\n",
    "\n",
    "    minReplicas = f\"minReplicas: {min_replicas}\" if min_replicas is not None else \"\"\n",
    "    maxReplicas = f\"maxReplicas: {max_replicas}\" if max_replicas else \"\"\n",
    "\n",
    "    service_spec = f\"\"\"\n",
    "    apiVersion: serving.kserve.io/v1beta1\n",
    "    kind: InferenceService\n",
    "    metadata:\n",
    "      name: {name}\n",
    "      annotations:\n",
    "        sidecar.istio.io/inject: \"false\"\n",
    "        # https://knative.dev/docs/serving/autoscaling/concurrency/#soft-limit\n",
    "        autoscaling.knative.dev/target: \"{concurrency_target}\"\n",
    "        autoscaling.knative.dev/metric: \"concurrency\"\n",
    "    spec:\n",
    "      transformer:\n",
    "        {minReplicas}\n",
    "        {maxReplicas}\n",
    "        serviceAccountName: kserve-inference-sa\n",
    "        containers:\n",
    "        - image: \"{transformer_image}\"\n",
    "          name: {name}-transformer\n",
    "          command: [\"python\", \"transform.py\"]\n",
    "          args: [\"--protocol=grpc-v2\"]\n",
    "          env:\n",
    "            - name: STORAGE_URI\n",
    "              value: {storage_uri}\n",
    "            - name: CLASS_LABELS\n",
    "              value: |\n",
    "                     {labels_json}\n",
    "\n",
    "      predictor:\n",
    "        {minReplicas}\n",
    "        {maxReplicas}\n",
    "        serviceAccountName: kserve-inference-sa\n",
    "        triton:\n",
    "          runtimeVersion: {triton_runtime_version}\n",
    "          args: [ \"--strict-model-config=false\"]\n",
    "          storageUri: {storage_uri}\n",
    "          ports:\n",
    "          - containerPort: 9000\n",
    "            name: h2c\n",
    "            protocol: TCP\n",
    "          env:\n",
    "          - name: OMP_NUM_THREADS\n",
    "            value: \"1\"\n",
    "          resources:\n",
    "            limits:\n",
    "               {gpu_resources}\n",
    "    \"\"\"\n",
    "\n",
    "    print(service_spec)\n",
    "    subprocess.run(\n",
    "        [\"kubectl\", \"apply\", \"-f\", \"-\"], input=service_spec, check=True, text=True\n",
    "    )\n",
    "\n",
    "    print(\"Waiting for inference service to become available\")\n",
    "    subprocess.run(\n",
    "        [\n",
    "            \"kubectl\",\n",
    "            \"wait\",\n",
    "            \"--for=condition=Ready\",\n",
    "            f\"inferenceservice/{name}\",\n",
    "            \"--timeout=600s\",\n",
    "        ],\n",
    "        check=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89a5f0a2-a06f-4e8e-8a5a-5ea396b00cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_inference_service_comp = create_component_from_func(\n",
    "    func=deploy_inference_service,\n",
    "    output_component_file=\"deploy_inference_service_component.yaml\",\n",
    "    base_image=BASE_IMAGE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a78336-9735-4f7b-9765-3ecaa69b342c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
