name: Deploy inference service
inputs:
- {name: name, type: String}
- {name: storage_uri, type: String}
- {name: minio_url, type: String}
- {name: class_labels, type: String}
- {name: transformer_image, type: String, default: 'quay.io/ntlawrence/monkeytransform:latest',
  optional: true}
- name: rm_existing
  type: Boolean
  default: "False"
  optional: true
- {name: minio_credential_secret, default: mlpipeline-minio-artifact, optional: true}
- {name: min_replicas, type: Integer, optional: true}
- {name: max_replicas, type: Integer, optional: true}
- {name: pred_gpus, type: Integer, default: '0', optional: true}
- {name: concurrency_target, type: Integer, default: '1', optional: true}
- {name: triton_runtime_version, type: String, default: 22.03-py3, optional: true}
implementation:
  container:
    image: quay.io/ibm/kubeflow-notebook-image-ppc64le:elyra3.13.0-py3.8-tf2.9.2-pt1.12.1-v0
    command:
    - sh
    - -ec
    - |
      program_path=$(mktemp)
      printf "%s" "$0" > "$program_path"
      python3 -u "$program_path" "$@"
    - "def deploy_inference_service(\n    name,\n    storage_uri,\n    minio_url,\n\
      \    class_labels,\n    transformer_image = \"quay.io/ntlawrence/monkeytransform:latest\"\
      ,\n    rm_existing = False,\n    minio_credential_secret=\"mlpipeline-minio-artifact\"\
      ,\n    min_replicas = None,\n    max_replicas = None,\n    pred_gpus = 0,\n\
      \    concurrency_target = 1,\n    triton_runtime_version = \"22.03-py3\",\n\
      ):\n    import os\n    import subprocess\n    import yaml\n\n    # https://kserve.github.io/website/modelserving/storage/s3/s3/#predict-on-a-inferenceservice-with-a-saved-model-on-s3\n\
      \    # https://kserve.github.io/website/reference/api/\n    # https://kserve.github.io/website/modelserving/autoscaling/autoscaling/\n\
      \    # https://kserve.github.io/website/modelserving/v1beta1/triton/torchscript/#inference-with-http-endpoint\n\
      \    # https://github.com/kserve/kserve/blob/master/docs/samples/multimodelserving/triton/README.md\n\
      \n    # It happens that the credentials for the minio user name and password\
      \ are already in a secret\n    # This just loads them so that we can create\
      \ our own secret to store the S3 connection information\n    # for the Inference\
      \ service\n    r = subprocess.run(\n        [\"kubectl\", \"get\", \"secret\"\
      , minio_credential_secret, \"-oyaml\"],\n        stdout=subprocess.PIPE,\n \
      \       check=True,\n        text=True,\n    )\n    secret = yaml.safe_load(r.stdout)\n\
      \n    s3_credentials_spec = f\"\"\"\n    apiVersion: v1\n    kind: Secret\n\
      \    metadata:\n      name: minio-credentials\n      annotations:\n        serving.kserve.io/s3-endpoint:\
      \ {minio_url} \n        serving.kserve.io/s3-usehttps: \"0\"\n        serving.kserve.io/s3-region:\
      \ \"us-west1\"\n        serving.kserve.io/s3-useanoncredential: \"false\"\n\
      \    type: Opaque\n    data:\n      AWS_ACCESS_KEY_ID: {secret['data']['accesskey']}\n\
      \      AWS_SECRET_ACCESS_KEY: {secret['data']['secretkey']}\n    \"\"\"\n\n\
      \    print(s3_credentials_spec)\n    subprocess.run(\n        [\"kubectl\",\
      \ \"apply\", \"-f\", \"-\"],\n        input=s3_credentials_spec,\n        check=True,\n\
      \        text=True,\n    )\n\n    sa_spec = \"\"\"\n    apiVersion: v1\n   \
      \ kind: ServiceAccount\n    metadata:\n      name: kserve-inference-sa\n   \
      \ secrets:\n    - name: minio-credentials\n    \"\"\"\n\n    print(sa_spec)\n\
      \    subprocess.run(\n        [\"kubectl\", \"apply\", \"-f\", \"-\"], input=sa_spec,\
      \ check=True, text=True\n    )\n\n    if rm_existing:\n        subprocess.run([\"\
      kubectl\", \"delete\", \"inferenceservice\", name], check=False)\n\n    with\
      \ open(class_labels, \"r\") as clf:\n        labels_json = clf.read()\n\n  \
      \  gpu_resources = f\"nvidia.com/gpu: {pred_gpus}\" if pred_gpus else \"\"\n\
      \n    minReplicas = f\"minReplicas: {min_replicas}\" if min_replicas is not\
      \ None else \"\"\n    maxReplicas = f\"maxReplicas: {max_replicas}\" if max_replicas\
      \ else \"\"\n\n    service_spec = f\"\"\"\n    apiVersion: serving.kserve.io/v1beta1\n\
      \    kind: InferenceService\n    metadata:\n      name: {name}\n      annotations:\n\
      \        sidecar.istio.io/inject: \"false\"\n        # https://knative.dev/docs/serving/autoscaling/concurrency/#soft-limit\n\
      \        autoscaling.knative.dev/target: \"{concurrency_target}\"\n        autoscaling.knative.dev/metric:\
      \ \"concurrency\"\n    spec:\n      transformer:\n        {minReplicas}\n  \
      \      {maxReplicas}\n        serviceAccountName: kserve-inference-sa\n    \
      \    containers:\n        - image: \"{transformer_image}\"\n          name:\
      \ {name}-transformer\n          command: [\"python\", \"transform.py\"]\n  \
      \        args: [\"--protocol=grpc-v2\"]\n          env:\n            - name:\
      \ STORAGE_URI\n              value: {storage_uri}\n            - name: CLASS_LABELS\n\
      \              value: |\n                     {labels_json}\n\n      predictor:\n\
      \        {minReplicas}\n        {maxReplicas}\n        serviceAccountName: kserve-inference-sa\n\
      \        triton:\n          runtimeVersion: {triton_runtime_version}\n     \
      \     args: [ \"--strict-model-config=false\"]\n          storageUri: {storage_uri}\n\
      \          ports:\n          - containerPort: 9000\n            name: h2c\n\
      \            protocol: TCP\n          env:\n          - name: OMP_NUM_THREADS\n\
      \            value: \"1\"\n          resources:\n            limits:\n     \
      \          {gpu_resources}\n    \"\"\"\n\n    print(service_spec)\n    subprocess.run(\n\
      \        [\"kubectl\", \"apply\", \"-f\", \"-\"], input=service_spec, check=True,\
      \ text=True\n    )\n\n    print(\"Waiting for inference service to become available\"\
      )\n    subprocess.run(\n        [\n            \"kubectl\",\n            \"\
      wait\",\n            \"--for=condition=Ready\",\n            f\"inferenceservice/{name}\"\
      ,\n            \"--timeout=600s\",\n        ],\n        check=True,\n    )\n\
      \ndef _deserialize_bool(s) -> bool:\n    from distutils.util import strtobool\n\
      \    return strtobool(s) == 1\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Deploy\
      \ inference service', description='')\n_parser.add_argument(\"--name\", dest=\"\
      name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
      --storage-uri\", dest=\"storage_uri\", type=str, required=True, default=argparse.SUPPRESS)\n\
      _parser.add_argument(\"--minio-url\", dest=\"minio_url\", type=str, required=True,\
      \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--class-labels\", dest=\"\
      class_labels\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
      --transformer-image\", dest=\"transformer_image\", type=str, required=False,\
      \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--rm-existing\", dest=\"\
      rm_existing\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n\
      _parser.add_argument(\"--minio-credential-secret\", dest=\"minio_credential_secret\"\
      , type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
      --min-replicas\", dest=\"min_replicas\", type=int, required=False, default=argparse.SUPPRESS)\n\
      _parser.add_argument(\"--max-replicas\", dest=\"max_replicas\", type=int, required=False,\
      \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--pred-gpus\", dest=\"\
      pred_gpus\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
      --concurrency-target\", dest=\"concurrency_target\", type=int, required=False,\
      \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--triton-runtime-version\"\
      , dest=\"triton_runtime_version\", type=str, required=False, default=argparse.SUPPRESS)\n\
      _parsed_args = vars(_parser.parse_args())\n\n_outputs = deploy_inference_service(**_parsed_args)\n"
    args:
    - --name
    - {inputValue: name}
    - --storage-uri
    - {inputValue: storage_uri}
    - --minio-url
    - {inputValue: minio_url}
    - --class-labels
    - {inputPath: class_labels}
    - if:
        cond: {isPresent: transformer_image}
        then:
        - --transformer-image
        - {inputValue: transformer_image}
    - if:
        cond: {isPresent: rm_existing}
        then:
        - --rm-existing
        - {inputValue: rm_existing}
    - if:
        cond: {isPresent: minio_credential_secret}
        then:
        - --minio-credential-secret
        - {inputValue: minio_credential_secret}
    - if:
        cond: {isPresent: min_replicas}
        then:
        - --min-replicas
        - {inputValue: min_replicas}
    - if:
        cond: {isPresent: max_replicas}
        then:
        - --max-replicas
        - {inputValue: max_replicas}
    - if:
        cond: {isPresent: pred_gpus}
        then:
        - --pred-gpus
        - {inputValue: pred_gpus}
    - if:
        cond: {isPresent: concurrency_target}
        then:
        - --concurrency-target
        - {inputValue: concurrency_target}
    - if:
        cond: {isPresent: triton_runtime_version}
        then:
        - --triton-runtime-version
        - {inputValue: triton_runtime_version}
