{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba72d66b-7e23-44c5-8fa3-e93d23f2088c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694bbf2f-551d-435a-90c9-e810c64971a3",
   "metadata": {},
   "source": [
    "# Honey Bee Computer Vision Example\n",
    "\n",
    "This example trains a yolov5 model to detect Honey Bees using the V7 version of the https://github.com/ultralytics/yolov5. The purpose of the example is to explain how to train a model using Kubeflow, YOLOV5 and sample data. \n",
    "\n",
    "Accessed classes are:\n",
    "* Bees (workers or foragers)\n",
    "* Bees carrying pollen\n",
    "* Drones\n",
    "* Queens\n",
    "\n",
    "\n",
    "For our purposes, we've included a sample of training data to use for this exercise. \n",
    "We also included initial weights calculated from a much larger version of this dataset with 500 epochs (This takes many hours to train).\n",
    "This should allow us to experiment with the pipeline design without needing to wait hours for training to complete.\n",
    "\n",
    "The dataset was sampled from here: https://universe.roboflow.com/matt-nudi/honey-bee-detection-model-zgjnb (creative commons license).\n",
    "\n",
    "\n",
    "The assumption is that this notebook has a data volume mounted (you can set this up when you create the notebook). The PVC should have been created with an access mode of ReadWriteMany. \n",
    "This allows other pods to mount the volume.\n",
    "\n",
    "The data set will be extracted to the data volume in these next few cells. The data will be loaded into the pipeline via a volume, rather than a download. This simulates a use case where the training data has been pre loaded on a volume in the kubernetes cluster, and is large enough where a download is expensive.\n",
    "\n",
    "The volume name is defined in this next cell, as is the path to the extracted Roboflow data set for the bees. To keep things simple, the mount point is the same for both the notebook server, and also for the containers that mount the volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b76914e-c3d1-496e-8a52-764eab5cd409",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOLUME_CLAIM_NAME = \"yolov5-work\"\n",
    "MOUNT_POINT = \"/home/jovyan/vol-1\"\n",
    "BEE_DATA_SET_PATH = f\"{MOUNT_POINT}/bee_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4f8c147-ef4d-4cad-9613-f52395b14fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {BEE_DATA_SET_PATH}\n",
    "!tar -xf data/dataset.tar.gz -C {BEE_DATA_SET_PATH} --strip-components 1\n",
    "!cp data/model.pt {MOUNT_POINT}/pre_trained_initial_weights.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf03b6e-d8cd-4900-896d-353f332a27c7",
   "metadata": {},
   "source": [
    "## Imports and constants\n",
    "\n",
    "The base image is an image that has been built to include the libraries for yolov5. The docker file is included in the \"Notebook Container Image Source\" directory. You can build this from the command line on SCOUT, but not from within a Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b555cbe3-c15a-4361-aa42-153fa814e9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.components import InputPath, OutputPath\n",
    "from kubernetes.client.models import (\n",
    "    V1Volume,\n",
    "    V1VolumeMount,\n",
    "    V1PersistentVolumeClaimVolumeSource,\n",
    ")\n",
    "from kfp.dsl import PipelineConf, data_passing_methods\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "BASE_IMAGE = \"quay.io/ntlawrence/yolov5:pt1.12.1-yolo7.0-v1.1\"\n",
    "COMPONENT_CATALOG_FOLDER = f\"{os.getenv('HOME')}/components\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f6c0e3-98ed-462c-8746-3adc4ecdf930",
   "metadata": {},
   "source": [
    "## Load Data component\n",
    "The first component in the pipeline copies the data from an input path to an output parameter.\n",
    "\n",
    "Essentially this moves the data from the volume into the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f800195c-1c9c-4a92-8106-d884a0975288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_url(\n",
    "    source: str,\n",
    "    dest: OutputPath(str),\n",
    "):\n",
    "    import os\n",
    "    import shutil\n",
    "    from urllib.request import urlretrieve\n",
    "    from urllib.parse import urlparse\n",
    "\n",
    "    # Make target directories if needed\n",
    "    parent_dirs = os.path.basename(dest)\n",
    "    if not os.path.exists(parent_dirs):\n",
    "        os.makedirs(parent_dirs)\n",
    "\n",
    "    # Option to use an empty file to indicate no weights\n",
    "    if not source:\n",
    "        with open(dest, \"w\") as _:\n",
    "            pass\n",
    "\n",
    "    source_details = urlparse(source)\n",
    "\n",
    "    if source_details.scheme == \"file\":\n",
    "        if os.path.isdir(source_details.path):\n",
    "            shutil.copytree(source_details.path, dest)\n",
    "        else:\n",
    "            shutil.copyfile(source_details.path, dest)\n",
    "    elif source_details.scheme in (\"http\", \"https\", \"ftp\", \"ftps\"):\n",
    "        urlretrieve(source, filename=dest)\n",
    "    else:\n",
    "        raise ValueError(f\"source does not use a supported url scheme\")\n",
    "\n",
    "\n",
    "load_from_url_comp = kfp.components.create_component_from_func(\n",
    "    load_from_url, base_image=BASE_IMAGE\n",
    ")\n",
    "\n",
    "\n",
    "def copy_data(source: str, dest: str):\n",
    "    import os\n",
    "    import shutil\n",
    "\n",
    "    # Make target directories if needed\n",
    "    parent_dirs = os.path.basename(dest)\n",
    "    if not os.path.exists(parent_dirs):\n",
    "        os.makedirs(parent_dirs)\n",
    "\n",
    "    if os.path.isdir(source):\n",
    "        shutil.copytree(source, dest)\n",
    "    else:\n",
    "        shutil.copyfile(source, dest)\n",
    "\n",
    "\n",
    "copy_data_comp = kfp.components.create_component_from_func(\n",
    "    copy_data, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3ec2ab-9d32-44af-b126-57afbe8e9c39",
   "metadata": {},
   "source": [
    "## Train Model component\n",
    "\n",
    "The training component has several steps to it:\n",
    "\n",
    "* Run the python train.py CLI to train the model\n",
    "* Convert the trained model to ONNX\n",
    "\n",
    "Performance could be improved by using distributed training. This will be included in the example in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "96e35b5a-ad5d-4510-9574-c41c910382f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: OutputPath(str),\n",
    "    results: OutputPath(str),\n",
    "    model_cfg: InputPath(str),\n",
    "    initial_weights: InputPath(str),\n",
    "    epochs: int,\n",
    "):\n",
    "    import subprocess\n",
    "    import pathlib\n",
    "    from ruamel.yaml import YAML\n",
    "    import os\n",
    "    import shutil\n",
    "\n",
    "    # Option to pass an empty file to train from scratch\n",
    "    weights = initial_weights if os.path.getsize(initial_weights) > 0 else \"\"\n",
    "\n",
    "    subprocess.run(\"find /dataset -print\", shell=True)\n",
    "    subprocess.run(\n",
    "        f\"python train.py --img 640 --batch -1 --noplots --epochs {epochs} --cache /home/jovyan/cache --cfg={model_cfg} \"\n",
    "        f\"--data /dataset/data.yaml --weights {weights} --workers=0 --device=0 --optimizer=Adam\",\n",
    "        check=True,\n",
    "        cwd=\"/yolov5\",\n",
    "        shell=True,\n",
    "    )\n",
    "\n",
    "    os.makedirs(os.path.dirname(model), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(results), exist_ok=True)\n",
    "\n",
    "    shutil.copyfile(\"/yolov5/runs/train/exp/weights/best.pt\", model)\n",
    "    shutil.copyfile(\"/yolov5/runs/train/exp/results.csv\", results)\n",
    "\n",
    "\n",
    "train_model_comp = kfp.components.create_component_from_func(\n",
    "    train_model, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfff5ee1-b5ad-402b-a70d-9ec970fc59e0",
   "metadata": {},
   "source": [
    "## Component to convert the model to ONNX\n",
    "\n",
    "The export tool supports quantizing, the model, and so we've added that option as a parameter to the component. The ability to quantize is important for models that will be used for inferencing on edge devices. Many edge devices do not have the resources to evaluate deep neural networks, and and smaller model makes it possible to run inferencing in those environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "87865918-1782-4392-9c6a-e14dd9c833b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_model_to_onnx(\n",
    "    model: InputPath(str),\n",
    "    model_format: str,\n",
    "    onnx_model: OutputPath(str),\n",
    "    quantize: str = \"\",\n",
    "):\n",
    "    import subprocess\n",
    "    import shutil\n",
    "    import os\n",
    "\n",
    "    # export.py uses the file name to determine the type of model\n",
    "    # mode is an input path where the name is generated by kubeflow\n",
    "    # We need to control the name that is used...\n",
    "    named_model = f\"/tmp/{os.path.basename(model)}.{model_format}\"\n",
    "    os.symlink(model, named_model)\n",
    "\n",
    "    quantize_param = f\"--{quantize}\" if quantize else \"\"\n",
    "\n",
    "    subprocess.run(\n",
    "        f\"python export.py --img 640 --include=onnx  {quantize_param} \"\n",
    "        f\"--data /dataset/data.yaml --weights {named_model} --device=cpu \",\n",
    "        check=True,\n",
    "        cwd=\"/yolov5\",\n",
    "        shell=True,\n",
    "    )\n",
    "\n",
    "    shutil.copyfile(f\"/tmp/{os.path.basename(model)}.onnx\", onnx_model)\n",
    "\n",
    "\n",
    "convert_model_to_onnx_comp = kfp.components.create_component_from_func(\n",
    "    convert_model_to_onnx, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939ae4ac-d364-49b7-ba74-5b6898b801b8",
   "metadata": {},
   "source": [
    "## Model evaluation component\n",
    "\n",
    "This component can be used for both the original and onnx models. This allows comparisons between the ONNX model (which might be quantized), and the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3693a4de-9443-49a0-9222-466ce0996ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    results: OutputPath(str),\n",
    "    model: InputPath(str),\n",
    "    model_format: str = \"onnx\",  # onnx, pt, tf ....\n",
    "    conf_thres: float = 0.001,\n",
    "    iou_thres: float = 0.6,\n",
    "    max_det: int = 300,\n",
    "):\n",
    "    import subprocess\n",
    "    import os\n",
    "    import torch\n",
    "    from ruamel.yaml import YAML\n",
    "    import pathlib\n",
    "    import shutil\n",
    "\n",
    "    print(f\"The size of the model is {os.path.getsize(model)}\")\n",
    "\n",
    "    if model_format == \"onnx\" and not torch.cuda.is_available():\n",
    "        # the base image is built with an onnxruntime for GPU\n",
    "        # This should work for both CPU and GPU, but val.py\n",
    "        # does it's own checking for CPU onnxruntime only\n",
    "        # Since that's not installed and not on pypl for ppc64le,\n",
    "        # The script won't work unless we change up the version\n",
    "        subprocess.run(\n",
    "            \"mamba install -c rocketce onnxruntime=1.13.1=hea80eff_cpu_py39_pb3.19_1 -y\",\n",
    "            check=True,\n",
    "            shell=True,\n",
    "        )\n",
    "\n",
    "    # valy.py uses the file name to determine the type of model\n",
    "    # mode is an input path where the name is generated by kubeflow\n",
    "    # We need to control the name that is used...\n",
    "    named_model = f\"/tmp/{os.path.basename(model)}.{model_format}\"\n",
    "    os.symlink(model, named_model)\n",
    "\n",
    "    subprocess.run(\n",
    "        f\"python val.py --weights {named_model} --data /dataset/data.yaml --img 640 \"\n",
    "        f\"--conf-thres {conf_thres} --iou-thres {iou_thres} --max-det {max_det} --workers=0 \",\n",
    "        check=True,\n",
    "        shell=True,\n",
    "        cwd=\"/yolov5\",\n",
    "    )\n",
    "\n",
    "    os.makedirs(os.path.dirname(results), exist_ok=True)\n",
    "    shutil.copytree(\"/yolov5/runs/val/exp\", results)\n",
    "    # subprocess.run(\"find . -print\", cwd=\"/yolov5\", shell=True, check=True)\n",
    "\n",
    "\n",
    "evaluate_model_comp = kfp.components.create_component_from_func(\n",
    "    evaluate_model, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d9d1ff-2a55-4ed3-bf8f-4a03eee3a5db",
   "metadata": {},
   "source": [
    "## Write artifact to PVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d76cbbb6-e6ac-4670-9abc-65de242a054c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_artifact_to_path(source: InputPath(str), dest: str(str)):\n",
    "    import os\n",
    "    import shutil\n",
    "\n",
    "    if not os.path.exists(dest):\n",
    "        os.makedirs(dest)\n",
    "\n",
    "    if os.path.isdir(source):\n",
    "        shutil.copytree(source, dest)\n",
    "    else:\n",
    "        shutil.copyfile(source, dest)\n",
    "\n",
    "\n",
    "write_artifact_to_path_comp = kfp.components.create_component_from_func(\n",
    "    write_artifact_to_path, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139dd24b-4266-4672-81e8-715de84a23cd",
   "metadata": {},
   "source": [
    "## Upload the ONNX model\n",
    "\n",
    "The upload component is the same component that is shared with other examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3e184495-f063-4a9d-b488-dd494ae3000a",
   "metadata": {},
   "outputs": [],
   "source": [
    "UPLOAD_MODEL_COMPONENT = (\n",
    "    f\"{COMPONENT_CATALOG_FOLDER}/model-building/upload-model/component.yaml\"\n",
    ")\n",
    "\n",
    "upload_model_comp = kfp.components.load_component_from_file(UPLOAD_MODEL_COMPONENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8784575-c885-48dc-a78f-2805964f08a8",
   "metadata": {},
   "source": [
    "## Deploy Model Component\n",
    "\n",
    "Deploys the model to a nvidia triton service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "343e326d-27bd-4e56-9d86-a136e8030c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPLOY_MODEL_COMPONENT = f\"{os.getenv('HOME')}/kubeflow-ppc64le-examples/deploy_triton_inference_service_component/deploy_triton_inference_service_component.yaml\"\n",
    "deploy_model_comp = kfp.components.load_component_from_file(DEPLOY_MODEL_COMPONENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a50e359-defd-469d-a22b-61bfb1d7ce69",
   "metadata": {},
   "source": [
    "## Pipeline Definition\n",
    "\n",
    "This defines the pipeline, and the pipeline's paramters. We'll compile this pipeline into a YAML file that we can upload, and access through the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e73b5831-97e9-44d2-87b5-d79679df21d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLACKBOARD_RESOURCE_NAME = \"ml-blackboard\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "bded8ffe-e62c-4b52-9d04-2f765ed6fcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"bee-yolov5\")\n",
    "def bee_yolov5(\n",
    "    data_vol_pvc_name: str,\n",
    "    data_vol_subpath: str,\n",
    "    epochs: int = 750,\n",
    "    model_config_url: str = \"https://github.com/ultralytics/yolov5/raw/v7.0/models/yolov5s.yaml\",\n",
    "    initial_weights_url: str = \"https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt\",\n",
    "    quantize_onnx: str = \"int8\",\n",
    "    minio_url=\"minio-service.kubeflow:9000\",\n",
    "    model_version: int = 1,\n",
    "    dataset_size: str = \"4Gi\",\n",
    "    artifact_vol_pvc_name: str = \"\",\n",
    "    artifact_vol_pvc_subpath: str = \"\",\n",
    "):\n",
    "    def mount_volume(task, pvc_name, mount_path, volume_subpath, read_only=False):\n",
    "        task.add_volume(\n",
    "            V1Volume(\n",
    "                name=pvc_name,\n",
    "                persistent_volume_claim=V1PersistentVolumeClaimVolumeSource(pvc_name),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        task.add_volume_mount(\n",
    "            V1VolumeMount(\n",
    "                name=pvc_name,\n",
    "                mount_path=mount_path,\n",
    "                sub_path=volume_subpath,\n",
    "                read_only=read_only,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    create_blackboard = dsl.VolumeOp(\n",
    "        name=\"Create Artefacts Blackboard\",\n",
    "        resource_name=BLACKBOARD_RESOURCE_NAME,\n",
    "        modes=dsl.VOLUME_MODE_RWO,\n",
    "        size=\"4Gi\",\n",
    "        set_owner_reference=True,\n",
    "    )\n",
    "\n",
    "    DATASET_VOLUME_NAME = \"dataset-pvc\"\n",
    "    # Pipeline automatically adds {{workflow.name} prefix when creating\n",
    "    # the resource!\n",
    "    DATASET_VOLUME = \"{{workflow.name}}-\" + DATASET_VOLUME_NAME\n",
    "    create_dataset_volume = dsl.VolumeOp(\n",
    "        name=DATASET_VOLUME_NAME,\n",
    "        resource_name=DATASET_VOLUME_NAME,\n",
    "        modes=dsl.VOLUME_MODE_RWM,\n",
    "        size=dataset_size,\n",
    "        set_owner_reference=True,\n",
    "    )\n",
    "\n",
    "    # Load config  and data Tasks\n",
    "    model_config_task = load_from_url_comp(model_config_url)\n",
    "    model_config_task.after(create_blackboard)\n",
    "\n",
    "    initial_weights_task = load_from_url_comp(initial_weights_url)\n",
    "    initial_weights_task.after(create_blackboard)\n",
    "\n",
    "    copy_dataset_task = copy_data_comp(\"/src-vol\", \"/dest-vol/dataset\")\n",
    "    mount_volume(\n",
    "        copy_dataset_task,\n",
    "        data_vol_pvc_name,\n",
    "        \"/src-vol\",\n",
    "        data_vol_subpath,\n",
    "        read_only=True,\n",
    "    )\n",
    "    mount_volume(copy_dataset_task, DATASET_VOLUME, \"/dest-vol\", \"\")\n",
    "    copy_dataset_task.after(create_dataset_volume)\n",
    "    copy_dataset_task.after(create_blackboard)\n",
    "\n",
    "    # Train Model\n",
    "    train_model_task = train_model_comp(\n",
    "        model_cfg=model_config_task.outputs[\"dest\"],\n",
    "        initial_weights=initial_weights_task.outputs[\"dest\"],\n",
    "        epochs=epochs,\n",
    "    )\n",
    "    train_model_task.set_gpu_limit(1)\n",
    "    train_model_task.set_memory_limit(\"30G\")\n",
    "    mount_volume(train_model_task, DATASET_VOLUME, \"/dataset\", \"/dataset\")\n",
    "    train_model_task.after(copy_dataset_task)\n",
    "\n",
    "    # convert to ONNX\n",
    "    convert_model_to_onnx_task = convert_model_to_onnx_comp(\n",
    "        model=train_model_task.outputs[\"model\"],\n",
    "        model_format=\"pt\",\n",
    "        quantize=quantize_onnx,\n",
    "    )\n",
    "\n",
    "    # Evaluate\n",
    "    evaluate_onnx_model_task = evaluate_model_comp(\n",
    "        model=convert_model_to_onnx_task.outputs[\"onnx_model\"],\n",
    "    )\n",
    "    evaluate_onnx_model_task.set_display_name(\"Evaluate ONNX\")\n",
    "    mount_volume(evaluate_onnx_model_task, DATASET_VOLUME, \"/dataset\", \"/dataset\")\n",
    "\n",
    "    evaluate_pt_model_task = evaluate_model_comp(\n",
    "        model=train_model_task.outputs[\"model\"],\n",
    "        model_format=\"pt\",\n",
    "    )\n",
    "    evaluate_pt_model_task.set_display_name(\"Evaluate with best weights\")\n",
    "    mount_volume(evaluate_pt_model_task, DATASET_VOLUME, \"/dataset\", \"/dataset\")\n",
    "\n",
    "    # Upload ONNX model\n",
    "    upload_model_task = upload_model_comp(\n",
    "        convert_model_to_onnx_task.outputs[\"onnx_model\"],\n",
    "        minio_url=minio_url,\n",
    "        export_bucket=\"{{workflow.namespace}}-bee\",\n",
    "        model_format=\"onnx\",\n",
    "        model_name=\"bee\",\n",
    "        model_version=model_version,\n",
    "    )\n",
    "\n",
    "    # Deploy Inference Service\n",
    "    deploy_model_task = deploy_model_comp(\n",
    "        name=\"bee\",\n",
    "        rm_existing=True,\n",
    "        storage_uri=\"s3://{{workflow.namespace}}-bee/onnx\",\n",
    "        minio_url=minio_url,\n",
    "        predictor_protocol=\"v2\",\n",
    "    )\n",
    "    deploy_model_task.after(upload_model_task)\n",
    "\n",
    "    # write artifacts\n",
    "    if artifact_vol_pvc_name:\n",
    "        write_artifact_to_path_task = write_artifact_to_path_comp(\n",
    "            evaluate_onnx_model_task.outputs[\"results\"],\n",
    "            \"/mnt/onnx-results/{{workflow.name}}\",\n",
    "        )\n",
    "        mount_volume(\n",
    "            write_artifact_to_path_task,\n",
    "            artifact_vol_pvc_name,\n",
    "            \"/mnt\",\n",
    "            artifact_vol_pvc_subpath,\n",
    "        )\n",
    "\n",
    "        write_artifact_to_path_task.after(deploy_model_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee05810-645a-4f59-8499-55e6dbd1a02c",
   "metadata": {},
   "source": [
    "## Compile Pipeline with configuration options\n",
    "\n",
    "These steps compile the pipeline with caching disabled. Because the training data is on a PVC and could change without pipeline awareness, this causes all steps of the pipeline to run, even if the inputs and outputs for a component don't change.\n",
    "\n",
    "This config uses the default (MinIo) method of passing data. The results of the validation steps can be downloaded from the output parameters, if the parameters are passed via MinIo. If using a PVC to pass parameters, we'd need to do something to mount the PVC after the pipline finishes to get at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e610d564-1c10-4b31-a557-248706deb2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See: https://www.kubeflow.org/docs/components/pipelines/overview/caching/#managing-caching-staleness\n",
    "def disable_cache_transformer(op):\n",
    "    if isinstance(op, dsl.ContainerOp):\n",
    "        op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    else:\n",
    "        op.add_pod_annotation(\n",
    "            name=\"pipelines.kubeflow.org/max_cache_staleness\", value=\"P0D\"\n",
    "        )\n",
    "    return op\n",
    "\n",
    "\n",
    "pipeline_conf = PipelineConf()\n",
    "pipeline_conf.add_op_transformer(disable_cache_transformer)\n",
    "\n",
    "pipeline_conf.data_passing_method = data_passing_methods.KubernetesVolume(\n",
    "    volume=V1Volume(\n",
    "        name=BLACKBOARD_RESOURCE_NAME,\n",
    "        persistent_volume_claim=V1PersistentVolumeClaimVolumeSource(\n",
    "            \"{{workflow.name}}-\" + BLACKBOARD_RESOURCE_NAME\n",
    "        ),\n",
    "    ),\n",
    "    path_prefix=f\"{BLACKBOARD_RESOURCE_NAME}/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f2156e60-c033-4e1c-bde1-23d1a85fded9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_NAME = \"Bee detector pipeline\"\n",
    "\n",
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=bee_yolov5,\n",
    "    package_path=f\"{PIPELINE_NAME}.yaml\",\n",
    "    pipeline_conf=pipeline_conf,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881cd7d6-cbfc-4631-a196-7086dd530452",
   "metadata": {},
   "source": [
    "## Upload pipeline programmatically\n",
    "\n",
    "The YAML file generated by the previous compile can be used to create a pipeline through the UI.\n",
    "* Download the file to your workstation\n",
    "* Click \"Pipelines\" from the side bar\n",
    "* Press the \"upload pipeline\" button, and upload the YAML.\n",
    "\n",
    "You can run the pipeline from the UI without looking at the pipeline code. This allows an inexperienced user to run the pipeline with specific parameters, without the compelxity of Kubeflow componet code or K8S Awareness.\n",
    "\n",
    "These next few cells upload and run the pipeline programmatically, so that the example is \"automated\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3bf9f3c5-9a0c-4389-a802-c3a22ff40731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pipeline(pipeline_name: str):\n",
    "    \"\"\"Delete's a pipeline with the specified name\"\"\"\n",
    "\n",
    "    client = kfp.Client()\n",
    "    existing_pipelines = client.list_pipelines(page_size=999).pipelines\n",
    "    matches = (\n",
    "        [ep.id for ep in existing_pipelines if ep.name == pipeline_name]\n",
    "        if existing_pipelines\n",
    "        else []\n",
    "    )\n",
    "    for id in matches:\n",
    "        client.delete_pipeline(id)\n",
    "\n",
    "\n",
    "def get_experiment_id(experiment_name: str) -> str:\n",
    "    \"\"\"Returns the id for the experiment, creating the experiment if needed\"\"\"\n",
    "    client = kfp.Client()\n",
    "    existing_experiments = client.list_experiments(page_size=999).experiments\n",
    "    matches = (\n",
    "        [ex.id for ex in existing_experiments if ex.name == experiment_name]\n",
    "        if existing_experiments\n",
    "        else []\n",
    "    )\n",
    "\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "\n",
    "    exp = client.create_experiment(experiment_name)\n",
    "    return exp.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ddf37a64-9df0-4b00-a4ee-5b28a6c765d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline names need to be unique, so before we upload,\n",
    "# check for and delete any pipeline with the same name\n",
    "delete_pipeline(PIPELINE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3e372f25-244e-4e31-96a3-9021d89b4412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=/pipeline/#/pipelines/details/6e6f82aa-b818-4723-b32e-ba9a75386e79>Pipeline details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# upload the pipeline\n",
    "client = kfp.Client()\n",
    "uploaded_pipeline = client.upload_pipeline(f\"{PIPELINE_NAME}.yaml\", PIPELINE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3864f8d0-83d0-4fbc-ad77-78260c7e86e0",
   "metadata": {},
   "source": [
    "## Run the pipeline with parameters\n",
    "\n",
    "This is equivalent to running the pipeline from the pipelines view in the UI. Since a pipeline run needs to be part of an experiment, this code will create the experiment if it does not exist.\n",
    "\n",
    "When we run this manually, we'll use initial weights that have been trained on a much larger superset of this data, and with many more epochs. That will give us results that can be demoed with a fast training cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b5f4733f-90fc-4dd3-842e-d37e5b9710d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/e8e587d9-dae2-4e0d-b914-943bb422f427\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline_params = {\n",
    "    \"data_vol_pvc_name\": VOLUME_CLAIM_NAME,\n",
    "    \"data_vol_subpath\": \"bee_dataset\",\n",
    "    \"initial_weights_url\": \"https://github.com/ntl-ibm/kubeflow-ppc64le-examples/raw/main/object-detection-yolov5/data/model.pt\",\n",
    "    \"epochs\": \"1\",\n",
    "}\n",
    "\n",
    "run = client.run_pipeline(\n",
    "    experiment_id=get_experiment_id(\"bee-exp\"),\n",
    "    job_name=\"bees\",\n",
    "    pipeline_id=uploaded_pipeline.id,\n",
    "    params=pipeline_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04249b8-0f60-44aa-a1b1-75749acee2ea",
   "metadata": {},
   "source": [
    "## Waits for the pipeline to complete \n",
    "\n",
    "* waits for up to 20 Min\n",
    "* Checks the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a774585c-f882-44ab-afc2-7e885fece3b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'Succeeded', 'error': None, 'time': '0:06:37'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TWENTY_MIN = 20 * 60\n",
    "result = client.wait_for_run_completion(run.id, timeout=TWENTY_MIN)\n",
    "{\n",
    "    \"status\": result.run.status,\n",
    "    \"error\": result.run.error,\n",
    "    \"time\": str(result.run.finished_at - result.run.created_at),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f2e097-3f09-4def-ac34-a0c56cd3401c",
   "metadata": {},
   "source": [
    "## Inference Example\n",
    "\n",
    "Since the pipeline deploys an inference service, you will see the inference service in the 'Models' list. \n",
    "\n",
    "The URL for the service can used for the weights. The detect tool knows how to talk to NVIDIA triton infrence servers, and the model has already been deployed to this service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7fadb0d2-c85e-4c86-bce5-21509765aa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE = f\"{BEE_DATA_SET_PATH}/test/images/Bee_Health_20170825312396--1-_jpg.rf.0831c785ed759222d5766ce5ebeeb8a2.jpg\"\n",
    "# image = mpimg.imread(IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aec10bc8-d2b6-4864-8c4d-cd9022946436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['http://bee.kubeflow-ntl.svc.cluster.local/v2/models/bee/infer'], source=/home/jovyan/vol-1/bee_dataset/test/images/Bee_Health_20170825312396--1-_jpg.rf.0831c785ed759222d5766ce5ebeeb8a2.jpg, data=/home/jovyan/vol-1/bee_data/data.yaml, imgsz=[640, 640], conf_thres=0.7, iou_thres=0.2, max_det=500, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=../../vol-1/yolov5/runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
      "YOLOv5 ðŸš€ v7.0-0-g915bbf2 Python-3.9.13 torch-1.12.1 CPU\n",
      "\n",
      "Using http://bee.kubeflow-ntl.svc.cluster.local/v2/models/bee/infer as Triton Inference Server...\n",
      "image 1/1 /home/jovyan/vol-1/bee_dataset/test/images/Bee_Health_20170825312396--1-_jpg.rf.0831c785ed759222d5766ce5ebeeb8a2.jpg: 640x640 (no detections), 15130.4ms\n",
      "Speed: 165.5ms pre-process, 15130.4ms inference, 0.8ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1m../../vol-1/yolov5/runs/detect/exp28\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python /home/jovyan/vol-1/yolov5/detect.py --weights=http://bee.kubeflow-ntl.svc.cluster.local/v2/models/bee/infer --data=/home/jovyan/vol-1/bee_data/data.yaml --source=$IMAGE --conf-thres=.7 --iou-thres=.2 --max-det=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3318ac4-ec13-4c9a-9c9c-02c81b911611",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
