{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba72d66b-7e23-44c5-8fa3-e93d23f2088c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lab_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext lab_black\n"
     ]
    }
   ],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694bbf2f-551d-435a-90c9-e810c64971a3",
   "metadata": {},
   "source": [
    "# Honey Bee Computer Vision Example\n",
    "\n",
    "This example shows how to use python scripts from open-source repos to train a yolov5 model, and evaluate the results.\n",
    "\n",
    "The example trains a model to detect Honey Bees using the V7 version of the https://github.com/ultralytics/yolov5. The purpose of the example is to explain how to train a model using Kubeflow, the YOLOV5 repo and sample data. \n",
    "\n",
    "Accessed classes are:\n",
    "* Bees (workers or foragers)\n",
    "* Bees carrying pollen\n",
    "* Drones\n",
    "* Queens\n",
    "\n",
    "\n",
    "Datasets for training of object detection models are usually large, and take a long time to train. They often contain many pre-augmented images. For our purposes, we've included a sample of training data to use for this exercise. The poor quantity and quality of the training data will not produce a good model, but it will train quickly, and allow for experimentation with the pipeline.\n",
    "\n",
    "For fun, we also included the weights from a model using a much larger version of this dataset with 500+ epochs (This takes many hours to train).\n",
    "\n",
    "The dataset was sampled from here: https://universe.roboflow.com/matt-nudi/honey-bee-detection-model-zgjnb (creative commons license, but requires an account/email to download).\n",
    "\n",
    "## Author\n",
    "\n",
    "Nick Lawrence ntl@us.ibm.com\n",
    "\n",
    "## License\n",
    "Apache-2.0 License\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4d10f2-c775-4816-a57e-5297c1c23d8a",
   "metadata": {},
   "source": [
    "The assumption is that this notebook has a data volume mounted (you can set this up when you create the notebook). The PVC should have been created with an access mode of ReadWriteMany. \n",
    "This allows other pods to mount the volume.\n",
    "\n",
    "Also assumed is that the notebook is running with a base image that has the yolov5 packages installed. You can see how the image was created by looking at the file in kubeflow-ppc64le-examples/object-detection-yolov5/Notebook Container Image Source/DockerFile.\n",
    "\n",
    "The data set will be extracted to the data volume in these next few cells. The data will be loaded into the pipeline via a volume, rather than a download. This simulates a use case where the training data has been pre loaded on a volume in the kubernetes cluster, and is large enough where a download is expensive.\n",
    "\n",
    "The volume name is defined in this next cell, as is the path to the extracted Roboflow data set for the bees. To keep things simple, the mount point is the same for both the notebook server, and also for the containers that mount the volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b76914e-c3d1-496e-8a52-764eab5cd409",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOLUME_CLAIM_NAME = \"yolov5-work\"\n",
    "MOUNT_POINT = \"/home/jovyan/vol-1\"\n",
    "BEE_DATA_SET_SUBPATH = \"bee_dataset\"\n",
    "BEE_DATA_SET_PATH = f\"{MOUNT_POINT}/{BEE_DATA_SET_SUBPATH}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4f8c147-ef4d-4cad-9613-f52395b14fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {BEE_DATA_SET_PATH}\n",
    "!tar -xf data/dataset.tar.gz -C {BEE_DATA_SET_PATH} --strip-components 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e41eb3a-7842-401b-ab07-79c2816c79b3",
   "metadata": {},
   "source": [
    "## Copy YOLOV5 files to (notebook) persistent storage\n",
    "The base image has cloned the yolov5 repo into the root directory tree. (`/yolov5`). \n",
    "\n",
    "When the yolov5 scripts are executed from within the notebook, they will produce output in the same directory tree.\n",
    "\n",
    "The root directory is part of the containers R/W layer. That means that changes there are _NOT_ persisted. The root directory is also not accessible from the navigation bar of our notebook server.\n",
    "\n",
    "We'll copy the yolov5 files into our home directory, and that way we can run commands interactively and examine results with the file browser. Because the home directory has been mounted to a volume, the changes there will not be lost if the notebook server is shutdown.\n",
    "\n",
    "This copy is for interactive work in the notebook, the pipeline does not use this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13e8229a-1d48-4ad6-ac60-1e74571755af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory is already copied\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "if [ ! -d /home/jovyan/yolov5 ] \n",
    "then \n",
    "   cp -R /yolov5 /home/jovyan/yolov5 \n",
    "else \n",
    "   echo \"The directory is already copied\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf03b6e-d8cd-4900-896d-353f332a27c7",
   "metadata": {},
   "source": [
    "## Imports and constants\n",
    "\n",
    "If you've build your own yolov5 container, you should change the BASE_IMAGE variable to your own image here. The Dockerfile for the image is included in the \"Notebook Container Image Source\" directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b555cbe3-c15a-4361-aa42-153fa814e9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.components import InputPath, OutputPath\n",
    "from kubernetes.client.models import (\n",
    "    V1Volume,\n",
    "    V1VolumeMount,\n",
    "    V1PersistentVolumeClaimVolumeSource,\n",
    ")\n",
    "from kfp.dsl import PipelineConf, data_passing_methods\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "BASE_IMAGE = \"quay.io/ntlawrence/yolov5:pt1.12.1-yolo7.0-v1.1\"\n",
    "COMPONENT_CATALOG_FOLDER = f\"{os.getenv('HOME')}/components\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f6c0e3-98ed-462c-8746-3adc4ecdf930",
   "metadata": {},
   "source": [
    "## Load Data component\n",
    "The first component in the pipeline copies the data from an input path to an output parameter.\n",
    "\n",
    "Essentially this moves the data from the volume into the pipeline. A common problem when using PVCs is that paths to data in a PVC (strings), are not interchangable with pipeline inputs and outputs (InputPath and OutputPath). Components like this are needed to transform data on a PVC into something that can be used by existing pipeline components that expect InputPath and OutputPath params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f800195c-1c9c-4a92-8106-d884a0975288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_url(\n",
    "    source: str,\n",
    "    dest: OutputPath(str),\n",
    "):\n",
    "    import os\n",
    "    import shutil\n",
    "    from urllib.request import urlretrieve\n",
    "    from urllib.parse import urlparse\n",
    "\n",
    "    # Make target directories if needed\n",
    "    parent_dirs = os.path.dirname(dest)\n",
    "    if not os.path.exists(parent_dirs):\n",
    "        os.makedirs(parent_dirs)\n",
    "\n",
    "    # Option to use an empty file to indicate no weights\n",
    "    if not source:\n",
    "        with open(dest, \"w\") as _:\n",
    "            pass\n",
    "\n",
    "    source_details = urlparse(source)\n",
    "\n",
    "    if source_details.scheme == \"file\":\n",
    "        if os.path.isdir(source_details.path):\n",
    "            shutil.copytree(source_details.path, dest)\n",
    "        else:\n",
    "            shutil.copyfile(source_details.path, dest)\n",
    "    elif source_details.scheme in (\"http\", \"https\", \"ftp\", \"ftps\"):\n",
    "        urlretrieve(source, filename=dest)\n",
    "    else:\n",
    "        raise ValueError(f\"source does not use a supported url scheme\")\n",
    "\n",
    "\n",
    "load_from_url_comp = kfp.components.create_component_from_func(\n",
    "    load_from_url, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dbcc2e-66d8-466e-8535-a29d25c1e30c",
   "metadata": {},
   "source": [
    "## Copy data from one path to another\n",
    "This component is a helper to move data from one volume path to another.\n",
    "\n",
    "This component allows us to copy pipeline artifacts to a mounted PVC for use outside of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd9fcda6-5f27-40dd-86da-f964b87852a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_data(source: str, dest: str):\n",
    "    import os\n",
    "    import shutil\n",
    "\n",
    "    # Make target directories if needed\n",
    "    parent_dirs = os.path.basename(dest)\n",
    "    if not os.path.exists(parent_dirs):\n",
    "        os.makedirs(parent_dirs)\n",
    "\n",
    "    if os.path.isdir(source):\n",
    "        shutil.copytree(source, dest)\n",
    "    else:\n",
    "        shutil.copyfile(source, dest)\n",
    "\n",
    "\n",
    "copy_data_comp = kfp.components.create_component_from_func(\n",
    "    copy_data, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3ec2ab-9d32-44af-b126-57afbe8e9c39",
   "metadata": {},
   "source": [
    "## Train Model component\n",
    "\n",
    "Run the python train.py CLI to train the model\n",
    "\n",
    "Performance could be improved by using distributed training. Distributed training with the TorchJob operator may be investigated and included in the example in the future.\n",
    "\n",
    "Integration with tensorboard is also an area that might be investigated and included in the future.\n",
    "\n",
    "The trained model and results.csv are reported as output parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96e35b5a-ad5d-4510-9574-c41c910382f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: OutputPath(str),\n",
    "    results: OutputPath(str),\n",
    "    model_cfg: InputPath(str),\n",
    "    initial_weights: InputPath(str),\n",
    "    epochs: int,\n",
    "):\n",
    "    import subprocess\n",
    "    import pathlib\n",
    "    from ruamel.yaml import YAML\n",
    "    import os\n",
    "    import shutil\n",
    "\n",
    "    # Option to pass an empty file to train from scratch\n",
    "    weights = initial_weights if os.path.getsize(initial_weights) > 0 else \"\"\n",
    "\n",
    "    subprocess.run(\"find /dataset -print\", shell=True)\n",
    "    subprocess.run(\n",
    "        f\"python train.py --img 640 --batch -1 --noplots --epochs {epochs} --cache /home/jovyan/cache --cfg={model_cfg} \"\n",
    "        f\"--data /dataset/data.yaml --weights {weights} --workers=0 --device=0 --optimizer=Adam\",\n",
    "        check=True,\n",
    "        cwd=\"/yolov5\",\n",
    "        shell=True,\n",
    "    )\n",
    "\n",
    "    os.makedirs(os.path.dirname(model), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(results), exist_ok=True)\n",
    "\n",
    "    # Copy trained model and results to output parameters\n",
    "    shutil.copyfile(\"/yolov5/runs/train/exp/weights/best.pt\", model)\n",
    "    shutil.copyfile(\"/yolov5/runs/train/exp/results.csv\", results)\n",
    "\n",
    "\n",
    "train_model_comp = kfp.components.create_component_from_func(\n",
    "    train_model, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfff5ee1-b5ad-402b-a70d-9ec970fc59e0",
   "metadata": {},
   "source": [
    "## Component to convert the model to ONNX\n",
    "\n",
    "The export tool supports quantizing, the model, and so we've added that option as a parameter to the component. The ability to quantize is important for models that will be used for inferencing on edge devices. Many edge devices do not have the resources to evaluate deep neural networks, and and smaller model makes it possible to run inferencing in those environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "87865918-1782-4392-9c6a-e14dd9c833b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_model_to_onnx(\n",
    "    model: InputPath(str),\n",
    "    model_format: str,\n",
    "    onnx_model: OutputPath(str),\n",
    "    quantize: str = \"\",\n",
    "):\n",
    "    import subprocess\n",
    "    import shutil\n",
    "    import os\n",
    "\n",
    "    # export.py uses the file name to determine the type of model\n",
    "    # mode is an input path where the name is generated by kubeflow\n",
    "    # We need to control the name that is used...\n",
    "    named_model = f\"/tmp/{os.path.basename(model)}.{model_format}\"\n",
    "    os.symlink(model, named_model)\n",
    "\n",
    "    quantize_param = f\"--{quantize}\" if quantize else \"\"\n",
    "\n",
    "    subprocess.run(\n",
    "        f\"python export.py --img 640 --include=onnx  {quantize_param} \"\n",
    "        f\"--data /dataset/data.yaml --weights {named_model} --device=cpu \",\n",
    "        check=True,\n",
    "        cwd=\"/yolov5\",\n",
    "        shell=True,\n",
    "    )\n",
    "\n",
    "    shutil.copyfile(f\"/tmp/{os.path.basename(model)}.onnx\", onnx_model)\n",
    "\n",
    "\n",
    "convert_model_to_onnx_comp = kfp.components.create_component_from_func(\n",
    "    convert_model_to_onnx, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939ae4ac-d364-49b7-ba74-5b6898b801b8",
   "metadata": {},
   "source": [
    "## Model evaluation component\n",
    "\n",
    "This component can be used for both the original and onnx models. This allows comparisons between the ONNX model (which might be quantized), and the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3693a4de-9443-49a0-9222-466ce0996ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    results: OutputPath(str),\n",
    "    model: InputPath(str),\n",
    "    model_format: str = \"onnx\",  # onnx, pt, tf ....\n",
    "    conf_thres: float = 0.001,\n",
    "    iou_thres: float = 0.6,\n",
    "    max_det: int = 300,\n",
    "):\n",
    "    import subprocess\n",
    "    import os\n",
    "    import torch\n",
    "    from ruamel.yaml import YAML\n",
    "    import pathlib\n",
    "    import shutil\n",
    "\n",
    "    print(f\"The size of the model is {os.path.getsize(model)}\")\n",
    "\n",
    "    if model_format == \"onnx\" and not torch.cuda.is_available():\n",
    "        # the base image is built with an onnxruntime for GPU\n",
    "        # This should work for both CPU and GPU, but val.py\n",
    "        # does it's own checking for CPU onnxruntime only\n",
    "        # Since that's not installed and not on pypl for ppc64le,\n",
    "        # The script won't work unless we change up the version\n",
    "        subprocess.run(\n",
    "            \"mamba install -c rocketce onnxruntime=1.13.1=hea80eff_cpu_py39_pb3.19_1 -y\",\n",
    "            check=True,\n",
    "            shell=True,\n",
    "        )\n",
    "\n",
    "    # valy.py uses the file name to determine the type of model\n",
    "    # mode is an input path where the name is generated by kubeflow\n",
    "    # We need to control the name that is used...\n",
    "    named_model = f\"/tmp/{os.path.basename(model)}.{model_format}\"\n",
    "    os.symlink(model, named_model)\n",
    "\n",
    "    subprocess.run(\n",
    "        f\"python val.py --weights {named_model} --data /dataset/data.yaml --img 640 \"\n",
    "        f\"--conf-thres {conf_thres} --iou-thres {iou_thres} --max-det {max_det} --workers=0 \",\n",
    "        check=True,\n",
    "        shell=True,\n",
    "        cwd=\"/yolov5\",\n",
    "    )\n",
    "\n",
    "    os.makedirs(os.path.dirname(results), exist_ok=True)\n",
    "    shutil.copytree(\"/yolov5/runs/val/exp\", results)\n",
    "\n",
    "\n",
    "evaluate_model_comp = kfp.components.create_component_from_func(\n",
    "    evaluate_model, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d9d1ff-2a55-4ed3-bf8f-4a03eee3a5db",
   "metadata": {},
   "source": [
    "## Write artifact to PVC\n",
    "\n",
    "Converts data from a pipeline parameter to a file stored on a PVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d76cbbb6-e6ac-4670-9abc-65de242a054c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_artifact_to_path(source: InputPath(str), dest: str(str)):\n",
    "    import os\n",
    "    import shutil\n",
    "\n",
    "    parent_dirs = os.path.dirname(dest)\n",
    "    os.makedirs(parent_dirs, exist_ok=True)\n",
    "\n",
    "    if os.path.isdir(source):\n",
    "        shutil.copytree(source, dest)\n",
    "    else:\n",
    "        shutil.copyfile(source, dest)\n",
    "\n",
    "\n",
    "write_artifact_to_path_comp = kfp.components.create_component_from_func(\n",
    "    write_artifact_to_path, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139dd24b-4266-4672-81e8-715de84a23cd",
   "metadata": {},
   "source": [
    "## Upload the ONNX model\n",
    "\n",
    "The upload component is the same component that is shared with other examples. It loads the ONNX model into MinIO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3e184495-f063-4a9d-b488-dd494ae3000a",
   "metadata": {},
   "outputs": [],
   "source": [
    "UPLOAD_MODEL_COMPONENT = (\n",
    "    f\"{COMPONENT_CATALOG_FOLDER}/model-building/upload-model/component.yaml\"\n",
    ")\n",
    "\n",
    "upload_model_comp = kfp.components.load_component_from_file(UPLOAD_MODEL_COMPONENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8784575-c885-48dc-a78f-2805964f08a8",
   "metadata": {},
   "source": [
    "## Deploy Model Component\n",
    "\n",
    "Deploys the model to a NVIDIA Triton inference service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "343e326d-27bd-4e56-9d86-a136e8030c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPLOY_MODEL_COMPONENT = f\"{os.getenv('HOME')}/kubeflow-ppc64le-examples/deploy_triton_inference_service_component/deploy_triton_inference_service_component.yaml\"\n",
    "deploy_model_comp = kfp.components.load_component_from_file(DEPLOY_MODEL_COMPONENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a50e359-defd-469d-a22b-61bfb1d7ce69",
   "metadata": {},
   "source": [
    "## Pipeline Definition\n",
    "\n",
    "This defines the pipeline, and the pipeline's paramters. We'll compile this pipeline into a YAML file that we can upload, and access through the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e73b5831-97e9-44d2-87b5-d79679df21d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLACKBOARD_RESOURCE_NAME = \"ml-blackboard\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bded8ffe-e62c-4b52-9d04-2f765ed6fcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"bee-yolov5\")\n",
    "def bee_yolov5(\n",
    "    data_vol_pvc_name: str,\n",
    "    data_vol_subpath: str,\n",
    "    epochs: int = 750,\n",
    "    model_config_url: str = \"https://github.com/ultralytics/yolov5/raw/v7.0/models/yolov5s.yaml\",\n",
    "    initial_weights_url: str = \"https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt\",\n",
    "    quantize_onnx: str = \"int8\",\n",
    "    minio_url=\"minio-service.kubeflow:9000\",\n",
    "    model_version: int = 1,\n",
    "    dataset_size: str = \"4Gi\",\n",
    "    artifact_vol_pvc_name: str = \"\",\n",
    "    artifact_vol_subpath: str = \"\",\n",
    "):\n",
    "    def mount_volume(task, pvc_name, mount_path, volume_subpath, read_only=False):\n",
    "        task.add_volume(\n",
    "            V1Volume(\n",
    "                name=pvc_name,\n",
    "                persistent_volume_claim=V1PersistentVolumeClaimVolumeSource(pvc_name),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        task.add_volume_mount(\n",
    "            V1VolumeMount(\n",
    "                name=pvc_name,\n",
    "                mount_path=mount_path,\n",
    "                sub_path=volume_subpath,\n",
    "                read_only=read_only,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    create_blackboard = dsl.VolumeOp(\n",
    "        name=\"Create Artefacts Blackboard\",\n",
    "        resource_name=BLACKBOARD_RESOURCE_NAME,\n",
    "        modes=dsl.VOLUME_MODE_RWO,\n",
    "        size=\"4Gi\",\n",
    "        set_owner_reference=True,\n",
    "    )\n",
    "\n",
    "    DATASET_VOLUME_NAME = \"dataset-pvc\"\n",
    "    # Pipeline automatically adds {{workflow.name} prefix when creating\n",
    "    # the resource!\n",
    "    DATASET_VOLUME = \"{{workflow.name}}-\" + DATASET_VOLUME_NAME\n",
    "    create_dataset_volume = dsl.VolumeOp(\n",
    "        name=f\"Create PVC for dataset\",\n",
    "        resource_name=DATASET_VOLUME_NAME,\n",
    "        modes=dsl.VOLUME_MODE_RWM,\n",
    "        size=dataset_size,\n",
    "        set_owner_reference=True,\n",
    "    )\n",
    "\n",
    "    # Load config  and data Tasks\n",
    "    model_config_task = load_from_url_comp(model_config_url)\n",
    "    model_config_task.after(create_blackboard)\n",
    "    model_config_task.set_display_name(\"Load model Config\")\n",
    "\n",
    "    initial_weights_task = load_from_url_comp(initial_weights_url)\n",
    "    initial_weights_task.after(create_blackboard)\n",
    "    initial_weights_task.set_display_name(\"Load initial weights\")\n",
    "\n",
    "    copy_dataset_task = copy_data_comp(\"/src-vol\", \"/dest-vol/dataset\")\n",
    "    mount_volume(\n",
    "        copy_dataset_task,\n",
    "        data_vol_pvc_name,\n",
    "        \"/src-vol\",\n",
    "        data_vol_subpath,\n",
    "        read_only=True,\n",
    "    )\n",
    "    mount_volume(copy_dataset_task, DATASET_VOLUME, \"/dest-vol\", \"\")\n",
    "    copy_dataset_task.after(create_dataset_volume)\n",
    "    copy_dataset_task.after(create_blackboard)\n",
    "    copy_dataset_task.set_display_name(\"Copy dataset to Pipeline owed PVC\")\n",
    "\n",
    "    # Train Model\n",
    "    train_model_task = train_model_comp(\n",
    "        model_cfg=model_config_task.outputs[\"dest\"],\n",
    "        initial_weights=initial_weights_task.outputs[\"dest\"],\n",
    "        epochs=epochs,\n",
    "    )\n",
    "    train_model_task.set_gpu_limit(1)\n",
    "    train_model_task.set_memory_limit(\"30G\")\n",
    "    mount_volume(train_model_task, DATASET_VOLUME, \"/dataset\", \"dataset\")\n",
    "    train_model_task.after(copy_dataset_task)\n",
    "\n",
    "    # convert to ONNX\n",
    "    convert_model_to_onnx_task = convert_model_to_onnx_comp(\n",
    "        model=train_model_task.outputs[\"model\"],\n",
    "        model_format=\"pt\",\n",
    "        quantize=quantize_onnx,\n",
    "    )\n",
    "\n",
    "    # Evaluate\n",
    "    evaluate_onnx_model_task = evaluate_model_comp(\n",
    "        model=convert_model_to_onnx_task.outputs[\"onnx_model\"],\n",
    "    )\n",
    "    evaluate_onnx_model_task.set_display_name(\"Evaluate ONNX Model\")\n",
    "    mount_volume(evaluate_onnx_model_task, DATASET_VOLUME, \"/dataset\", \"dataset\")\n",
    "\n",
    "    evaluate_pt_model_task = evaluate_model_comp(\n",
    "        model=train_model_task.outputs[\"model\"],\n",
    "        model_format=\"pt\",\n",
    "    )\n",
    "    evaluate_pt_model_task.set_display_name(\"Evaluate Torch Model\")\n",
    "    mount_volume(evaluate_pt_model_task, DATASET_VOLUME, \"/dataset\", \"dataset\")\n",
    "\n",
    "    # Upload ONNX model\n",
    "    upload_model_task = upload_model_comp(\n",
    "        convert_model_to_onnx_task.outputs[\"onnx_model\"],\n",
    "        minio_url=minio_url,\n",
    "        export_bucket=\"{{workflow.namespace}}-bee\",\n",
    "        model_format=\"onnx\",\n",
    "        model_name=\"bee\",\n",
    "        model_version=model_version,\n",
    "    )\n",
    "\n",
    "    # Deploy Inference Service\n",
    "    deploy_model_task = deploy_model_comp(\n",
    "        name=\"bee\",\n",
    "        rm_existing=True,\n",
    "        storage_uri=\"s3://{{workflow.namespace}}-bee/onnx\",\n",
    "        minio_url=minio_url,\n",
    "        predictor_protocol=\"v2\",\n",
    "    )\n",
    "    deploy_model_task.after(upload_model_task)\n",
    "\n",
    "    ##### Write evaluation results\n",
    "    with dsl.Condition(artifact_vol_pvc_name != \"\", name=\"store_onnx_metrics\"):\n",
    "        ## Save ONNX Metrics\n",
    "        write_onnx_artifact_to_path_task = write_artifact_to_path_comp(\n",
    "            evaluate_onnx_model_task.outputs[\"results\"],\n",
    "            \"/mnt/{{workflow.name}}/onnx-metrics\",\n",
    "        )\n",
    "        mount_volume(\n",
    "            write_onnx_artifact_to_path_task,\n",
    "            artifact_vol_pvc_name,\n",
    "            \"/mnt\",\n",
    "            artifact_vol_subpath,\n",
    "        )\n",
    "        write_onnx_artifact_to_path_task.set_display_name(\"Save ONNX metrics (PVC)\")\n",
    "\n",
    "        ## Save Torch Metrics\n",
    "        write_torch_artifact_to_path_task = write_artifact_to_path_comp(\n",
    "            evaluate_pt_model_task.outputs[\"results\"],\n",
    "            \"/mnt/{{workflow.name}}/pt-metrics\",\n",
    "        )\n",
    "        mount_volume(\n",
    "            write_torch_artifact_to_path_task,\n",
    "            artifact_vol_pvc_name,\n",
    "            \"/mnt\",\n",
    "            artifact_vol_subpath,\n",
    "        )\n",
    "        write_torch_artifact_to_path_task.set_display_name(\"Save Torch results (PVC)\")\n",
    "\n",
    "        ## Save Torch Model\n",
    "        write_torch_model_artifact_to_path_task = write_artifact_to_path_comp(\n",
    "            train_model_task.outputs[\"model\"],\n",
    "            \"/mnt/{{workflow.name}}/model.pt\",\n",
    "        )\n",
    "        mount_volume(\n",
    "            write_torch_model_artifact_to_path_task,\n",
    "            artifact_vol_pvc_name,\n",
    "            \"/mnt\",\n",
    "            artifact_vol_subpath,\n",
    "        )\n",
    "        write_torch_model_artifact_to_path_task.set_display_name(\n",
    "            \"Save Torch model (PVC)\"\n",
    "        )\n",
    "\n",
    "        ## Save ONNX Model\n",
    "        write_onnx_model_artifact_to_path_task = write_artifact_to_path_comp(\n",
    "            convert_model_to_onnx_task.outputs[\"onnx_model\"],\n",
    "            \"/mnt/{{workflow.name}}/model.onnx\",\n",
    "        )\n",
    "        mount_volume(\n",
    "            write_onnx_model_artifact_to_path_task,\n",
    "            artifact_vol_pvc_name,\n",
    "            \"/mnt\",\n",
    "            artifact_vol_subpath,\n",
    "        )\n",
    "        write_onnx_model_artifact_to_path_task.set_display_name(\"Save ONNX model (PVC)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee05810-645a-4f59-8499-55e6dbd1a02c",
   "metadata": {},
   "source": [
    "## Compile Pipeline with configuration options\n",
    "\n",
    "Uses a PVC for passing data between components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e610d564-1c10-4b31-a557-248706deb2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See: https://www.kubeflow.org/docs/components/pipelines/overview/caching/#managing-caching-staleness\n",
    "def disable_cache_transformer(op):\n",
    "    if isinstance(op, dsl.ContainerOp):\n",
    "        op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    else:\n",
    "        op.add_pod_annotation(\n",
    "            name=\"pipelines.kubeflow.org/max_cache_staleness\", value=\"P0D\"\n",
    "        )\n",
    "    return op\n",
    "\n",
    "\n",
    "pipeline_conf = PipelineConf()\n",
    "pipeline_conf.add_op_transformer(disable_cache_transformer)\n",
    "\n",
    "pipeline_conf.data_passing_method = data_passing_methods.KubernetesVolume(\n",
    "    volume=V1Volume(\n",
    "        name=BLACKBOARD_RESOURCE_NAME,\n",
    "        persistent_volume_claim=V1PersistentVolumeClaimVolumeSource(\n",
    "            \"{{workflow.name}}-\" + BLACKBOARD_RESOURCE_NAME\n",
    "        ),\n",
    "    ),\n",
    "    path_prefix=f\"{BLACKBOARD_RESOURCE_NAME}/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f2156e60-c033-4e1c-bde1-23d1a85fded9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_NAME = \"Bee detector pipeline\"\n",
    "\n",
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=bee_yolov5,\n",
    "    package_path=f\"{PIPELINE_NAME}.yaml\",\n",
    "    pipeline_conf=pipeline_conf,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881cd7d6-cbfc-4631-a196-7086dd530452",
   "metadata": {},
   "source": [
    "## Upload pipeline programmatically\n",
    "\n",
    "The YAML file generated by the previous compile can be used to create a pipeline through the UI.\n",
    "* Download the file to your workstation\n",
    "* Click \"Pipelines\" from the side bar\n",
    "* Press the \"upload pipeline\" button, and upload the YAML.\n",
    "\n",
    "After adding the pipline, you can run the pipeline from the UI without looking at the pipeline code. This allows an inexperienced user to run the pipeline with specific parameters, without the compelxity of Kubeflow componet code or K8S Awareness.\n",
    "\n",
    "These next few cells upload and run the pipeline programmatically, so that the example is \"automated\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3bf9f3c5-9a0c-4389-a802-c3a22ff40731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pipeline(pipeline_name: str):\n",
    "    \"\"\"Delete's a pipeline with the specified name\"\"\"\n",
    "\n",
    "    client = kfp.Client()\n",
    "    existing_pipelines = client.list_pipelines(page_size=999).pipelines\n",
    "    matches = (\n",
    "        [ep.id for ep in existing_pipelines if ep.name == pipeline_name]\n",
    "        if existing_pipelines\n",
    "        else []\n",
    "    )\n",
    "    for id in matches:\n",
    "        client.delete_pipeline(id)\n",
    "\n",
    "\n",
    "def get_experiment_id(experiment_name: str) -> str:\n",
    "    \"\"\"Returns the id for the experiment, creating the experiment if needed\"\"\"\n",
    "    client = kfp.Client()\n",
    "    existing_experiments = client.list_experiments(page_size=999).experiments\n",
    "    matches = (\n",
    "        [ex.id for ex in existing_experiments if ex.name == experiment_name]\n",
    "        if existing_experiments\n",
    "        else []\n",
    "    )\n",
    "\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "\n",
    "    exp = client.create_experiment(experiment_name)\n",
    "    return exp.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ddf37a64-9df0-4b00-a4ee-5b28a6c765d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline names need to be unique, so before we upload,\n",
    "# check for and delete any pipeline with the same name\n",
    "delete_pipeline(PIPELINE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3e372f25-244e-4e31-96a3-9021d89b4412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=/pipeline/#/pipelines/details/50b0e5e3-c2ea-46b9-9e0e-df73076fa535>Pipeline details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# upload the pipeline\n",
    "client = kfp.Client()\n",
    "uploaded_pipeline = client.upload_pipeline(f\"{PIPELINE_NAME}.yaml\", PIPELINE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3864f8d0-83d0-4fbc-ad77-78260c7e86e0",
   "metadata": {},
   "source": [
    "## Run the pipeline with parameters\n",
    "\n",
    "This is equivalent to running the pipeline from the pipelines view in the UI. Since a pipeline run needs to be part of an experiment, this code will create the experiment if it does not exist.\n",
    "\n",
    "When we run this manually, we'll use initial weights that have been trained on a much larger superset of this data, and with many more epochs. That will give us results that can be demoed with a fast training cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b5f4733f-90fc-4dd3-842e-d37e5b9710d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/573386ee-81d0-49ee-997b-4e9dbbc5dec7\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline_params = {\n",
    "    \"data_vol_pvc_name\": VOLUME_CLAIM_NAME,\n",
    "    \"data_vol_subpath\": BEE_DATA_SET_SUBPATH,\n",
    "    \"epochs\": \"15\",\n",
    "    \"artifact_vol_pvc_name\": VOLUME_CLAIM_NAME,\n",
    "    \"artifact_vol_subpath\": \"runs\",\n",
    "}\n",
    "\n",
    "run = client.run_pipeline(\n",
    "    experiment_id=get_experiment_id(\"bee-exp\"),\n",
    "    job_name=\"bees\",\n",
    "    pipeline_id=uploaded_pipeline.id,\n",
    "    params=pipeline_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04249b8-0f60-44aa-a1b1-75749acee2ea",
   "metadata": {},
   "source": [
    "## Waits for the pipeline to complete \n",
    "\n",
    "* waits for up to 20 Min\n",
    "* Checks the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a774585c-f882-44ab-afc2-7e885fece3b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'Succeeded', 'error': None, 'time': '0:05:51'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TWENTY_MIN = 20 * 60\n",
    "result = client.wait_for_run_completion(run.id, timeout=TWENTY_MIN)\n",
    "{\n",
    "    \"status\": result.run.status,\n",
    "    \"error\": result.run.error,\n",
    "    \"time\": str(result.run.finished_at - result.run.created_at),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f2e097-3f09-4def-ac34-a0c56cd3401c",
   "metadata": {},
   "source": [
    "## Inference Example\n",
    "\n",
    "Since the pipeline deploys an inference service, you will see the inference service in the 'Models' list. \n",
    "\n",
    "The URL for the service can used for the weights. The detect tool knows how to talk to NVIDIA triton infrence servers, and the model has already been deployed to this service by the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7fadb0d2-c85e-4c86-bce5-21509765aa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE = f\"{BEE_DATA_SET_PATH}/test/images/2016-03-15-04-09-38-1024x673_jpg.rf.aa352f3bd2a105dd7ff560e0ab42ae69.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aec10bc8-d2b6-4864-8c4d-cd9022946436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['http://bee.kubeflow-ntl.svc.cluster.local/v2/models/bee/infer'], source=/home/jovyan/vol-1/bee_dataset/test/images/2016-03-15-04-09-38-1024x673_jpg.rf.aa352f3bd2a105dd7ff560e0ab42ae69.jpg, data=/home/jovyan/vol-1/bee_dataset/data.yaml, imgsz=[640, 640], conf_thres=0.7, iou_thres=0.2, max_det=500, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=../../yolov5/runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
      "YOLOv5 ğŸš€ v7.0-0-g915bbf2 Python-3.9.13 torch-1.12.1 CPU\n",
      "\n",
      "Using http://bee.kubeflow-ntl.svc.cluster.local/v2/models/bee/infer as Triton Inference Server...\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m YOLOv5 requirement \"tritonclient[all]\" not found, attempting AutoUpdate...\n",
      "Collecting tritonclient[all]\n",
      "  Using cached tritonclient-2.30.0-py3-none-any.whl (67 kB)\n",
      "Collecting python-rapidjson>=0.9.1\n",
      "  Using cached python_rapidjson-1.9-cp39-cp39-linux_ppc64le.whl\n",
      "Requirement already satisfied: numpy>=1.19.1 in /opt/conda/lib/python3.9/site-packages (from tritonclient[all]) (1.23.5)\n",
      "Collecting geventhttpclient<=2.0.2,>=1.4.4\n",
      "  Using cached geventhttpclient-2.0.2-cp39-cp39-linux_ppc64le.whl\n",
      "Requirement already satisfied: grpcio>=1.41.0 in /opt/conda/lib/python3.9/site-packages (from tritonclient[all]) (1.42.0)\n",
      "Requirement already satisfied: aiohttp>=3.8.1 in /opt/conda/lib/python3.9/site-packages (from tritonclient[all]) (3.8.3)\n",
      "Requirement already satisfied: protobuf<4,>=3.5.0 in /opt/conda/lib/python3.9/site-packages (from tritonclient[all]) (3.19.6)\n",
      "Requirement already satisfied: packaging>=14.1 in /opt/conda/lib/python3.9/site-packages (from tritonclient[all]) (22.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp>=3.8.1->tritonclient[all]) (6.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp>=3.8.1->tritonclient[all]) (21.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp>=3.8.1->tritonclient[all]) (2.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp>=3.8.1->tritonclient[all]) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp>=3.8.1->tritonclient[all]) (1.3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp>=3.8.1->tritonclient[all]) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp>=3.8.1->tritonclient[all]) (1.8.1)\n",
      "Collecting brotli\n",
      "  Using cached Brotli-1.0.9-cp39-cp39-linux_ppc64le.whl\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.9/site-packages (from geventhttpclient<=2.0.2,>=1.4.4->tritonclient[all]) (2022.12.7)\n",
      "Collecting gevent>=0.13\n",
      "  Using cached gevent-22.10.2-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl (5.0 MB)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from geventhttpclient<=2.0.2,>=1.4.4->tritonclient[all]) (1.16.0)\n",
      "Collecting zope.interface\n",
      "  Using cached zope.interface-5.5.2-cp39-cp39-linux_ppc64le.whl\n",
      "Requirement already satisfied: greenlet>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from gevent>=0.13->geventhttpclient<=2.0.2,>=1.4.4->tritonclient[all]) (2.0.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from gevent>=0.13->geventhttpclient<=2.0.2,>=1.4.4->tritonclient[all]) (63.4.1)\n",
      "Collecting zope.event\n",
      "  Using cached zope.event-4.6-py2.py3-none-any.whl (6.8 kB)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.9/site-packages (from yarl<2.0,>=1.0->aiohttp>=3.8.1->tritonclient[all]) (3.4)\n",
      "Installing collected packages: brotli, zope.interface, zope.event, python-rapidjson, tritonclient, gevent, geventhttpclient\n",
      "Successfully installed brotli-1.0.9 gevent-22.10.2 geventhttpclient-2.0.2 python-rapidjson-1.9 tritonclient-2.30.0 zope.event-4.6 zope.interface-5.5.2\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m 1 package updated per ['tritonclient[all]']\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m âš ï¸ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n",
      "image 1/1 /home/jovyan/vol-1/bee_dataset/test/images/2016-03-15-04-09-38-1024x673_jpg.rf.aa352f3bd2a105dd7ff560e0ab42ae69.jpg: 640x640 (no detections), 15526.7ms\n",
      "Speed: 147.9ms pre-process, 15526.7ms inference, 0.8ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1m../../yolov5/runs/detect/exp4\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python /home/jovyan/yolov5/detect.py --weights=http://bee.kubeflow-ntl.svc.cluster.local/v2/models/bee/infer --data=$BEE_DATA_SET_PATH/data.yaml --source=$IMAGE --conf-thres=.7 --iou-thres=.2 --max-det=500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6407ee4-e7bb-4328-965d-52d20d218086",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed71b423-b33d-41fd-9c7d-e35bae6b304e",
   "metadata": {},
   "source": [
    "Because we have an ONNX model in our repo that was trained over many epocs, we can use that model, instead of the InferenceService.\n",
    "\n",
    "Because we are running the inference in the notebook, and because the tool expects a specific version of ONNX Runtime, we must install that first.\n",
    "\n",
    "There are actually several ways to solve this problem:\n",
    "\n",
    "* Install onnxruntime in each container that is running onnxruntime on the CPU (The approach taken here)\n",
    "* Change the container image to install the CPU version, and do the install in components that need the GPU version\n",
    "* Update the yolov5 code so that it understands the package versions available in conda/rocketce.\n",
    "\n",
    "The best choice can only be determined by considering a real problem that is being solved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8966d87c-e5c3-4919-a551-7ca5327c95c0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                  __    __    __    __\n",
      "                 /  \\  /  \\  /  \\  /  \\\n",
      "                /    \\/    \\/    \\/    \\\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ/  /â–ˆâ–ˆ/  /â–ˆâ–ˆ/  /â–ˆâ–ˆ/  /â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "              /  / \\   / \\   / \\   / \\  \\____\n",
      "             /  /   \\_/   \\_/   \\_/   \\    o \\__,\n",
      "            / _/                       \\_____/  `\n",
      "            |/\n",
      "        â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—\n",
      "        â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—\n",
      "        â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘\n",
      "        â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘\n",
      "        â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘\n",
      "        â•šâ•â•     â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â• â•šâ•â•  â•šâ•â•\n",
      "\n",
      "        mamba (0.25.0) supported by @QuantStack\n",
      "\n",
      "        GitHub:  https://github.com/mamba-org/mamba\n",
      "        Twitter: https://twitter.com/QuantStack\n",
      "\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\n",
      "\n",
      "Looking for: ['onnxruntime==1.13.1=hea80eff_cpu_py39_pb3.19_1']\n",
      "\n",
      "\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\n",
      "rocketce/linux-ppc64le                             \u001b[90mâ”â”â”â”â•¸\u001b[0m\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m   0.0 B  0.0s\n",
      "rocketce/noarch                                    \u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[0m\u001b[90mâ”â”\u001b[0m   0.0 B  0.0s\n",
      "https://ftp.osuosl.org/pub/open-ce/current/linux.. \u001b[90mâ”â•¸\u001b[0m\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   0.0 B  0.0s\n",
      "https://ftp.osuosl.org/pub/open-ce/current/noarch  \u001b[33mâ”â”â”â”â”â”â”â”â•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”\u001b[0m   0.0 B  0.0s\n",
      "pkgs/main/linux-ppc64le                            \u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[0m\u001b[90mâ”\u001b[0m   0.0 B  0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.1s\n",
      "rocketce/linux-ppc64le                             \u001b[33mâ”â”â”â”â”â”â”â•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”\u001b[0m   0.0 B  0.1s\n",
      "rocketce/noarch                                    \u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[0m\u001b[90mâ”â”\u001b[0m   0.0 B  0.1s\n",
      "https://ftp.osuosl.org/pub/open-ce/current/linux.. \u001b[90mâ”â”â•¸\u001b[0m\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   0.0 B  0.1s\n",
      "https://ftp.osuosl.org/pub/open-ce/current/noarch  \u001b[33mâ”â”â”â”â”â”â”â”â”â•¸\u001b[0m\u001b[90mâ”â”â”â”â”\u001b[0m   0.0 B  0.1s\n",
      "pkgs/main/linux-ppc64le                            \u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   0.0 B  0.1s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.2s\n",
      "rocketce/linux-ppc64le                             \u001b[33mâ”â”â”â”â”â”â”â”â”â•¸\u001b[0m\u001b[90mâ”â”â”â”â”\u001b[0m   0.0 B  0.2s\n",
      "rocketce/noarch                                    \u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   0.0 B  0.2s\n",
      "https://ftp.osuosl.org/pub/open-ce/current/linux.. \u001b[90mâ”â”â”â•¸\u001b[0m\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”\u001b[0m   0.0 B  0.2s\n",
      "https://ftp.osuosl.org/pub/open-ce/current/noarch  \u001b[33mâ”â”â”â”â”â”â”â”â”â”â•¸\u001b[0m\u001b[90mâ”â”â”â”\u001b[0m   0.0 B  0.2s\n",
      "pkgs/main/linux-ppc64le                            \u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 482.0 B  0.2s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.3s\n",
      "rocketce/linux-ppc64le                             \u001b[33mâ”â”â”â”â”â”â”â”â”â•¸\u001b[0m\u001b[90mâ”â”â”â”â”\u001b[0m   0.0 B  0.3s\n",
      "rocketce/noarch                                    \u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   0.0 B  0.3s\n",
      "https://ftp.osuosl.org/pub/open-ce/current/linux.. \u001b[90mâ”â”â”â”â•¸\u001b[0m\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m   0.0 B  0.3s\n",
      "https://ftp.osuosl.org/pub/open-ce/current/noarch  \u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[0m\u001b[90mâ”â”â”\u001b[0m   0.0 B  0.3s\n",
      "pkgs/main/linux-ppc64le                            \u001b[90mâ•¸\u001b[0m\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m  98.7kB  0.3s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.4s\n",
      "rocketce/linux-ppc64le                             \u001b[33mâ”â”â”â”â”â”â”â”â”â”â•¸\u001b[0m\u001b[90mâ”â”â”â”\u001b[0m 699.0 B  0.4s\n",
      "rocketce/noarch                                    \u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   0.0 B  0.4s\n",
      "https://ftp.osuosl.org/pub/open-ce/current/linux.. \u001b[90mâ”â”â”â”â”â•¸\u001b[0m\u001b[33mâ”â”â”â”â”â”â”â”â”\u001b[0m   0.0 B  0.4s\n",
      "https://ftp.osuosl.org/pub/open-ce/current/noarch  \u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[0m\u001b[90mâ”â”â”\u001b[0m   0.0 B  0.4s\n",
      "pkgs/main/linux-ppc64le                            \u001b[90mâ”â•¸\u001b[0m\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 102.9kB  0.4s\n",
      "pkgs/r/noarch                                      \u001b[33mâ”â”â”â”â”â”â”â”â•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”\u001b[0m   0.0 B  0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Grocketce/noarch                                     0.4s\n",
      "https://ftp.osuosl.org/pub/open-ce/current/linux..            No change\n",
      "https://ftp.osuosl.org/pub/open-ce/current/noarch             No change\n",
      "rocketce/linux-ppc64le                              0.5s\n",
      "[+] 0.5s\n",
      "pkgs/main/linux-ppc64le \u001b[90mâ”â”â”â”â”â”â•¸\u001b[0m\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   1.3MB /  ??.?MB @   2.7MB/s  0.5s\n",
      "pkgs/main/noarch        \u001b[90mâ”â•¸\u001b[0m\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[0m\u001b[90mâ”â”\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.1s\n",
      "pkgs/r/linux-ppc64le    \u001b[90mâ”â•¸\u001b[0m\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[0m\u001b[90mâ”â”\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.1s\n",
      "pkgs/r/noarch           \u001b[33mâ”â”â”â”â”â”â”â”â”â•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.1s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gpkgs/r/noarch                                                 No change\n",
      "pkgs/r/linux-ppc64le                                          No change\n",
      "[+] 0.6s\n",
      "pkgs/main/linux-ppc64le \u001b[90mâ”â”â”â”â”â”â”â”â•¸\u001b[0m\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”\u001b[0m   1.9MB /  ??.?MB @   3.3MB/s  0.6s\n",
      "pkgs/main/noarch        \u001b[90mâ”â”â”â•¸\u001b[0m\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m  30.0kB /  ??.?MB @  51.4kB/s  0.2s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.7s\n",
      "pkgs/main/linux-ppc64le \u001b[90mâ”â”â”â”â”â”â”â”â”â”â•¸\u001b[0m\u001b[33mâ”â”â”â”â”â”â”â”â”\u001b[0m   2.4MB /  ??.?MB @   3.6MB/s  0.7s\n",
      "pkgs/main/noarch        \u001b[90mâ”â”â”â”â”â•¸\u001b[0m\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 450.5kB /  ??.?MB @ 657.3kB/s  0.3s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gpkgs/main/noarch                                   818.9kB @   1.1MB/s  0.3s\n",
      "[+] 0.8s\n",
      "pkgs/main/linux-ppc64le \u001b[33mâ”â”â”â”â”â”â”â”â”â•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”\u001b[0m   2.9MB /  ??.?MB @   3.8MB/s  0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[0Gpkgs/main/linux-ppc64le                              3.1MB @   3.8MB/s  0.8s\n",
      "\u001b[?25h\n",
      "Pinned packages:\n",
      "  - python 3.9.*\n",
      "  - python 3.9.13\n",
      "  - tensorflow 2.9.2.*\n",
      "  - pytorch 1.12.1.*\n",
      "\n",
      "\n",
      "Transaction\n",
      "\n",
      "  Prefix: /opt/conda\n",
      "\n",
      "  Updating specs:\n",
      "\n",
      "   - onnxruntime==1.13.1=hea80eff_cpu_py39_pb3.19_1\n",
      "   - ca-certificates\n",
      "   - certifi\n",
      "   - openssl\n",
      "\n",
      "\n",
      "  Package               Version  Build                            Channel                                 Size\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  Install:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "\u001b[32m  + protobuf       \u001b[00m      3.19.1  py39h29c3540_0                   pkgs/main/linux-ppc64le                338kB\n",
      "\n",
      "  Upgrade:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "\u001b[31m  - ca-certificates\u001b[00m  2022.10.11  h6ffa863_0                       pkgs/main                                   \n",
      "\u001b[32m  + ca-certificates\u001b[00m  2023.01.10  h6ffa863_0                       pkgs/main/linux-ppc64le                123kB\n",
      "\u001b[31m  - onnxruntime    \u001b[00m      1.12.1  hf9ee15a_cuda11.4_py39_pb3.19_1  ftp.osuosl.org/pub/open-ce/current          \n",
      "\u001b[32m  + onnxruntime    \u001b[00m      1.13.1  hea80eff_cpu_py39_pb3.19_1       rocketce/linux-ppc64le                   6MB\n",
      "\n",
      "  Downgrade:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "\u001b[31m  - numpy          \u001b[00m      1.23.5  py39h181cc9a_0                   pkgs/main                                   \n",
      "\u001b[32m  + numpy          \u001b[00m      1.23.4  py39h6ac58bb_0                   rocketce/linux-ppc64le                  11kB\n",
      "\u001b[31m  - numpy-base     \u001b[00m      1.23.5  py39h1bde650_0                   pkgs/main                                   \n",
      "\u001b[32m  + numpy-base     \u001b[00m      1.23.4  py39h4c1029e_0                   rocketce/linux-ppc64le                   6MB\n",
      "\n",
      "  Summary:\n",
      "\n",
      "  Install: 1 packages\n",
      "  Upgrade: 2 packages\n",
      "  Downgrade: 2 packages\n",
      "\n",
      "  Total download: 12MB\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\n",
      "Downloading  (5) \u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   0.0 B ca-certificates            0.0s\n",
      "Extracting       \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m       0                            0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.1s\n",
      "Downloading  (5) \u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   0.0 B ca-certificates            0.1s\n",
      "Extracting       \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m       0                            0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gca-certificates                                    122.7kB @ 639.9kB/s  0.2s\n",
      "[+] 0.2s\n",
      "Downloading  (4) \u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 186.3kB numpy                      0.2s\n",
      "Extracting   (1) \u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”\u001b[0m       0 ca-certificates            0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.3s\n",
      "Downloading  (4) \u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 326.9kB numpy                      0.3s\n",
      "Extracting   (1) \u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”\u001b[0m       0 ca-certificates            0.1s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gprotobuf                                           338.0kB @   1.1MB/s  0.3s\n",
      "[+] 0.4s\n",
      "Downloading  (3) \u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 460.7kB numpy                      0.4s\n",
      "Extracting   (2) \u001b[90mâ•¸\u001b[0m\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”\u001b[0m       0 ca-certificates            0.2s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.5s\n",
      "Downloading  (3) \u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 460.7kB numpy                      0.5s\n",
      "Extracting   (2) \u001b[90mâ”â•¸\u001b[0m\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[0m\u001b[90mâ”â”â”â”â”\u001b[0m       0 ca-certificates            0.3s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.6s\n",
      "Downloading  (3) \u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 460.7kB numpy-base                 0.6s\n",
      "Extracting   (1) â”â”â”â•¸\u001b[33mâ”â”â”â”â”â•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m       1 protobuf                   0.4s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.7s\n",
      "Downloading  (3) \u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 460.7kB numpy-base                 0.7s\n",
      "Extracting       â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â•¸\u001b[0m\u001b[90mâ”â”â”â”â”\u001b[0m       2                            0.5s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.8s\n",
      "Downloading  (3) \u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 460.7kB numpy-base                 0.8s\n",
      "Extracting       â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â•¸\u001b[0m\u001b[90mâ”â”â”â”â”\u001b[0m       2                            0.5s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gnumpy                                               11.3kB @  13.8kB/s  0.8s\n",
      "[+] 0.9s\n",
      "Downloading  (2) \u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 472.1kB numpy-base                 0.9s\n",
      "Extracting   (1) â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”\u001b[0m       2 numpy                      0.5s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.0s\n",
      "Downloading  (2) \u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 488.9kB onnxruntime                1.0s\n",
      "Extracting   (1) â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”\u001b[0m       2 numpy                      0.6s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.1s\n",
      "Downloading  (2) \u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 540.0kB onnxruntime                1.1s\n",
      "Extracting   (1) â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”\u001b[0m       2 numpy                      0.7s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.2s\n",
      "Downloading  (2) \u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 574.8kB onnxruntime                1.2s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.3s\n",
      "Downloading  (2) â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 644.5kB onnxruntime                1.3s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.4s\n",
      "Downloading  (2) â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 714.1kB numpy-base                 1.4s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.5s\n",
      "Downloading  (2) â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 766.3kB numpy-base                 1.5s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.6s\n",
      "Downloading  (2) â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 835.9kB numpy-base                 1.6s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.7s\n",
      "Downloading  (2) â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 870.8kB numpy-base                 1.7s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.8s\n",
      "Downloading  (2) â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 906.6kB onnxruntime                1.8s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.9s\n",
      "Downloading  (2) â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   1.0MB onnxruntime                1.9s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.0s\n",
      "Downloading  (2) â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   1.1MB onnxruntime                2.0s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.1s\n",
      "Downloading  (2) â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   1.2MB onnxruntime                2.1s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.2s\n",
      "Downloading  (2) â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   1.3MB numpy-base                 2.2s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.3s\n",
      "Downloading  (2) â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   1.4MB numpy-base                 2.3s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.4s\n",
      "Downloading  (2) â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   1.4MB numpy-base                 2.4s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.5s\n",
      "Downloading  (2) â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   1.5MB numpy-base                 2.5s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.6s\n",
      "Downloading  (2) â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   1.6MB onnxruntime                2.6s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.7s\n",
      "Downloading  (2) â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   1.7MB onnxruntime                2.7s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.8s\n",
      "Downloading  (2) â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   1.8MB onnxruntime                2.8s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.9s\n",
      "Downloading  (2) â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   1.8MB onnxruntime                2.9s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.0s\n",
      "Downloading  (2) â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   2.0MB numpy-base                 3.0s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.1s\n",
      "Downloading  (2) â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   2.1MB numpy-base                 3.1s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.2s\n",
      "Downloading  (2) â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   2.2MB numpy-base                 3.2s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.3s\n",
      "Downloading  (2) â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   2.4MB numpy-base                 3.3s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.4s\n",
      "Downloading  (2) â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   2.5MB onnxruntime                3.4s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.5s\n",
      "Downloading  (2) â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   2.6MB onnxruntime                3.5s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.6s\n",
      "Downloading  (2) â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   2.7MB onnxruntime                3.6s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.7s\n",
      "Downloading  (2) â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   2.9MB onnxruntime                3.7s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.8s\n",
      "Downloading  (2) â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   2.9MB numpy-base                 3.8s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.9s\n",
      "Downloading  (2) â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   3.1MB numpy-base                 3.9s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.0s\n",
      "Downloading  (2) â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   3.2MB numpy-base                 4.0s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.1s\n",
      "Downloading  (2) â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   3.4MB numpy-base                 4.1s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.2s\n",
      "Downloading  (2) â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   3.6MB onnxruntime                4.2s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.3s\n",
      "Downloading  (2) â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   3.7MB onnxruntime                4.3s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.4s\n",
      "Downloading  (2) â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   3.9MB onnxruntime                4.4s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.5s\n",
      "Downloading  (2) â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   4.1MB onnxruntime                4.5s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.6s\n",
      "Downloading  (2) â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   4.2MB numpy-base                 4.6s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.7s\n",
      "Downloading  (2) â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   4.4MB numpy-base                 4.7s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.8s\n",
      "Downloading  (2) â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   4.5MB numpy-base                 4.8s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.9s\n",
      "Downloading  (2) â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   4.6MB numpy-base                 4.9s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 5.0s\n",
      "Downloading  (2) â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   4.9MB onnxruntime                5.0s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 5.1s\n",
      "Downloading  (2) â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   5.1MB onnxruntime                5.1s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 5.2s\n",
      "Downloading  (2) â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   5.2MB onnxruntime                5.2s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 5.3s\n",
      "Downloading  (2) â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   5.5MB onnxruntime                5.3s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 5.4s\n",
      "Downloading  (2) â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   5.6MB numpy-base                 5.4s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 5.5s\n",
      "Downloading  (2) â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   5.8MB numpy-base                 5.5s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 5.6s\n",
      "Downloading  (2) â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   6.1MB numpy-base                 5.6s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 5.7s\n",
      "Downloading  (2) â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   6.2MB numpy-base                 5.7s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 5.8s\n",
      "Downloading  (2) â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   6.5MB onnxruntime                5.8s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 5.9s\n",
      "Downloading  (2) â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m   6.6MB onnxruntime                5.9s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 6.0s\n",
      "Downloading  (2) â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”\u001b[0m   6.7MB onnxruntime                6.0s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 6.1s\n",
      "Downloading  (2) â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”â”\u001b[0m   7.1MB onnxruntime                6.1s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 6.2s\n",
      "Downloading  (2) â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m   7.3MB numpy-base                 6.2s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 6.3s\n",
      "Downloading  (2) â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m   7.5MB numpy-base                 6.3s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 6.4s\n",
      "Downloading  (2) â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”\u001b[0m   7.8MB numpy-base                 6.4s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 6.5s\n",
      "Downloading  (2) â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”\u001b[0m   7.9MB numpy-base                 6.5s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 6.6s\n",
      "Downloading  (2) â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”\u001b[0m   8.2MB onnxruntime                6.6s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 6.7s\n",
      "Downloading  (2) â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”\u001b[0m   8.4MB onnxruntime                6.7s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 6.8s\n",
      "Downloading  (2) â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”\u001b[0m   8.7MB onnxruntime                6.8s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 6.9s\n",
      "Downloading  (2) â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”\u001b[0m   9.0MB onnxruntime                6.9s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 7.0s\n",
      "Downloading  (2) â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”\u001b[0m   9.1MB numpy-base                 7.0s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 7.1s\n",
      "Downloading  (2) â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”\u001b[0m   9.4MB numpy-base                 7.1s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 7.2s\n",
      "Downloading  (2) â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”\u001b[0m   9.6MB numpy-base                 7.2s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 7.3s\n",
      "Downloading  (2) â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”\u001b[0m   9.9MB numpy-base                 7.3s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 7.4s\n",
      "Downloading  (2) â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”\u001b[0m  10.3MB onnxruntime                7.4s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 7.5s\n",
      "Downloading  (2) â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”\u001b[0m  10.4MB onnxruntime                7.5s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 7.6s\n",
      "Downloading  (2) â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”\u001b[0m  10.8MB onnxruntime                7.6s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 7.7s\n",
      "Downloading  (2) â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”\u001b[0m  10.9MB onnxruntime                7.7s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 7.8s\n",
      "Downloading  (2) â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”\u001b[0m  11.3MB numpy-base                 7.8s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 7.9s\n",
      "Downloading  (2) â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”\u001b[0m  11.5MB numpy-base                 7.9s\n",
      "Extracting       â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â”â”â”â”â”\u001b[0m       3                            0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gonnxruntime                                          5.7MB @ 712.4kB/s  7.9s\n",
      "[+] 8.0s\n",
      "Downloading  (1) â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”\u001b[0m  11.8MB numpy-base                 8.0s\n",
      "Extracting   (1) â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â•¸\u001b[0m\u001b[90mâ”â”â”â”\u001b[0m       3 onnxruntime                0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 8.1s\n",
      "Downloading  (1) â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”\u001b[0m  11.8MB numpy-base                 8.1s\n",
      "Extracting   (1) â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â•¸\u001b[0m\u001b[90mâ”â”â”â”\u001b[0m       3 onnxruntime                0.9s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 8.2s\n",
      "Downloading  (1) â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”\u001b[0m  12.2MB numpy-base                 8.2s\n",
      "Extracting   (1) â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â•¸\u001b[0m\u001b[90mâ”â”â”â”\u001b[0m       3 onnxruntime                1.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 8.3s\n",
      "Downloading  (1) â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”\u001b[0m  12.2MB numpy-base                 8.3s\n",
      "Extracting   (1) â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”â•¸\u001b[0m\u001b[90mâ”â”â”â”\u001b[0m       3 onnxruntime                1.1s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gnumpy-base                                           6.3MB @ 754.7kB/s  8.4s\n",
      "[+] 8.4s\n",
      "Downloading      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  12.5MB                            8.4s\n",
      "Extracting   (1) â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”\u001b[0m       4 numpy-base                 1.2s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 8.5s\n",
      "Downloading      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  12.5MB                            8.4s\n",
      "Extracting   (1) â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”\u001b[0m       4 numpy-base                 1.3s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 8.6s\n",
      "Downloading      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  12.5MB                            8.4s\n",
      "Extracting   (1) â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”\u001b[0m       4 numpy-base                 1.4s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 8.7s\n",
      "Downloading      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  12.5MB                            8.4s\n",
      "Extracting   (1) â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”\u001b[0m       4 numpy-base                 1.5s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 8.8s\n",
      "Downloading      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  12.5MB                            8.4s\n",
      "Extracting   (1) â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”\u001b[0m       4 numpy-base                 1.6s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 8.9s\n",
      "Downloading      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  12.5MB                            8.4s\n",
      "Extracting   (1) â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸\u001b[33mâ”â”â”â”â”\u001b[0m       4 numpy-base                 1.7s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G\u001b[?25h\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!mamba install -c rocketce onnxruntime=1.13.1=hea80eff_cpu_py39_pb3.19_1 -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3549f11-c48f-488f-bafd-d3619934577b",
   "metadata": {},
   "source": [
    "Now we can run the inference script in the notebook, against the (better) model that was included in the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c3318ac4-ec13-4c9a-9c9c-02c81b911611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/home/jovyan/kubeflow-ppc64le-examples/object-detection-yolov5/data/model.onnx'], source=/home/jovyan/vol-1/bee_dataset/test/images/2016-03-15-04-09-38-1024x673_jpg.rf.aa352f3bd2a105dd7ff560e0ab42ae69.jpg, data=/home/jovyan/vol-1/bee_data/data.yaml, imgsz=[640, 640], conf_thres=0.7, iou_thres=0.2, max_det=500, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=../../yolov5/runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
      "YOLOv5 ğŸš€ v7.0-0-g915bbf2 Python-3.9.13 torch-1.12.1 CPU\n",
      "\n",
      "Loading /home/jovyan/kubeflow-ppc64le-examples/object-detection-yolov5/data/model.onnx for ONNX Runtime inference...\n",
      "image 1/1 /home/jovyan/vol-1/bee_dataset/test/images/2016-03-15-04-09-38-1024x673_jpg.rf.aa352f3bd2a105dd7ff560e0ab42ae69.jpg: 640x640 24 bees, 4 queens, 6799.9ms\n",
      "Speed: 267.2ms pre-process, 6799.9ms inference, 299.9ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1m../../yolov5/runs/detect/exp5\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python /home/jovyan/yolov5/detect.py --weights=/home/jovyan/kubeflow-ppc64le-examples/object-detection-yolov5/data/model.onnx --data=/home/jovyan/vol-1/bee_data/data.yaml --source=$IMAGE --conf-thres=.7 --iou-thres=.2 --max-det=500"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
