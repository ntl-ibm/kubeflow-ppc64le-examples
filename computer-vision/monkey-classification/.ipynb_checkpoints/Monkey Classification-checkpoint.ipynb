{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "changing-medication",
   "metadata": {},
   "source": [
    "# Monkey Classification: End-to-end Pipeline\n",
    "In this notebook we're going to show how you can train and serve a simple model using Kubeflow pipelines!<br>\n",
    "\n",
    "The dataset we're using consists of two folders; training and validation. Each folder contains 10 subforders labeled as n0~n9, each corresponding a species from Wikipedia's monkey cladogram. Images are 400x300 px or larger and JPEG format.\n",
    "\n",
    "You'll need:\n",
    "- Just this notebook\n",
    "- Kubeflow deployed on either OpenShift or Kubernetes (make sure it is on IBM Power, we don't support x86!)\n",
    "- Internet connection to download the sample images (you can also manually upload them to your preferred S3 bucket - e.g. minio)\n",
    "\n",
    "Datasource: https://www.kaggle.com/slothkong/10-monkey-species\n",
    "\n",
    "## Authors\n",
    "- Marvin Giessing <MARVING@de.ibm.com>\n",
    "- Sebastian Lehrig <sebastian.lehrig1@ibm.com>\n",
    "\n",
    "## References\n",
    "- https://www.philschmid.de/image-classification-huggingface-transformers-keras\n",
    "- https://discuss.huggingface.co/t/solved-image-dataset-seems-slow-for-larger-image-size/10960/6\n",
    "\n",
    "## License\n",
    "Apache-2.0 License"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-think",
   "metadata": {},
   "source": [
    "## 0.) Imports & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8472735-d399-41fc-bdae-e9116de11b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lab_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext lab_black\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import json\n",
    "import kfp\n",
    "from kfp.components import InputPath, OutputPath\n",
    "import kfp.dsl as dsl\n",
    "from kfp.dsl import PipelineConf, data_passing_methods\n",
    "from kubernetes.client.models import V1Volume, V1PersistentVolumeClaimVolumeSource\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "from typing import List, NamedTuple\n",
    "\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a8d8565-390a-4c97-9228-997df96e7092",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPONENT_CATALOG_FOLDER = f\"{os.getenv('HOME')}/components\"\n",
    "\n",
    "LOAD_DATASET_COMPONENT = (\n",
    "    f\"{COMPONENT_CATALOG_FOLDER}/data-collection/load-dataset/component.yaml\"\n",
    ")\n",
    "TRAIN_MODEL_COMPONENT = (\n",
    "    f\"{COMPONENT_CATALOG_FOLDER}/model-building/train-model-job/component.yaml\"\n",
    ")\n",
    "PLOT_CONFUSION_MATRIX_COMPONENT = (\n",
    "    f\"{COMPONENT_CATALOG_FOLDER}/model-building/plot-confusion-matrix/component.yaml\"\n",
    ")\n",
    "CONVERT_MODEL_TO_ONNX_COMPONENT = (\n",
    "    f\"{COMPONENT_CATALOG_FOLDER}/model-building/convert-to-onnx/component.yaml\"\n",
    ")\n",
    "UPLOAD_MODEL_COMPONENT = (\n",
    "    f\"{COMPONENT_CATALOG_FOLDER}/model-building/upload-model/component.yaml\"\n",
    ")\n",
    "\n",
    "DEPLOY_MODEL_COMPONENT = f\"{os.getenv('HOME')}/kubeflow-ppc64le-examples/deploy_triton_inference_service_component/deploy_triton_inference_service_component.yaml\"\n",
    "\n",
    "\n",
    "BASE_IMAGE = (\n",
    "    \"quay.io/ibm/kubeflow-notebook-image-ppc64le:elyra3.14.1-py3.9-tf2.10.1-pt1.10.2-v0\"\n",
    ")\n",
    "\n",
    "with open(\"/var/run/secrets/kubernetes.io/serviceaccount/namespace\") as f:\n",
    "    NAMESPACE = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-bedroom",
   "metadata": {},
   "source": [
    "## 1.) Let's start with creating a client object for interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "republican-pantyhose",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-waste",
   "metadata": {},
   "source": [
    "## 2.) The main part consists of defining the end-to-end workflow functions and create components from them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d5e024-32a5-48d2-8281-adf08da1cb9c",
   "metadata": {},
   "source": [
    "### 2.1) Load dataset (by reusing a Kubeflow component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "handled-daily",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dataset_comp = kfp.components.load_component_from_file(LOAD_DATASET_COMPONENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3da1297-7058-43ff-b761-f41c79a80b7c",
   "metadata": {},
   "source": [
    "### 2.2) Preprocess data (normalizing, one hot encoding, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a14b3e9-2c78-465c-960d-6fd6a01d103d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(\n",
    "    dataset_dir: InputPath(str),\n",
    "    train_dataset_dir: OutputPath(str),\n",
    "    validation_dataset_dir: OutputPath(str),\n",
    "    test_dataset_dir: OutputPath(str),\n",
    "    test_size: float = 0.2,\n",
    "    seed: int = 42,\n",
    "    size: int = 224,\n",
    "    batch_size: int = 16,\n",
    "):\n",
    "    \"\"\"Split data into train/dev/test data. Saves result into `prep_dataset_dir`.\"\"\"\n",
    "\n",
    "    from datasets import Array3D, DatasetDict, Features, load_from_disk, Sequence, Value\n",
    "    import numpy as np\n",
    "    import os\n",
    "    from transformers import DefaultDataCollator, ImageFeatureExtractionMixin\n",
    "\n",
    "    print(f\"Loading input dataset from {dataset_dir}...\")\n",
    "    dataset = load_from_disk(dataset_dir)\n",
    "    print(\"Dataset loaded.\")\n",
    "\n",
    "    # Preprocess\n",
    "    num_classes = dataset[\"train\"].features[\"label\"].num_classes\n",
    "    one_hot_matrix = np.eye(num_classes)\n",
    "    feature_extractor = ImageFeatureExtractionMixin()\n",
    "\n",
    "    def to_pixels(image):\n",
    "        image = feature_extractor.resize(image, size=size)\n",
    "        image = feature_extractor.to_numpy_array(image, channel_first=False)\n",
    "        image = image / 255.0\n",
    "        return image\n",
    "\n",
    "    def process(examples):\n",
    "        examples[\"pixel_values\"] = [to_pixels(image) for image in examples[\"image\"]]\n",
    "        examples[\"label\"] = [one_hot_matrix[label] for label in examples[\"label\"]]\n",
    "        return examples\n",
    "\n",
    "    features = Features(\n",
    "        {\n",
    "            \"pixel_values\": Array3D(dtype=\"float32\", shape=(size, size, 3)),\n",
    "            \"label\": Sequence(feature=Value(dtype=\"int32\"), length=num_classes),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    prep_dataset = dataset.map(\n",
    "        process,\n",
    "        remove_columns=[\"image\"],\n",
    "        batched=True,\n",
    "        batch_size=batch_size,\n",
    "        num_proc=2,\n",
    "        features=features,\n",
    "        keep_in_memory=True,\n",
    "    )\n",
    "\n",
    "    # prep_dataset = prep_dataset.with_format(\"numpy\")\n",
    "\n",
    "    # Split\n",
    "    print(\"Splitting dataset...\")\n",
    "    dev_test_dataset = prep_dataset[\"test\"].train_test_split(\n",
    "        test_size=test_size, shuffle=True, seed=seed\n",
    "    )\n",
    "\n",
    "    train_dev_test_dataset = DatasetDict(\n",
    "        {\n",
    "            \"train\": prep_dataset[\"train\"],\n",
    "            \"validation\": dev_test_dataset[\"train\"],\n",
    "            \"test\": dev_test_dataset[\"test\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    def save_as_tfdataset(\n",
    "        dataset, columns, label_columns, data_collator, directory, shuffle\n",
    "    ):\n",
    "        import tensorflow as tf\n",
    "\n",
    "        tf_dataset = dataset.to_tf_dataset(\n",
    "            columns=columns,\n",
    "            label_cols=label_columns,\n",
    "            shuffle=shuffle,\n",
    "            batch_size=batch_size,\n",
    "            collate_fn=data_collator,\n",
    "        )\n",
    "\n",
    "        print(f\"Saving pre-processed dataset to '{directory}'...\")\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        tf.data.experimental.save(tf_dataset, directory)\n",
    "\n",
    "        print(f\"Pre-processed dataset saved. Contents of '{directory}':\")\n",
    "        print(os.listdir(directory))\n",
    "\n",
    "    data_collator = DefaultDataCollator(return_tensors=\"tf\")\n",
    "    columns = [\"pixel_values\"]\n",
    "    label_columns = [\"labels\"]\n",
    "    save_as_tfdataset(\n",
    "        train_dev_test_dataset[\"train\"],\n",
    "        columns,\n",
    "        label_columns,\n",
    "        data_collator,\n",
    "        train_dataset_dir,\n",
    "        True,\n",
    "    )\n",
    "    save_as_tfdataset(\n",
    "        train_dev_test_dataset[\"validation\"],\n",
    "        columns,\n",
    "        label_columns,\n",
    "        data_collator,\n",
    "        validation_dataset_dir,\n",
    "        False,\n",
    "    )\n",
    "    save_as_tfdataset(\n",
    "        train_dev_test_dataset[\"test\"],\n",
    "        columns,\n",
    "        label_columns,\n",
    "        data_collator,\n",
    "        test_dataset_dir,\n",
    "        False,\n",
    "    )\n",
    "\n",
    "    print(\"Finished.\")\n",
    "\n",
    "\n",
    "preprocess_dataset_comp = kfp.components.create_component_from_func(\n",
    "    func=preprocess_dataset, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee800f7b-b065-40c0-8592-071f67d0faed",
   "metadata": {},
   "source": [
    "### 2.3 Train model (Distributed with horovod):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55a0998a-c453-49af-8f31-ba5ec270df9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_distributed_model(\n",
    "    train_dataset_dir: InputPath(str),\n",
    "    validation_dataset_dir: InputPath(str),\n",
    "    model_dir: OutputPath(str),\n",
    "    epochs: int = 100,\n",
    "    batch_size: int = 32,\n",
    "):\n",
    "    \"\"\"Uses transfer learning on a prepared dataset. Once trained, the model is persisted to `model_dir`.\"\"\"\n",
    "\n",
    "    import horovod.tensorflow.keras as hvd\n",
    "    import os\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import Sequential\n",
    "    from tensorflow.keras.applications import InceptionV3\n",
    "    from tensorflow.keras.layers import (\n",
    "        BatchNormalization,\n",
    "        Dense,\n",
    "        Dropout,\n",
    "        GlobalAveragePooling2D,\n",
    "    )\n",
    "    from tensorflow.keras.callbacks import (\n",
    "        EarlyStopping,\n",
    "        ModelCheckpoint,\n",
    "        ReduceLROnPlateau,\n",
    "        TensorBoard,\n",
    "    )\n",
    "    import tensorflow_datasets as tfds\n",
    "    import time\n",
    "\n",
    "    def load_datasets():\n",
    "        train_dataset = tf.data.experimental.load(train_dataset_dir)\n",
    "        validation_dataset = tf.data.experimental.load(validation_dataset_dir)\n",
    "        return (train_dataset, validation_dataset)\n",
    "\n",
    "    def build_model(shape):\n",
    "        backbone = InceptionV3(include_top=False, weights=\"imagenet\", input_shape=shape)\n",
    "\n",
    "        for layer in backbone.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(backbone)\n",
    "        model.add(GlobalAveragePooling2D())\n",
    "        model.add(Dense(128, activation=\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(64, activation=\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "        return model\n",
    "\n",
    "    print(\"Initializing Horovod/MPI for distributed training...\")\n",
    "    hvd.init()\n",
    "\n",
    "    # Pin GPU to be used to process local rank (one GPU per process)\n",
    "    gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    if gpus:\n",
    "        tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], \"GPU\")\n",
    "\n",
    "    # Prepare distributed training with GPU support\n",
    "    os.environ[\"NCCL_DEBUG\"] = \"INFO\"\n",
    "    tfds.disable_progress_bar()\n",
    "\n",
    "    if hvd.rank() == 0:\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "\n",
    "    # see https://horovod.readthedocs.io/en/stable/api.html\n",
    "    print(\"==============================================\")\n",
    "    print(f\"hvd.rank(): {str(hvd.rank())}\")\n",
    "    print(f\"hvd.local_rank(): {str(hvd.local_rank())}\")\n",
    "    print(f\"hvd.size(): {str(hvd.size())}\")\n",
    "    print(f\"hvd.local_size(): {str(hvd.local_size())}\")\n",
    "    print(\"gpus:\")\n",
    "    print(gpus)\n",
    "    print(\"==============================================\")\n",
    "\n",
    "    print(\"Loading datasets...\")\n",
    "    train_dataset, validation_dataset = load_datasets()\n",
    "    shape = (224, 224, 3)\n",
    "\n",
    "    print(\"Making traininig dataset ready for distributed training...\")\n",
    "    # Best shuffling needs a buffer with size equal to the size of the\n",
    "    # dataset. Approximate values should be fine here.\n",
    "    dataset_elements = 1400  # hard to determine dynamically in TFDataset\n",
    "    approx_shard_train_size = dataset_elements // hvd.size() + 1\n",
    "\n",
    "    # References:\n",
    "    # - shard: https://github.com/horovod/horovod/issues/2623#issuecomment-768435610\n",
    "    # - cache & prefetch: https://stackoverflow.com/questions/59228816/what-do-the-tensorflow-datasets-functions-cache-and-prefetch-do\n",
    "    # - shuffle: https://stackoverflow.com/questions/64372390/what-does-buffer-size-do-in-tensorflow-dataset-shuffling\n",
    "    distributed_train_dataset = (\n",
    "        train_dataset.unbatch()  # Batch after sharding\n",
    "        .shard(num_shards=hvd.size(), index=hvd.rank())  # 1 shard per worker\n",
    "        .cache()  # Reuse data on next epoch\n",
    "        .shuffle(\n",
    "            buffer_size=approx_shard_train_size, seed=42, reshuffle_each_iteration=False\n",
    "        )  # Randomize shards\n",
    "        .batch(batch_size)\n",
    "        .repeat()  # Avoid last batch being of unequal size\n",
    "        .prefetch(tf.data.AUTOTUNE)  # Overlap preprocessing and training\n",
    "    )\n",
    "\n",
    "    print(\"Building model...\")\n",
    "    model = build_model(shape)\n",
    "    print(model.summary())\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.001 * hvd.size())\n",
    "    # Horovod: add Horovod DistributedOptimizer.\n",
    "    opt = hvd.DistributedOptimizer(opt)\n",
    "\n",
    "    print(\"Compiling model...\")\n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"categorical_accuracy\"],\n",
    "        experimental_run_tf_function=False,\n",
    "    )\n",
    "\n",
    "    print(\"Initializing training callbacks...\")\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=20, verbose=0, mode=\"min\"),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor=\"val_loss\",\n",
    "            factor=0.1,\n",
    "            patience=7,\n",
    "            verbose=1,\n",
    "            min_delta=0.0001,\n",
    "            mode=\"min\",\n",
    "        ),\n",
    "        TensorBoard(\n",
    "            log_dir=f\"s3://mlpipeline/tensorboard/{os.environ['JOB_NAME']}\",\n",
    "            histogram_freq=1,\n",
    "        ),\n",
    "        # Horovod: broadcast initial variable states from rank 0 to all other processes.\n",
    "        # This is necessary to ensure consistent initialization of all workers when\n",
    "        # training is started with random weights or restored from a checkpoint.\n",
    "        hvd.callbacks.BroadcastGlobalVariablesCallback(0),\n",
    "    ]\n",
    "    # Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them.\n",
    "    if hvd.rank() == 0:\n",
    "        callbacks.append(\n",
    "            ModelCheckpoint(\n",
    "                f\"{model_dir}/best_model.keras\",\n",
    "                monitor=\"val_loss\",\n",
    "                save_best_only=True,\n",
    "                save_weights_only=True,\n",
    "                mode=\"min\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    print(\"Starting model training...\")\n",
    "    start = time.time()\n",
    "    hist = model.fit(\n",
    "        distributed_train_dataset,\n",
    "        validation_data=validation_dataset,\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=approx_shard_train_size // batch_size\n",
    "        + 1,  # Needed when using repeat()\n",
    "        callbacks=callbacks,\n",
    "        verbose=1 if hvd.rank() == 0 else 0,\n",
    "    )\n",
    "\n",
    "    if hvd.rank() == 0:\n",
    "        print(\"\\n\\nTraining took \", time.time() - start, \"seconds\")\n",
    "\n",
    "        print(\"Model train history:\")\n",
    "        print(hist.history)\n",
    "\n",
    "        print(f\"Saving model to: {model_dir}\")\n",
    "        model.save(model_dir)\n",
    "        print(f\"Model saved to: {model_dir}\")\n",
    "\n",
    "        print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "753fda4f-e2c6-4c5d-9085-e9f565e72df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_specification = kfp.components.func_to_component_text(\n",
    "    func=train_distributed_model\n",
    ")\n",
    "train_model_comp = kfp.components.load_component_from_file(TRAIN_MODEL_COMPONENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-logistics",
   "metadata": {},
   "source": [
    "### 2.4) Evaluate model with validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "owned-background",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    test_dataset_dir: InputPath(str), model_dir: InputPath(str), batch_size: int = 20\n",
    ") -> NamedTuple(\"EvaluationOutput\", [(\"mlpipeline_metrics\", \"Metrics\")]):\n",
    "    \"\"\"Loads a saved model from file and uses a pre-downloaded dataset for evaluation.\n",
    "    Model metrics are persisted to `{metrics_path}` for Kubeflow Pipelines metadata.\"\"\"\n",
    "\n",
    "    from collections import namedtuple\n",
    "    import json\n",
    "    import tensorflow as tf\n",
    "\n",
    "    test_dataset = tf.data.experimental.load(test_dataset_dir)\n",
    "    model = tf.keras.models.load_model(model_dir)\n",
    "    (loss, accuracy) = model.evaluate(test_dataset)\n",
    "\n",
    "    print((loss, accuracy))\n",
    "\n",
    "    metrics = {\n",
    "        \"metrics\": [\n",
    "            {\"name\": \"loss\", \"numberValue\": str(loss), \"format\": \"PERCENTAGE\"},\n",
    "            {\"name\": \"accuracy\", \"numberValue\": str(accuracy), \"format\": \"PERCENTAGE\"},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    out_tuple = namedtuple(\"EvaluationOutput\", [\"mlpipeline_metrics\"])\n",
    "\n",
    "    return out_tuple(json.dumps(metrics))\n",
    "\n",
    "\n",
    "evaluate_model_comp = kfp.components.create_component_from_func(\n",
    "    func=evaluate_model, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b974913f-6ea7-4c53-832c-8ac7b729f27f",
   "metadata": {},
   "source": [
    "### 2.5) Create confusion matrix (by reusing a Kubeflow component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "059ceb98-f8d9-4da1-9e9d-c870d9f85bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix_comp = kfp.components.load_component_from_file(\n",
    "    PLOT_CONFUSION_MATRIX_COMPONENT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34537fe7-7b4e-4abf-bf7b-f8ac3e0463ba",
   "metadata": {},
   "source": [
    "### 2.6) Convert model to ONNX (by reusing a Kubeflow component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "840fecc8-afb1-46e6-ba35-547f0ede9d5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "convert_model_to_onnx_comp = kfp.components.load_component_from_file(\n",
    "    CONVERT_MODEL_TO_ONNX_COMPONENT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-commonwealth",
   "metadata": {},
   "source": [
    "### 2.7) Upload model to MinIO artifact store (by reusing a Kubeflow component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "direct-norwegian",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_model_comp = kfp.components.load_component_from_file(UPLOAD_MODEL_COMPONENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ac44c3-407a-4aa4-938f-9ac06713f745",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.8) Deploy the model using KServe (by reusing a Kubeflow component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e8e4c32-b78c-4f70-aa26-61ccee18f8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_model_comp = kfp.components.load_component_from_file(DEPLOY_MODEL_COMPONENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-suicide",
   "metadata": {},
   "source": [
    "## 3.) Create the actual pipeline by combining the components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "material-morning",
   "metadata": {},
   "outputs": [],
   "source": [
    "blackboard_pvc: str = \"artefacts\"\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"End-to-end monkey species classification pipeline\",\n",
    "    description=\"An example pipeline that performs an image classification and determines different monkey species\",\n",
    ")\n",
    "def monkey_pipeline(\n",
    "    dataset_url: str = \"Lehrig/Monkey-Species-Collection\",\n",
    "    dataset_configuration: str = \"downsized\",\n",
    "    dataset_label_columns: List[str] = [\"label\"],\n",
    "    model_name: str = \"monkey-classification\",\n",
    "    cluster_configuration_secret: str = \"\",\n",
    "    training_gpus: int = 1,\n",
    "    number_of_workers: int = 1,\n",
    "    distribution_type: str = \"Job\",\n",
    "    training_node_selector: str = \"\",\n",
    "    epochs: int = 100,\n",
    "    minio_url=\"minio-service.kubeflow:9000\",\n",
    "):\n",
    "    create_blackboard = dsl.VolumeOp(\n",
    "        name=\"Create Artefacts Blackboard\",\n",
    "        resource_name=blackboard_pvc,\n",
    "        modes=dsl.VOLUME_MODE_RWO,\n",
    "        size=\"4Gi\",\n",
    "        set_owner_reference=True,\n",
    "    )\n",
    "\n",
    "    load_dataset_task = load_dataset_comp(\n",
    "        path=dataset_url,\n",
    "        configuration=dataset_configuration,\n",
    "        label_columns=dataset_label_columns,\n",
    "    )\n",
    "    load_dataset_task.after(create_blackboard)\n",
    "\n",
    "    preprocess_dataset_task = preprocess_dataset_comp(\n",
    "        dataset_dir=load_dataset_task.outputs[\"dataset_dir\"]\n",
    "    )\n",
    "\n",
    "    # InputPath and OutputPath like \"prep_dataset_dir\" & \"model_dir\":\n",
    "    # Use name of parameters of train component on right-hand side.\n",
    "    train_parameters = {\n",
    "        \"train_dataset_dir\": \"train_dataset_dir\",\n",
    "        \"validation_dataset_dir\": \"validation_dataset_dir\",\n",
    "        \"model_dir\": \"model_dir\",\n",
    "        \"epochs\": epochs,\n",
    "    }\n",
    "\n",
    "    distribution_specification = {\n",
    "        \"distribution_type\": distribution_type,\n",
    "        \"number_of_workers\": number_of_workers,\n",
    "    }\n",
    "\n",
    "    train_model_task = train_model_comp(\n",
    "        preprocess_dataset_task.outputs[\"train_dataset_dir\"],\n",
    "        preprocess_dataset_task.outputs[\"validation_dataset_dir\"],\n",
    "        train_specification,\n",
    "        train_parameters,\n",
    "        model_name=model_name,\n",
    "        gpus=training_gpus,\n",
    "        node_selector=training_node_selector,\n",
    "        cluster_configuration_secret=cluster_configuration_secret,\n",
    "        distribution_specification=distribution_specification,\n",
    "    )\n",
    "\n",
    "    evaluate_model_comp(\n",
    "        preprocess_dataset_task.outputs[\"test_dataset_dir\"],\n",
    "        train_model_task.outputs[\"model_dir\"],\n",
    "    )\n",
    "\n",
    "    plot_confusion_matrix_comp(\n",
    "        input_columns=[\"pixel_values\"],\n",
    "        label_columns=load_dataset_task.outputs[\"labels\"],\n",
    "        test_dataset_dir=preprocess_dataset_task.outputs[\"test_dataset_dir\"],\n",
    "        model_dir=train_model_task.outputs[\"model_dir\"],\n",
    "    )\n",
    "\n",
    "    convert_model_to_onnx_task = convert_model_to_onnx_comp(\n",
    "        train_model_task.outputs[\"model_dir\"]\n",
    "    )\n",
    "\n",
    "    upload_model_task = upload_model_comp(\n",
    "        convert_model_to_onnx_task.outputs[\"onnx_model_dir\"], model_name=model_name\n",
    "    )\n",
    "\n",
    "    deploy_model_task = deploy_model_comp(\n",
    "        name=model_name,\n",
    "        rm_existing=True,\n",
    "        storage_uri=\"s3://{{workflow.namespace}}-bee/onnx\",\n",
    "        minio_url=minio_url,\n",
    "        predictor_protocol=\"v2\",\n",
    "    )\n",
    "\n",
    "    deploy_model_task.after(upload_model_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a3bec0-144a-49d4-9c92-af5b84442231",
   "metadata": {},
   "source": [
    "## 4.) Run the pipeline within an experiment\n",
    "Create a pipeline run, using the client you initialized in a prior step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8a2cf14a-9d24-4919-ab2c-f93f99b5340d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See: https://www.kubeflow.org/docs/components/pipelines/overview/caching/#managing-caching-staleness\n",
    "def disable_cache_transformer(op):\n",
    "    if isinstance(op, dsl.ContainerOp):\n",
    "        op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    else:\n",
    "        op.add_pod_annotation(\n",
    "            name=\"pipelines.kubeflow.org/max_cache_staleness\", value=\"P0D\"\n",
    "        )\n",
    "    return op\n",
    "\n",
    "\n",
    "pipeline_conf = PipelineConf()\n",
    "pipeline_conf.add_op_transformer(disable_cache_transformer)\n",
    "pipeline_conf.data_passing_method = data_passing_methods.KubernetesVolume(\n",
    "    volume=V1Volume(\n",
    "        name=blackboard_pvc,\n",
    "        persistent_volume_claim=V1PersistentVolumeClaimVolumeSource(\n",
    "            \"{{workflow.name}}-%s\" % blackboard_pvc\n",
    "        ),\n",
    "    ),\n",
    "    path_prefix=f\"{blackboard_pvc}/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "be67671b-37d8-498d-b4bd-fd8f5866ee8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=/pipeline/#/pipelines/details/043700ae-10d8-47d5-b38d-2a184342dd53>Pipeline details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def delete_pipeline(pipeline_name: str):\n",
    "    \"\"\"Delete's a pipeline with the specified name\"\"\"\n",
    "\n",
    "    client = kfp.Client()\n",
    "    existing_pipelines = client.list_pipelines(page_size=999).pipelines\n",
    "    matches = (\n",
    "        [ep.id for ep in existing_pipelines if ep.name == pipeline_name]\n",
    "        if existing_pipelines\n",
    "        else []\n",
    "    )\n",
    "    for id in matches:\n",
    "        client.delete_pipeline(id)\n",
    "\n",
    "\n",
    "def get_experiment_id(experiment_name: str) -> str:\n",
    "    \"\"\"Returns the id for the experiment, creating the experiment if needed\"\"\"\n",
    "    client = kfp.Client()\n",
    "    existing_experiments = client.list_experiments(page_size=999).experiments\n",
    "    matches = (\n",
    "        [ex.id for ex in existing_experiments if ex.name == experiment_name]\n",
    "        if existing_experiments\n",
    "        else []\n",
    "    )\n",
    "\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "\n",
    "    exp = client.create_experiment(experiment_name)\n",
    "    return exp.id\n",
    "\n",
    "\n",
    "PIPELINE_NAME = \"Monkey Classification Pipeline\"\n",
    "delete_pipeline(PIPELINE_NAME)\n",
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=monkey_pipeline,\n",
    "    package_path=f\"{PIPELINE_NAME}.yaml\",\n",
    "    pipeline_conf=pipeline_conf,\n",
    ")\n",
    "uploaded_pipeline = client.upload_pipeline(f\"{PIPELINE_NAME}.yaml\", PIPELINE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2c1b3c-8e52-46e5-b35a-ef7b8f05cf82",
   "metadata": {},
   "source": [
    "## Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1914c3cd-4e08-459b-90ab-621fbe33d55d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/3e274369-312b-473b-9825-ceea5d064331\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/3b6e4172-0cf5-4d14-90ec-59cb2b56b602\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ARGUMENTS = {\n",
    "    \"dataset_url\": \"Lehrig/Monkey-Species-Collection\",\n",
    "    \"dataset_configuration\": \"downsized\",\n",
    "    \"dataset_label_columns\": [\"label\"],\n",
    "    \"model_name\": \"monkey-classification\",\n",
    "    \"cluster_configuration_secret\": \"\",\n",
    "    \"training_gpus\": \"1\",\n",
    "    \"number_of_workers\": \"1\",\n",
    "    \"distribution_type\": \"Job\",\n",
    "    \"training_node_selector\": \"\",\n",
    "}\n",
    "\n",
    "\n",
    "run = client.run_pipeline(\n",
    "    experiment_id=get_experiment_id(\"monkey-classification\"),\n",
    "    job_name=\"monkey-classification\",\n",
    "    pipeline_id=uploaded_pipeline.id,\n",
    "    params=ARGUMENTS,\n",
    ")\n",
    "\n",
    "# client.create_run_from_pipeline_func(\n",
    "#    monkey_pipeline,\n",
    "#    arguments=ARGUMENTS,\n",
    "#    namespace=NAMESPACE,\n",
    "#    pipeline_conf=pipeline_conf,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7d3ead-c191-47f6-8d7e-1a1bb258cf75",
   "metadata": {},
   "source": [
    "## Wait for completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82f6412-f754-41d9-abff-dfa82c5e22fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "TWENTY_MIN = 20 * 60\n",
    "result = client.wait_for_run_completion(run.id, timeout=TWENTY_MIN)\n",
    "{\n",
    "    \"status\": result.run.status,\n",
    "    \"error\": result.run.error,\n",
    "    \"time\": str(result.run.finished_at - result.run.created_at),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-dayton",
   "metadata": {},
   "source": [
    "## 5.) Test model deployment\n",
    "See API documentation: https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md\n",
    "\n",
    "### 5.1) Check model endpoint availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7df43a-b070-439c-af9a-a2f9a9017ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = f\"{MODEL_NAME}-predictor-default.{NAMESPACE}\"\n",
    "HEADERS = {\"Host\": HOST}\n",
    "MODEL_ENDPOINT = f\"http://{MODEL_NAME}-predictor-default/v2/models/{MODEL_NAME}\"\n",
    "\n",
    "res = requests.get(MODEL_ENDPOINT, headers=HEADERS)\n",
    "response = json.loads(res.text)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742a447c-de11-4938-976e-f272e7bdeed8",
   "metadata": {},
   "source": [
    "Note you can also do this:\n",
    "```curl -H \"Host: $HOST\" $MODEL_ENDPOINT```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3c55f8-8b15-4f8b-8e54-4b773dd18779",
   "metadata": {},
   "source": [
    "### 5.2) Get test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-gates",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE = \"Monkey.jpg\"\n",
    "# IMAGE_URL = \"https://upload.wikimedia.org/wikipedia/commons/0/00/Uakari_male.jpg\" #Uakari\n",
    "IMAGE_URL = \"https://upload.wikimedia.org/wikipedia/commons/thumb/8/80/Panamanian_Male_Adult_Howler_Monkey.jpg/2560px-Panamanian_Male_Adult_Howler_Monkey.jpg\"  # Howler\n",
    "# IMAGE_URL = \"https://upload.wikimedia.org/wikipedia/commons/c/c6/Patas_Monkey.jpg\" # Patas\n",
    "MONKEYS = np.array(\n",
    "    [\n",
    "        \"Mantled howler (Alouatta palliata)\",\n",
    "        \"Patas monkey (Erythrocebus patas)\",\n",
    "        \"Bald uakari (Cacajao calvus)\",\n",
    "        \"Japanese macaque (Macaca fuscata)\",\n",
    "        \"Pygmy marmoset (Cebuella pygmaea)\",\n",
    "        \"Colombian white-faced capuchin (Cebus capucinus)\",\n",
    "        \"Silvery marmoset (Mico argentatus)\",\n",
    "        \"Common squirrel monkey (Saimiri sciureus)\",\n",
    "        \"Black headed night monkey (Aotus nigriceps)\",\n",
    "        \"Nilgiri langur (Trachypithecus johnii)\",\n",
    "    ]\n",
    ")  # Note: last monkey after sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07acf81-d510-4965-b03e-18830415e3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget $IMAGE_URL -O $IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe5e4d4-0aff-4933-b102-68b03491a8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = mpimg.imread(IMAGE)\n",
    "image_plot = plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c73903-06c3-4c8c-a339-5c37c3c4553d",
   "metadata": {},
   "source": [
    "### 5.3) Predict monkey kind of test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f03f70c-8892-41c3-b3c0-38155bde2405",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICT_ENDPOINT = MODEL_ENDPOINT + \"/infer\"\n",
    "\n",
    "test_image = cv2.resize(cv2.imread(IMAGE), dsize=(224, 224)) / 255.0\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"inception_v3_input\",\n",
    "            \"shape\": [1, 224, 224, 3],\n",
    "            \"datatype\": \"FP32\",\n",
    "            \"data\": test_image.tolist(),\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "res = requests.post(PREDICT_ENDPOINT, headers=HEADERS, data=json.dumps(payload))\n",
    "response = json.loads(res.text)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a219d72-83de-42ef-89a8-ae78fb6a6877",
   "metadata": {},
   "source": [
    "Note you can also do this:\n",
    "```curl -s -X POST -H \"Host: $HOST\" -d @$JSON_FILE $PREDICT_ENDPOINT```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2c9fcb-3d5c-40ba-945f-7af6ec58278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Looks like a \" + MONKEYS[np.argmax(response[\"outputs\"][0][\"data\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f31c43-9cf1-49d0-9cd8-452f9877c570",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
