{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0d4c233-4a49-417f-aa3e-2896ffe8cc6b",
   "metadata": {},
   "source": [
    "# Large Language Model for Summarization\n",
    "\n",
    "This pipeline is largely inspired by [this](https://huggingface.co/docs/transformers/tasks/summarization) example from Hugging Face.\n",
    "\n",
    "The deployed model is used to summarize text. The t5-small LLM from google is used as a base model, and this is fine tuned using the billsum dataset.\n",
    "\n",
    "Some key things to notice about this pipeline.\n",
    "* Source code for training the model is stored in GitHub. This allows the train/eval/deploy process to be defined in the pipeline, while the procedure is defined in Git.  The separation allows an ML/OPs developer to work on the pipeline independently of a Data Scientist that is working on the implementation of the model. It also alows tools such as VSCode to be used to build more complex models than can be inlined.\n",
    "* Pipeline stages operate on a PVC. Early stages clone the git repo and setup a 'workspace'. Other steps cache data, such as preprocessed input data, in the PVC.\n",
    "* Training happens on the larger training GPUs.\n",
    "* The inference service is deployed on the smaller inference GPUs.\n",
    "* Pipeline parameters allow customization of the training hyperparamters. For example, the base model can be changed from t5-small to pegasus-xsum simply by running the pipeline from the UI with different parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3411c54f-3af8-4f6e-992a-0b3c0dc507c8",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "753ae334-3fdc-41df-90ea-adc3c36275ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp.components import InputPath, OutputPath\n",
    "from kfp import dsl\n",
    "from typing import List, Tuple, Dict\n",
    "from kfp.dsl import ContainerOp\n",
    "from kubernetes.client.models import V1EnvVar,V1EnvVarSource, V1SecretKeySelector,V1ConfigMapKeySelector\n",
    "from typing import NamedTuple\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8567a0-10ed-44b6-98ea-e6993e673f91",
   "metadata": {},
   "source": [
    "## Container Images\n",
    "\n",
    "This pipeline runs tasks using a customized container image that has a recent build of PyTorch for IBM Power.  It also containers additional support for the Hugging Face framework, and KServe.\n",
    "\n",
    "The predictor image is built on top of the base image and contains the custom predictor. It is much faster to rebuild the predictor if the base image already exists in the cluster. This is because the base image has all of the GPU libraries (and these are very large). Rebuilding the predictor does not require repackaging these large libraries because they are in the base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "425f3a27-aed5-425f-b6b8-05dbb57615ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BASE_IMAGE=\"quay.io/ntlawrence/summary:1.0.20\"\n",
    "PREDICTOR_IMAGE=\"quay.io/ntlawrence/summary-predictor:1.0.20\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff2dbe8-2c4a-4805-a161-3d5615f9530a",
   "metadata": {},
   "source": [
    "## Node Selectors\n",
    "\n",
    "An important observation of GPUs on Power 9 nodes is that each node has only one type of GPU. These constants are used to ensure that pipeline tasks or inference servers run on the correct type of node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dedfca1-3f7c-4a9d-9fc2-1b2c93ea6f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_NODE_LABEL='nvidia.com/gpu.product'\n",
    "INFERENCE_GPU_PRODUCT='Tesla-T4'\n",
    "TRAINING_GPU_PRODUCT='Tesla-V100-SXM2-32GB'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73db0ec3-0587-45fa-b138-0462e00e7ef6",
   "metadata": {},
   "source": [
    "## Define a component to run commands\n",
    "\n",
    "This pipeline uses python scripts to implement the preperation, training, and evaluation of the pipeline.\n",
    "\n",
    "This component allows a generic command to be run. The assumption is that the command will modify persistent storage. Future pipeline tasks can then retrieve relevant information from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "534509db-fba4-42bd-87b3-c77d42c27db9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_commands(commands: List[str], cwd: str):\n",
    "    import subprocess\n",
    "\n",
    "    for command in commands:\n",
    "        print(command)\n",
    "        subprocess.run(command, shell=True, cwd=cwd, check=True)\n",
    "\n",
    "\n",
    "run_commands_comp = kfp.components.create_component_from_func(\n",
    "    run_commands, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8650950b-2772-48df-a265-6ae9b5e20741",
   "metadata": {},
   "source": [
    "## Define a componet tensorboard monitoring component\n",
    "\n",
    "Hugging Face trainer support Tensorboard logging. This component allows us to monitor the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c274499-5ebd-4726-8ec9-fbe981e8d028",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIGURE_TENSORBOARD_COMPONENT = f\"{os.getenv('HOME')}/kubeflow-ppc64le-examples/configure_tensorboard_component/configure_tensorboard_component.yaml\"\n",
    "\n",
    "configure_tensorboard_comp = kfp.components.load_component_from_file(\n",
    "    CONFIGURE_TENSORBOARD_COMPONENT\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ee5599-09c5-4948-a847-fae23e61c4d1",
   "metadata": {},
   "source": [
    "## Define a component to evaluate the model\n",
    "\n",
    "This evaluates the model, rougeL and rouge2 metrics are reported on the KubeFlow UI as part of the KFP experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7008869-cdda-4a9d-940f-1a3cd49c76a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model_dir: str,\n",
    "                   dataset_dir: str,\n",
    "                   cwd: str) -> NamedTuple(\"EvaluationOutput\", [(\"mlpipeline_metrics\", \"Metrics\")]):\n",
    "    import subprocess\n",
    "    import json\n",
    "    from collections import namedtuple\n",
    "\n",
    "    subprocess.run((\"python eval.py \"\n",
    "                    f\"--prepared_dataset_dir={dataset_dir} \"\n",
    "                    f\"--model_dir={model_dir} \"\n",
    "                    f\"--results_json=/tmp/results.json \"\n",
    "                   ),\n",
    "                   shell=True,\n",
    "                   cwd=cwd,\n",
    "                   check=True)\n",
    "    \n",
    "    with open(\"/tmp/results.json\", \"r\") as f:\n",
    "        metrics = json.load(f)\n",
    "        \n",
    "    metrics = {\n",
    "        \"metrics\": [\n",
    "            {\"name\": \"rougeL\", \n",
    "             \"numberValue\": metrics[\"eval_rougeL\"],\n",
    "             \"format\": \"RAW\"},\n",
    "            {\"name\": \"rouge2\", \n",
    "             \"numberValue\": metrics[\"eval_rouge2\"],\n",
    "             \"format\": \"RAW\"},\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    out_tuple = namedtuple(\"EvaluationOutput\", [\"mlpipeline_metrics\"])\n",
    "    return out_tuple(json.dumps(metrics))\n",
    "    \n",
    "evaluate_model_comp = kfp.components.create_component_from_func(\n",
    "    func=evaluate_model, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8124267-606d-4f3c-882d-3dedd22fe0df",
   "metadata": {},
   "source": [
    "## Define a component to create the model archive\n",
    "\n",
    "Hugging Face models are stored in a directory and have many pieces. This component archives the directory so that it can be pushed up to MinIO as a single file.\n",
    "\n",
    "One point to be aware of is that the path inside the tar much match the expected path used by the inference service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f19facd-394f-43df-8f10-51c0e3e8fad3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model_archive(model_dir: str,\n",
    "                         archive: OutputPath(str),\n",
    "                         model_name: str = \"billsum\",\n",
    "                         version: str = \"1\"):\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    import tarfile\n",
    "\n",
    "    os.makedirs(Path(archive).parent.absolute(), exist_ok=True)\n",
    "\n",
    "    with tarfile.open(name=archive, mode=\"w:gz\") as f:\n",
    "        for file in Path(model_dir).rglob(\"*\"):\n",
    "            if not file.is_dir():\n",
    "                f.add(file.absolute(), arcname=f\"{version}/{model_name}/{file.relative_to(model_dir)}\")\n",
    "\n",
    "create_model_archive_comp = kfp.components.create_component_from_func(\n",
    "    func=create_model_archive, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b2972c-07c2-4149-ac96-65f2570f569f",
   "metadata": {},
   "source": [
    "## Define a component to upload the model to MinIO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41258d64-0a20-46cd-aa95-0a87623ddc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_archive(\n",
    "    archive: InputPath(str),\n",
    "    archive_name: str,\n",
    "    service_name: str,\n",
    "    minio_url: str = \"minio-service.kubeflow:9000\",\n",
    "    version: str = \"1\",\n",
    ") -> NamedTuple(\"UploadOutput\", [(\"s3_address\", str)]):\n",
    "    \"\"\"Uploads a model file to MinIO artifact store.\"\"\"\n",
    "\n",
    "    from collections import namedtuple\n",
    "    import logging\n",
    "    from minio import Minio\n",
    "    import sys\n",
    "    import tarfile\n",
    "    import os\n",
    "\n",
    "    logging.basicConfig(\n",
    "        stream=sys.stdout,\n",
    "        level=logging.INFO,\n",
    "        format=\"%(levelname)s %(asctime)s: %(message)s\",\n",
    "    )\n",
    "    logger = logging.getLogger()\n",
    "\n",
    "\n",
    "    minio_client = Minio(\n",
    "            minio_url, \n",
    "            access_key=os.environ[\"MINIO_ID\"], \n",
    "            secret_key=os.environ[\"MINIO_PWD\"], secure=False\n",
    "        )\n",
    "\n",
    "    # Create export bucket if it does not yet exist\n",
    "    export_bucket=\"{{workflow.namespace}}\"\n",
    "    existing_bucket = next(filter(lambda bucket: bucket.name == export_bucket, minio_client.list_buckets()), None)\n",
    "\n",
    "    if not existing_bucket:\n",
    "        logger.info(f\"Creating bucket '{export_bucket}'...\")\n",
    "        minio_client.make_bucket(bucket_name=export_bucket)\n",
    "\n",
    "    path = f\"{service_name}/{version}/{archive_name}\"\n",
    "    s3_address = f\"s3://{export_bucket}/{service_name}\"\n",
    "\n",
    "    logger.info(f\"Saving tar file to MinIO (s3 address: {s3_address})...\")\n",
    "    minio_client.fput_object(\n",
    "        bucket_name=export_bucket,  # bucket name in Minio\n",
    "        object_name=path,  # file name in bucket of Minio \n",
    "        file_path=archive,  # file path / name in local system\n",
    "    )\n",
    "\n",
    "    logger.info(\"Finished.\")\n",
    "    out_tuple = namedtuple(\"UploadOutput\", [\"s3_address\"])\n",
    "    return out_tuple(s3_address)\n",
    "\n",
    "\n",
    "upload_archive_comp = kfp.components.create_component_from_func(\n",
    "    func=upload_archive, base_image=BASE_IMAGE, packages_to_install=[\"minio==7.1.13\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb5ed3e-124c-4716-b364-833a81a8d95f",
   "metadata": {},
   "source": [
    "## Define a component to deploy the inference service\n",
    "\n",
    "The deployment needs to be able to:\n",
    "* Run on a node with INFERENCE GPUs (we use a node selector for this)\n",
    "* Use serverless technology so that the GPU is not held forever (min replicas = 0)\n",
    "* Run under a service account with limited capabilities\n",
    "* Replace an existing inference service (handle conflict on creation)\n",
    "\n",
    "The component uses the predictor image, because the predictor image includes the KServe SDK, which we use to create the template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "436e4f8a-b2cc-417c-908a-54d42788ec6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def deploy_inference_service(name: str,\n",
    "                             version: int,\n",
    "                             model_archive_s3: str,\n",
    "                             predictor_image: str,\n",
    "                             predictor_max_replicas: int = 1,\n",
    "                             predictor_min_replicas: int = 0,\n",
    "                             predictor_concurrency_target: int = None,\n",
    "                             predictor_node_selector: Dict[str, str] = None,\n",
    "                             prefix: str = \"\",\n",
    "                             suffix: str = \"\",\n",
    "                            ):\n",
    "    import kserve\n",
    "    from kubernetes import client, config\n",
    "    from kubernetes.client import (V1ServiceAccount, \n",
    "                                   V1Container, \n",
    "                                   V1EnvVar, \n",
    "                                   V1ObjectMeta, \n",
    "                                   V1ContainerPort, \n",
    "                                   V1ObjectReference,\n",
    "                                   V1ResourceRequirements\n",
    "                                  )\n",
    "    from kserve import KServeClient\n",
    "    from kserve import constants\n",
    "    from kserve import V1beta1PredictorSpec\n",
    "    from kserve import V1beta1ExplainerSpec\n",
    "    from kserve import V1beta1TransformerSpec\n",
    "    from kserve import V1beta1InferenceServiceSpec\n",
    "    from kserve import V1beta1InferenceService\n",
    "    import json\n",
    "    from http import HTTPStatus\n",
    "    import logging\n",
    "    import yaml\n",
    "    from time import sleep\n",
    "\n",
    "\n",
    "    config.load_incluster_config()\n",
    "    \n",
    "    \n",
    "    SERVICE_ACCOUNT = \"summary-inference-sa\"\n",
    "\n",
    "    sa = V1ServiceAccount(\n",
    "        api_version=\"v1\",\n",
    "        kind=\"ServiceAccount\",\n",
    "        metadata=V1ObjectMeta(name=SERVICE_ACCOUNT, \n",
    "                              namespace=\"{{workflow.namespace}}\"),\n",
    "        secrets=[V1ObjectReference(name=\"minio-credentials\")]\n",
    "    )\n",
    "    corev1 = client.CoreV1Api()\n",
    "    \n",
    "    try:\n",
    "        corev1.create_namespaced_service_account(namespace=\"{{workflow.namespace}}\",\n",
    "                                                 body=sa)\n",
    "    except client.exceptions.ApiException as e:\n",
    "        if e.status == HTTPStatus.CONFLICT:\n",
    "            corev1.patch_namespaced_service_account(name=SERVICE_ACCOUNT,\n",
    "                                                    namespace=\"{{workflow.namespace}}\",\n",
    "                                                    body=sa)\n",
    "        else:\n",
    "            raise\n",
    "    \n",
    "    if prefix:\n",
    "        prefix = prefix + \" \"\n",
    "    if suffix:\n",
    "        suffix = \" \" + suffix\n",
    "        \n",
    "    predictor_spec = V1beta1PredictorSpec(\n",
    "        max_replicas=predictor_max_replicas,\n",
    "        min_replicas=predictor_min_replicas,\n",
    "        scale_target=predictor_concurrency_target,\n",
    "        scale_metric=\"concurrency\",\n",
    "        containers=[\n",
    "            V1Container(\n",
    "                name=\"kserve-container\",\n",
    "                image=predictor_image,\n",
    "                args=[\"python\", \n",
    "                      \"inference_service.py\", \n",
    "                      f\"--model_name={name}\", \n",
    "                      f\"--model_version={version}\",\n",
    "                     ],\n",
    "\n",
    "                resources=V1ResourceRequirements(\n",
    "                    limits={\"memory\": \"50Gi\"},\n",
    "                    requests={\"memory\": \"2Gi\"},\n",
    "                ),\n",
    "                env=[\n",
    "                 V1EnvVar(\n",
    "                     name=\"STORAGE_URI\", value=model_archive_s3\n",
    "                 ),\n",
    "                 V1EnvVar(\n",
    "                     name=\"PREFIX\", value=prefix\n",
    "                 ),\n",
    "                 V1EnvVar(\n",
    "                     name=\"SUFFIX\", value=suffix\n",
    "                 )\n",
    "                ],\n",
    "            )\n",
    "        ],\n",
    "        service_account_name=SERVICE_ACCOUNT,\n",
    "        node_selector=predictor_node_selector\n",
    "    )\n",
    "\n",
    "    inference_service = V1beta1InferenceService(\n",
    "        api_version=constants.KSERVE_V1BETA1,\n",
    "        kind=constants.KSERVE_KIND,\n",
    "        metadata=V1ObjectMeta(name=name, \n",
    "                              namespace=\"{{workflow.namespace}}\",\n",
    "                              annotations={\"sidecar.istio.io/inject\": \"false\",\n",
    "                                           \"serving.kserve.io/enable-prometheus-scraping\" : \"true\"}),\n",
    "        spec=V1beta1InferenceServiceSpec(predictor=predictor_spec)\n",
    "    )\n",
    "    # serving.kserve.io/inferenceservice: credit-risk\n",
    "    logging.info(\n",
    "        yaml.dump(\n",
    "            client.ApiClient().sanitize_for_serialization(inference_service)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # KServeClient doesn't throw ApiException for CONFLICT\n",
    "    # Using the k8s API directly for the create\n",
    "    api_instance = client.CustomObjectsApi()\n",
    "        \n",
    "    while True:\n",
    "        try:\n",
    "            api_instance.create_namespaced_custom_object(\n",
    "                    group=constants.KSERVE_GROUP,\n",
    "                    version=inference_service.api_version.split(\"/\")[1],\n",
    "                    namespace=\"{{workflow.namespace}}\",\n",
    "                    plural=constants.KSERVE_PLURAL,\n",
    "                    body=inference_service)\n",
    "            break\n",
    "        except client.exceptions.ApiException as api_exception:\n",
    "            if api_exception.status==HTTPStatus.CONFLICT:\n",
    "                try:\n",
    "                    api_instance.delete_namespaced_custom_object(\n",
    "                        group=constants.KSERVE_GROUP,\n",
    "                        version=inference_service.api_version.split(\"/\")[1],\n",
    "                        namespace=\"{{workflow.namespace}}\",\n",
    "                        plural=constants.KSERVE_PLURAL,\n",
    "                        name=name)\n",
    "                    sleep(15)\n",
    "                except client.exceptions.ApiException as api_exception2:\n",
    "                    if api_exception2.status in {HTTPStatus.NOT_FOUND, HTTPStatus.GONE}:\n",
    "                        pass\n",
    "                    else:\n",
    "                        raise\n",
    "\n",
    "            else:\n",
    "                raise\n",
    "            \n",
    "    kclient = KServeClient()\n",
    "    kclient.wait_isvc_ready(name=name, namespace=\"{{workflow.namespace}}\")\n",
    "    \n",
    "    if not kclient.is_isvc_ready(name=name, namespace=\"{{workflow.namespace}}\"):\n",
    "        raise RuntimeError(f\"The inference service {name} is not ready!\")\n",
    "\n",
    "deploy_inference_service_comp = kfp.components.create_component_from_func(\n",
    "    func=deploy_inference_service, base_image=PREDICTOR_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f053fb98-f672-43f2-afd8-c1234bc44d6c",
   "metadata": {},
   "source": [
    "## Define the pipeline\n",
    "\n",
    "In this notebook, we'll compile and upload the pipeline, and then we'll run it programatically.\n",
    "\n",
    "However once defined, it is also possible to run the pipeline from the UI, and change the parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8b9d085-8742-46a9-9288-a86781e737b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PIPELINE_NAME = \"summarize\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97a3c990-1c35-405b-bd3e-ed2a63ef3d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=PIPELINE_NAME)\n",
    "def summarize_pipeline(\n",
    "    source_repo: str = \"https://github.com/ntl-ibm/kubeflow-ppc64le-examples.git\",\n",
    "    source_branch: str = \"3.0.0\",\n",
    "    source_context: str = \"natural-language-processing/huggingface-summarization/src\",\n",
    "    minio_endpoint=\"minio-service.kubeflow:9000\",\n",
    "    checkpoint: str=\"t5-small\",\n",
    "    model_max_length: int = 512,\n",
    "    model_version: int = 1,\n",
    "    epochs: int = 3,\n",
    "    model_name: str = \"billsum\",\n",
    "    prefix: str = \"summarize: \",\n",
    "    suffix: str = \"\"\n",
    "):\n",
    "    \n",
    "    def mount_volume(task, pvc_name, mount_path, volume_subpath, read_only=False):\n",
    "        \"\"\" This function mounts a volume to a task \"\"\"\n",
    "        task.add_volume(\n",
    "            V1Volume(\n",
    "                name=pvc_name,\n",
    "                persistent_volume_claim=V1PersistentVolumeClaimVolumeSource(pvc_name),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        task.add_volume_mount(\n",
    "            V1VolumeMount(\n",
    "                name=pvc_name,\n",
    "                mount_path=mount_path,\n",
    "                sub_path=volume_subpath,\n",
    "                read_only=read_only,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    def env_var_from_secret(env_var_name: str, secret_name: str, secret_key: str) -> V1EnvVar:\n",
    "        \"\"\" This function addas an enviroment variable (stored in a secret) to a task \"\"\"\n",
    "        return V1EnvVar(name=env_var_name,\n",
    "                                     value_from=V1EnvVarSource(\n",
    "                                         secret_key_ref=V1SecretKeySelector(\n",
    "                                             name=secret_name,\n",
    "                                             key=secret_key\n",
    "                                         )\n",
    "                                     )\n",
    "                                    )\n",
    "\n",
    "    ### Create the workspace\n",
    "    workspace_volume_volop = dsl.VolumeOp(\n",
    "        name=\"Create workspace\",\n",
    "        resource_name=\"shared-workspace-pvc\",\n",
    "        modes=dsl.VOLUME_MODE_RWM,\n",
    "        size=\"4Gi\",\n",
    "        set_owner_reference=True,\n",
    "    )\n",
    "\n",
    "    ### Clone the repo\n",
    "    clone_repo_task = run_commands_comp(\n",
    "        [\n",
    "            f\"git clone {source_repo}  /workspace/repo -b {source_branch} || true\"\n",
    "        ],\n",
    "        \"/workspace\",\n",
    "    )\n",
    "    clone_repo_task.add_pvolumes({\"/workspace\": workspace_volume_volop.volume})\n",
    "    clone_repo_task.set_display_name(\"Clone Repo\")\n",
    "    \n",
    "    \n",
    "    ### Prepare the training data\n",
    "    preprocess_data_task = run_commands_comp(\n",
    "        [(\"python prepare.py \" +\n",
    "           f\"--checkpoint={checkpoint} \" +\n",
    "            \"--prepared_dataset_dir=/workspace/dataset \" +\n",
    "           f\"--model_max_len={model_max_length} \" +\n",
    "           (f\"--prefix={prefix} \" if prefix else \"\") +\n",
    "           (f\"--suffix={suffix} \" if suffix else \"\")\n",
    "         )\n",
    "        ],\n",
    "        f\"/workspace/repo/{source_context}\"\n",
    "    )\n",
    "    preprocess_data_task.add_pvolumes({\"/workspace\": workspace_volume_volop.volume})\n",
    "    preprocess_data_task.after(clone_repo_task)\n",
    "    preprocess_data_task.set_display_name(\"Load and Preprocess data\")\n",
    "    \n",
    "    \n",
    "    ### Setup the tensorboard\n",
    "    create_tensorboard_volume = dsl.VolumeOp(\n",
    "        name=f\"Create PVC for tensorboard\",\n",
    "        resource_name=\"tensorboard\",\n",
    "        modes=dsl.VOLUME_MODE_RWM,\n",
    "        size=\"4G\",\n",
    "        set_owner_reference=True,\n",
    "    )\n",
    "    \n",
    "    configure_tensorboard_task = configure_tensorboard_comp(\n",
    "        pipeline_name=PIPELINE_NAME,\n",
    "        pvc_name=create_tensorboard_volume.volume.persistent_volume_claim.claim_name\n",
    "    )\n",
    "    \n",
    "    ### Train the model\n",
    "    ### This task has adjusted the amount of available memory and CPU\n",
    "    train_model_task = run_commands_comp(\n",
    "        [(\"python train.py \"\n",
    "           f\"--checkpoint={checkpoint} \"\n",
    "            \"--prepared_dataset_dir=/workspace/dataset \"\n",
    "            \"--model_dir=/workspace/billsum \"\n",
    "            \"--tensorboard=/tensorboard \"\n",
    "            f\"--epochs={epochs}\"\n",
    "         )\n",
    "        ],\n",
    "        f\"/workspace/repo/{source_context}\"\n",
    "    )\n",
    "    train_model_task.add_pvolumes({\"/workspace\": workspace_volume_volop.volume,\n",
    "                                   \"/tensorboard\": create_tensorboard_volume.volume\n",
    "                                  })\n",
    "    train_model_task.after(preprocess_data_task)\n",
    "    train_model_task.after(configure_tensorboard_task)\n",
    "    train_model_task.set_display_name(\"Train Model\")\n",
    "    train_model_task.set_gpu_limit(1)\n",
    "    train_model_task.set_cpu_limit('1')\n",
    "    train_model_task.set_memory_request('40G')\n",
    "    train_model_task.set_memory_limit('1024G')\n",
    "    train_model_task.add_node_selector_constraint(label_name=GPU_NODE_LABEL,\n",
    "                                                  value=TRAINING_GPU_PRODUCT)\n",
    "\n",
    "    ### Evaluate the model\n",
    "    evaluate_model_task = evaluate_model_comp(model_dir=\"/workspace/billsum\",\n",
    "                                              dataset_dir=\"/workspace/dataset\",\n",
    "                                              cwd=f\"/workspace/repo/{source_context}\")\n",
    "    evaluate_model_task.add_pvolumes({\"/workspace\": workspace_volume_volop.volume})\n",
    "    evaluate_model_task.after(train_model_task)\n",
    "    evaluate_model_task.set_display_name(\"Evaluate Model\")\n",
    "    evaluate_model_task.set_gpu_limit(1)\n",
    "\n",
    "    ### Create a tar file for the model\n",
    "    create_archive_task = create_model_archive_comp(model_dir=f\"/workspace/{model_name}\", \n",
    "                                                    model_name=model_name)\n",
    "    create_archive_task.add_pvolumes({\"/workspace\": workspace_volume_volop.volume})\n",
    "    create_archive_task.after(evaluate_model_task)\n",
    "    create_archive_task.set_display_name(\"create archive\")\n",
    "    \n",
    "    ### Upload model to MinIO\n",
    "    upload_archive_task = upload_archive_comp(\n",
    "        archive = create_archive_task.outputs[\"archive\"],\n",
    "        archive_name = f\"{model_name}.tar\",\n",
    "        service_name=model_name\n",
    "    )\n",
    "    upload_archive_task.container.add_env_variable(env_var_from_secret(\"MINIO_ID\", \"mlpipeline-minio-artifact\", \"accesskey\"))\n",
    "    upload_archive_task.container.add_env_variable(env_var_from_secret(\"MINIO_PWD\", \"mlpipeline-minio-artifact\", \"secretkey\"))\n",
    "    upload_archive_task.after(create_archive_task)\n",
    "\n",
    "    \n",
    "    ### Deploy the inference service\n",
    "    ### Make the model scale to 0, so that GPUs are not used for an idle service!\n",
    "    ### Make sure that the model runs on an inference node, not a training node\n",
    "    deploy_model_task = deploy_inference_service_comp(name=model_name,\n",
    "                                                      version=1,\n",
    "                                                      model_archive_s3=upload_archive_task.outputs[\"s3_address\"],\n",
    "                                                      predictor_image=PREDICTOR_IMAGE,\n",
    "                                                      prefix = prefix,\n",
    "                                                      suffix = suffix,\n",
    "                                                      predictor_min_replicas=0,\n",
    "                                                      predictor_node_selector={GPU_NODE_LABEL: INFERENCE_GPU_PRODUCT}\n",
    "                                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebb5108-5fd4-43eb-9e5b-1afafd462a18",
   "metadata": {},
   "source": [
    "### Define the configuration for the pipeline\n",
    "\n",
    "The configuration disables caching for all tasks. This pipeline depends on data stored on disk, rather than data passed with parameters. That difference makes the caching less valuable and more problematic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41c21543-2987-486d-ae1e-608bbc737b73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_conf = kfp.dsl.PipelineConf()\n",
    "\n",
    "# Disable Caching\n",
    "def disable_cache_transformer(op: dsl.ContainerOp):\n",
    "    if isinstance(op, dsl.ContainerOp):\n",
    "        op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    else:\n",
    "        op.add_pod_annotation(\n",
    "            name=\"pipelines.kubeflow.org/max_cache_staleness\", value=\"P0D\"\n",
    "        )\n",
    "    return op\n",
    "\n",
    "\n",
    "pipeline_conf.add_op_transformer(disable_cache_transformer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d83423c-4173-44c0-bb9a-373bcdcf8dd2",
   "metadata": {},
   "source": [
    "## Define helper functions to delete an existing pipeline and create or reuse an experiment\n",
    "\n",
    "Pipelines run within an experiment, so we'll need to make sure one exists for the run. Pipelines must have unique names, so we'll need to delete this pipeline if we created it as part of a previous notebook run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d30eea98-851a-43fd-8117-c2f97114b13f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def delete_pipeline(pipeline_name: str):\n",
    "    \"\"\"Delete's a pipeline with the specified name\"\"\"\n",
    "\n",
    "    client = kfp.Client()\n",
    "    existing_pipelines = client.list_pipelines(page_size=999).pipelines\n",
    "    matches = (\n",
    "        [ep.id for ep in existing_pipelines if ep.name == pipeline_name]\n",
    "        if existing_pipelines\n",
    "        else []\n",
    "    )\n",
    "    for id in matches:\n",
    "        client.delete_pipeline(id)\n",
    "\n",
    "\n",
    "def get_experiment_id(experiment_name: str) -> str:\n",
    "    \"\"\"Returns the id for the experiment, creating the experiment if needed\"\"\"\n",
    "    client = kfp.Client()\n",
    "    existing_experiments = client.list_experiments(page_size=999).experiments\n",
    "    matches = (\n",
    "        [ex.id for ex in existing_experiments if ex.name == experiment_name]\n",
    "        if existing_experiments\n",
    "        else []\n",
    "    )\n",
    "\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "\n",
    "    exp = client.create_experiment(experiment_name)\n",
    "    return exp.id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f841c9-163f-407b-bdec-2abc4464552b",
   "metadata": {},
   "source": [
    "## Compile, upload, and run the pipeline.\n",
    "\n",
    "compile() creates a yaml file for the pipeline. \n",
    "\n",
    "upload_pipeline() creates the pipeline in Kubeflow from the yaml.\n",
    "\n",
    "run_pipeline() invokes the pipeline. The pipeline runs within an experiment, pipeline metrics are reported in the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c2c9cff-452e-4255-a8c5-122055aa7f15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=/pipeline/#/pipelines/details/e19dc0d0-df3f-457e-ba44-17a3ae088440>Pipeline details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/0b656dfe-312d-4d75-80f6-a11a87c91a1e\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PIPELINE_NAME = \"summarize\"\n",
    "\n",
    "client = kfp.Client()\n",
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=summarize_pipeline,\n",
    "    package_path=f\"{PIPELINE_NAME}.yaml\",\n",
    "    pipeline_conf=pipeline_conf,\n",
    ")\n",
    "\n",
    "delete_pipeline(PIPELINE_NAME)\n",
    "uploaded_pipeline = client.upload_pipeline(f\"{PIPELINE_NAME}.yaml\", PIPELINE_NAME)\n",
    "run = client.run_pipeline(\n",
    "    experiment_id=get_experiment_id(\"summarize-exp\"),\n",
    "    job_name=\"summarize\",\n",
    "    pipeline_id=uploaded_pipeline.id,\n",
    "    params={},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744e5d63-535d-40d7-b559-47b2fbffdeb7",
   "metadata": {},
   "source": [
    "## Wait for pipeline completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6fffc0-5fef-42a4-a024-ad64b1f28bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "TWENTY_MIN = 20 * 60\n",
    "result = client.wait_for_run_completion(run.id, timeout=TWENTY_MIN)\n",
    "{\n",
    "    \"status\": result.run.status,\n",
    "    \"error\": result.run.error,\n",
    "    \"time\": str(result.run.finished_at - result.run.created_at),\n",
    "    \"metrics\": result.run.metrics,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d317792e-0ed1-441d-9d12-ba323865e22d",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc10906-cb91-42ad-afc2-b43f9484c92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import subprocess\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b0c3f9-4b8f-49d0-a18e-d13f102648e4",
   "metadata": {},
   "source": [
    "## Get service URL\n",
    "\n",
    "The URL as it is retreived here is a cluster local URL, meaning it can only be used within the cluster. The format for the URL's path uses the V1 version of the KServe API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46239ca5-1587-416e-b914-a891e68804bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p = subprocess.run(\n",
    "    \"oc get inferenceservice billsum -o yaml\", shell=True, stdout=subprocess.PIPE\n",
    ")\n",
    "\n",
    "desc = yaml.safe_load(p.stdout)\n",
    "url = desc[\"status\"][\"address\"][\"url\"]\n",
    "url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f58422f-0ee6-4e10-8136-5397292cc1e5",
   "metadata": {},
   "source": [
    "## Define some text to summarize\n",
    "\n",
    "We'll print it word-wrapped so that readers of this notebook can read it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4069abb3-aeb1-47a5-80cd-92570f5c3e91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"The Inflation Reduction Act lowers prescription drug costs, health care costs, and energy costs.  It's the most aggressive action on tackling the climate crisis in American history, which will lift up American workers and create good-paying, union jobs across the country. It'll lower the deficit and ask the ultra-wealthy and corporations to pay their fair share. And no one making under $400,000 per year will pay a penny more in taxes.\"\n",
    "  \n",
    "import textwrap\n",
    "\n",
    "for line in textwrap.wrap(text, width=70):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2adafc-25cf-450d-9291-50c7cb233ce5",
   "metadata": {},
   "source": [
    "## Request a summary\n",
    "\n",
    "This might take a little while. If the service has not been used for a while, it will scale down. This means when a rest request is made to it, the pod must be started again. When the pod starts, the model must be reloaded.\n",
    "\n",
    "Repeated requests will be much faster, since the model is already warmed up. When the model is not used for a while, it will scale back down automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20e7652-8449-41cc-b852-f115111e2407",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "r = requests.post(url,\n",
    "              json=\n",
    "                  {\"instances\" : [text]}\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66560ea-2e7b-4767-bbd5-cb16394c75f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in textwrap.wrap(r.json()[\"summary\"], width=70):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea659c6-970c-45d5-8131-d14111ce79d8",
   "metadata": {},
   "source": [
    "# Additional comments\n",
    "\n",
    "The t5 model tends to 'extract' information to use in the summary. Another approach would be to use the google/pegasus-xsum model as a base. This model was initially trained to predict missing sentences, which allows the summary to be more expressive.\n",
    "\n",
    "Any model based on LLMs can result in halucinations. These are cases where the model produces text that looks a good solution, but is factually not correct."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
