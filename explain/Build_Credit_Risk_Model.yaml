apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: credit-risk-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.18, pipelines.kubeflow.org/pipeline_compilation_time: '2023-12-05T19:12:34.661526',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "An example pipeline that
      builds and deploys a credit risk model", "name": "Credit Risk"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.18}
spec:
  entrypoint: credit-risk
  templates:
  - name: configure-tensorboard
    container:
      args: [--pvc-name, '{{inputs.parameters.create-pvc-for-tensorboard-name}}',
        --pvc-path, '', --tensorboard-name, '', --mlpipeline-ui-metadata, /tmp/outputs/mlpipeline_ui_metadata/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'kubernetes' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'kubernetes' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef configure_tensorboard(\n    mlpipeline_ui_metadata_path,\n    pvc_name,\n\
        \    pvc_path = \"\",\n    tensorboard_name = \"\",\n):\n    \"\"\"\n    Monitors\
        \ a training job based on Tensorboard logs. \n    Logs are expected to be\
        \ written to the specified subpath of the pvc\n    \"\"\"\n    from collections\
        \ import namedtuple\n    import json\n    from kubernetes import client, config,\
        \ watch\n    import logging\n    import sys\n    import os\n    import yaml\n\
        \    import textwrap\n    import json\n    import http\n\n    logging.basicConfig(\n\
        \        stream=sys.stdout,\n        level=logging.INFO,\n        format=\"\
        %(levelname)s %(asctime)s: %(message)s\",\n    )\n    logger = logging.getLogger()\n\
        \n    if not tensorboard_name:\n        tensorboard_name=\"{{workflow.name}}\"\
        \n\n    namespace=\"{{workflow.namespace}}\"\n\n    config.load_incluster_config()\n\
        \    api_client = client.ApiClient()\n    apps_api = client.AppsV1Api(api_client)\n\
        \    custom_object_api = client.CustomObjectsApi(api_client)\n\n    # Delete\
        \ possible existing tensorboard\n    try:\n        custom_object_api.delete_namespaced_custom_object(\n\
        \            group=\"tensorboard.kubeflow.org\",\n            version=\"v1alpha1\"\
        ,\n            plural=\"tensorboards\",\n            namespace=namespace,\n\
        \            name=tensorboard_name,\n            body=client.V1DeleteOptions()\n\
        \        )\n    except client.exceptions.ApiException as e:\n        if e.status\
        \ != http.HTTPStatus.NOT_FOUND:\n            raise\n\n    tensorboard_spec\
        \ = textwrap.dedent(f'''\\\n            apiVersion: tensorboard.kubeflow.org/v1alpha1\n\
        \            kind: Tensorboard\n            metadata:\n              name:\
        \ \"{tensorboard_name}\"\n              namespace: \"{namespace}\"\n     \
        \         ownerReferences:\n                - apiVersion: v1\n           \
        \       kind: Workflow\n                  name: \"{{workflow.name}}\"\n  \
        \                uid: \"{{workflow.uid}}\"\n            spec:\n          \
        \    logspath: \"pvc://{pvc_name}/{pvc_path}\"\n            '''\n    )\n\n\
        \    logger.info(tensorboard_spec)\n\n    custom_object_api.create_namespaced_custom_object(\n\
        \        group=\"tensorboard.kubeflow.org\",\n        version=\"v1alpha1\"\
        ,\n        plural=\"tensorboards\",\n        namespace=namespace,\n      \
        \  body=yaml.safe_load(tensorboard_spec),\n        pretty=True)\n\n    tensorboard_watch\
        \ = watch.Watch()\n    try:\n        for tensorboard_event in tensorboard_watch.stream(\n\
        \            custom_object_api.list_namespaced_custom_object,\n          \
        \  group=\"tensorboard.kubeflow.org\",\n            version=\"v1alpha1\",\n\
        \            plural=\"tensorboards\",\n            namespace=namespace,\n\
        \            field_selector=f\"metadata.name={tensorboard_name}\",\n     \
        \       timeout_seconds=0,\n        ):\n\n            logger.info(f\"tensorboard_event:\
        \ {json.dumps(tensorboard_event, indent=2)}\")\n\n            if tensorboard_event[\"\
        type\"]==\"DELETED\":\n                raise RuntimeError(\"The tensorboard\
        \ was deleted!\")\n\n            tensorboard = tensorboard_event[\"object\"\
        ]\n\n            if \"status\" not in tensorboard:\n                continue\n\
        \n            deployment_state = \"Progressing\"\n            if \"conditions\"\
        \ in tensorboard[\"status\"]:\n                deployment_state = tensorboard[\"\
        status\"][\"conditions\"][-1][\n                    \"deploymentState\"\n\
        \                ]\n\n            if deployment_state == \"Progressing\":\n\
        \                logger.info(\"Tensorboard deployment is progressing...\"\
        )\n            elif deployment_state == \"Available\":\n                logger.info(\"\
        Tensorboard deployment is Available.\")\n                break\n         \
        \   elif deployment_state == \"ReplicaFailure\":\n                raise RuntimeError(\"\
        Tensorboard deployment failed with a ReplicaFailure!\")\n            else:\n\
        \                raise RuntimeError(f\"Unknown deployment state: {deployment_state}\"\
        )\n    finally:\n        tensorboard_watch.stop()\n\n    button_style =(\n\
        \        \"align-items: center; \"\n        \"appearance: none; \"\n     \
        \   \"background-color: rgb(26, 115, 232); \"\n        \"border: 0px none\
        \ rgb(255, 255, 255); \"\n        \"border-radius: 3px; \"\n        \"box-sizing:\
        \ border-box; \"\n        \"color: rgb(255, 255, 255); \"\n        \"cursor:\
        \ pointer; \"\n        \"display: inline-flex; \" \n        \"font-family:\
        \ 'Google Sans', 'Helvetica Neue', sans-serif; \"\n        \"font-size: 14px;\
        \ \"\n        \"font-stretch: 100%; \"\n        \"font-style: normal; font-weight:\
        \ 700; \"\n        \"justify-content: center; \"\n        \"letter-spacing:\
        \ normal; \"\n        \"line-height: 24.5px; \"\n        \"margin: 0px 10px\
        \ 2px 0px; \"\n        \"min-height: 25px; \"\n        \"min-width: 64px;\
        \ \"\n        \"padding: 2px 6px 2px 6px; \"\n        \"position: relative;\
        \ \"\n        \"tab-size: 4; \"\n        \"text-align: center; \"\n      \
        \  \"text-indent: 0px; \"\n        \"text-rendering: auto; \"\n        \"\
        text-shadow: none; \"\n        \"text-size-adjust: 100%; \"\n        \"text-transform:\
        \ none; \"\n        \"user-select: none; \"\n        \"vertical-align: middle;\
        \ \"\n        \"word-spacing: 0px; \"\n        \"writing-mode: horizontal-tb;\"\
        \n    )\n\n    # See: https://github.com/kubeflow/kubeflow/blob/master/components/crud-web-apps/tensorboards/frontend/src/app/pages/index/index.component.ts\n\
        \    # window.open(`/tensorboard/${tensorboard.namespace}/${tensorboard.name}/`);\n\
        \    ui_address = f\"/tensorboard/{namespace}/{tensorboard_name}/#scalars\"\
        \n\n    markdown = textwrap.dedent(\n        f'''\\\n        # Tensorboard\n\
        \        - <a href=\"{ui_address}\" style=\"{button_style}\" target=\"_blank\"\
        >Connect</a>\n        - <a href=\"/_/tensorboards/\" style=\"{button_style}\"\
        \ target=\"_blank\">Manage all</a>\n        '''\n    )\n\n    markdown_output\
        \ = {\n        \"type\": \"markdown\",\n        \"storage\": \"inline\",\n\
        \        \"source\": markdown,\n    }\n\n    ui_metadata = {\"outputs\": [markdown_output]}\n\
        \    with open(mlpipeline_ui_metadata_path, \"w\") as metadata_file:\n   \
        \     json.dump(ui_metadata, metadata_file)\n\n    logging.info(\"Finished.\"\
        )\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Configure tensorboard',\
        \ description='Monitors a training job based on Tensorboard logs.')\n_parser.add_argument(\"\
        --pvc-name\", dest=\"pvc_name\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--pvc-path\", dest=\"pvc_path\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--tensorboard-name\"\
        , dest=\"tensorboard_name\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--mlpipeline-ui-metadata\", dest=\"mlpipeline_ui_metadata_path\"\
        , type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n\
        _parsed_args = vars(_parser.parse_args())\n\n_outputs = configure_tensorboard(**_parsed_args)\n"
      env:
      - name: COLUMNS
        valueFrom:
          configMapKeyRef: {key: columns, name: credit-risk-columns}
      image: quay.io/ntlawrence/demo-workflow:3.0.0
    inputs:
      parameters:
      - {name: create-pvc-for-tensorboard-name}
    outputs:
      artifacts:
      - {name: mlpipeline-ui-metadata, path: /tmp/outputs/mlpipeline_ui_metadata/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Monitors
          a training job based on Tensorboard logs.", "implementation": {"container":
          {"args": ["--pvc-name", {"inputValue": "pvc_name"}, {"if": {"cond": {"isPresent":
          "pvc_path"}, "then": ["--pvc-path", {"inputValue": "pvc_path"}]}}, {"if":
          {"cond": {"isPresent": "tensorboard_name"}, "then": ["--tensorboard-name",
          {"inputValue": "tensorboard_name"}]}}, "--mlpipeline-ui-metadata", {"outputPath":
          "mlpipeline_ui_metadata"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''kubernetes''
          || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''kubernetes'' --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef configure_tensorboard(\n    mlpipeline_ui_metadata_path,\n    pvc_name,\n    pvc_path
          = \"\",\n    tensorboard_name = \"\",\n):\n    \"\"\"\n    Monitors a training
          job based on Tensorboard logs. \n    Logs are expected to be written to
          the specified subpath of the pvc\n    \"\"\"\n    from collections import
          namedtuple\n    import json\n    from kubernetes import client, config,
          watch\n    import logging\n    import sys\n    import os\n    import yaml\n    import
          textwrap\n    import json\n    import http\n\n    logging.basicConfig(\n        stream=sys.stdout,\n        level=logging.INFO,\n        format=\"%(levelname)s
          %(asctime)s: %(message)s\",\n    )\n    logger = logging.getLogger()\n\n    if
          not tensorboard_name:\n        tensorboard_name=\"{{workflow.name}}\"\n\n    namespace=\"{{workflow.namespace}}\"\n\n    config.load_incluster_config()\n    api_client
          = client.ApiClient()\n    apps_api = client.AppsV1Api(api_client)\n    custom_object_api
          = client.CustomObjectsApi(api_client)\n\n    # Delete possible existing
          tensorboard\n    try:\n        custom_object_api.delete_namespaced_custom_object(\n            group=\"tensorboard.kubeflow.org\",\n            version=\"v1alpha1\",\n            plural=\"tensorboards\",\n            namespace=namespace,\n            name=tensorboard_name,\n            body=client.V1DeleteOptions()\n        )\n    except
          client.exceptions.ApiException as e:\n        if e.status != http.HTTPStatus.NOT_FOUND:\n            raise\n\n    tensorboard_spec
          = textwrap.dedent(f''''''\\\n            apiVersion: tensorboard.kubeflow.org/v1alpha1\n            kind:
          Tensorboard\n            metadata:\n              name: \"{tensorboard_name}\"\n              namespace:
          \"{namespace}\"\n              ownerReferences:\n                - apiVersion:
          v1\n                  kind: Workflow\n                  name: \"{{workflow.name}}\"\n                  uid:
          \"{{workflow.uid}}\"\n            spec:\n              logspath: \"pvc://{pvc_name}/{pvc_path}\"\n            ''''''\n    )\n\n    logger.info(tensorboard_spec)\n\n    custom_object_api.create_namespaced_custom_object(\n        group=\"tensorboard.kubeflow.org\",\n        version=\"v1alpha1\",\n        plural=\"tensorboards\",\n        namespace=namespace,\n        body=yaml.safe_load(tensorboard_spec),\n        pretty=True)\n\n    tensorboard_watch
          = watch.Watch()\n    try:\n        for tensorboard_event in tensorboard_watch.stream(\n            custom_object_api.list_namespaced_custom_object,\n            group=\"tensorboard.kubeflow.org\",\n            version=\"v1alpha1\",\n            plural=\"tensorboards\",\n            namespace=namespace,\n            field_selector=f\"metadata.name={tensorboard_name}\",\n            timeout_seconds=0,\n        ):\n\n            logger.info(f\"tensorboard_event:
          {json.dumps(tensorboard_event, indent=2)}\")\n\n            if tensorboard_event[\"type\"]==\"DELETED\":\n                raise
          RuntimeError(\"The tensorboard was deleted!\")\n\n            tensorboard
          = tensorboard_event[\"object\"]\n\n            if \"status\" not in tensorboard:\n                continue\n\n            deployment_state
          = \"Progressing\"\n            if \"conditions\" in tensorboard[\"status\"]:\n                deployment_state
          = tensorboard[\"status\"][\"conditions\"][-1][\n                    \"deploymentState\"\n                ]\n\n            if
          deployment_state == \"Progressing\":\n                logger.info(\"Tensorboard
          deployment is progressing...\")\n            elif deployment_state == \"Available\":\n                logger.info(\"Tensorboard
          deployment is Available.\")\n                break\n            elif deployment_state
          == \"ReplicaFailure\":\n                raise RuntimeError(\"Tensorboard
          deployment failed with a ReplicaFailure!\")\n            else:\n                raise
          RuntimeError(f\"Unknown deployment state: {deployment_state}\")\n    finally:\n        tensorboard_watch.stop()\n\n    button_style
          =(\n        \"align-items: center; \"\n        \"appearance: none; \"\n        \"background-color:
          rgb(26, 115, 232); \"\n        \"border: 0px none rgb(255, 255, 255); \"\n        \"border-radius:
          3px; \"\n        \"box-sizing: border-box; \"\n        \"color: rgb(255,
          255, 255); \"\n        \"cursor: pointer; \"\n        \"display: inline-flex;
          \" \n        \"font-family: ''Google Sans'', ''Helvetica Neue'', sans-serif;
          \"\n        \"font-size: 14px; \"\n        \"font-stretch: 100%; \"\n        \"font-style:
          normal; font-weight: 700; \"\n        \"justify-content: center; \"\n        \"letter-spacing:
          normal; \"\n        \"line-height: 24.5px; \"\n        \"margin: 0px 10px
          2px 0px; \"\n        \"min-height: 25px; \"\n        \"min-width: 64px;
          \"\n        \"padding: 2px 6px 2px 6px; \"\n        \"position: relative;
          \"\n        \"tab-size: 4; \"\n        \"text-align: center; \"\n        \"text-indent:
          0px; \"\n        \"text-rendering: auto; \"\n        \"text-shadow: none;
          \"\n        \"text-size-adjust: 100%; \"\n        \"text-transform: none;
          \"\n        \"user-select: none; \"\n        \"vertical-align: middle; \"\n        \"word-spacing:
          0px; \"\n        \"writing-mode: horizontal-tb;\"\n    )\n\n    # See: https://github.com/kubeflow/kubeflow/blob/master/components/crud-web-apps/tensorboards/frontend/src/app/pages/index/index.component.ts\n    #
          window.open(`/tensorboard/${tensorboard.namespace}/${tensorboard.name}/`);\n    ui_address
          = f\"/tensorboard/{namespace}/{tensorboard_name}/#scalars\"\n\n    markdown
          = textwrap.dedent(\n        f''''''\\\n        # Tensorboard\n        -
          <a href=\"{ui_address}\" style=\"{button_style}\" target=\"_blank\">Connect</a>\n        -
          <a href=\"/_/tensorboards/\" style=\"{button_style}\" target=\"_blank\">Manage
          all</a>\n        ''''''\n    )\n\n    markdown_output = {\n        \"type\":
          \"markdown\",\n        \"storage\": \"inline\",\n        \"source\": markdown,\n    }\n\n    ui_metadata
          = {\"outputs\": [markdown_output]}\n    with open(mlpipeline_ui_metadata_path,
          \"w\") as metadata_file:\n        json.dump(ui_metadata, metadata_file)\n\n    logging.info(\"Finished.\")\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Configure tensorboard'',
          description=''Monitors a training job based on Tensorboard logs.'')\n_parser.add_argument(\"--pvc-name\",
          dest=\"pvc_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--pvc-path\",
          dest=\"pvc_path\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--tensorboard-name\",
          dest=\"tensorboard_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlpipeline-ui-metadata\",
          dest=\"mlpipeline_ui_metadata_path\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = configure_tensorboard(**_parsed_args)\n"], "image": "quay.io/ntlawrence/demo-workflow:3.0.0"}},
          "inputs": [{"name": "pvc_name", "type": "String"}, {"default": "", "name":
          "pvc_path", "optional": true, "type": "String"}, {"default": "", "name":
          "tensorboard_name", "optional": true, "type": "String"}], "name": "Configure
          tensorboard", "outputs": [{"name": "mlpipeline_ui_metadata"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"pvc_name": "{{inputs.parameters.create-pvc-for-tensorboard-name}}",
          "pvc_path": "", "tensorboard_name": ""}'}
  - name: convert-model-to-onnx
    container:
      args: [--tf-model, /tmp/inputs/tf_model/data, --onnx-model, /tmp/outputs/onnx_model/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef convert_model_to_onnx(tf_model,\n                          onnx_model):\n\
        \    import tf2onnx\n    import tensorflow as tf\n    import onnx\n\n    keras_model\
        \ = tf.keras.models.load_model(tf_model)    \n    converted_model, _ = tf2onnx.convert.from_keras(keras_model)\n\
        \    onnx.save_model(converted_model, onnx_model)\n\nimport argparse\n_parser\
        \ = argparse.ArgumentParser(prog='Convert model to onnx', description='')\n\
        _parser.add_argument(\"--tf-model\", dest=\"tf_model\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--onnx-model\", dest=\"\
        onnx_model\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n\
        _parsed_args = vars(_parser.parse_args())\n\n_outputs = convert_model_to_onnx(**_parsed_args)\n"
      env:
      - name: COLUMNS
        valueFrom:
          configMapKeyRef: {key: columns, name: credit-risk-columns}
      image: quay.io/ntlawrence/demo-workflow:3.0.0
    inputs:
      artifacts:
      - {name: train-model, path: /tmp/inputs/tf_model/data}
    outputs:
      artifacts:
      - {name: convert-model-to-onnx-onnx_model, path: /tmp/outputs/onnx_model/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--tf-model", {"inputPath": "tf_model"}, "--onnx-model", {"outputPath":
          "onnx_model"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef convert_model_to_onnx(tf_model,\n                          onnx_model):\n    import
          tf2onnx\n    import tensorflow as tf\n    import onnx\n\n    keras_model
          = tf.keras.models.load_model(tf_model)    \n    converted_model, _ = tf2onnx.convert.from_keras(keras_model)\n    onnx.save_model(converted_model,
          onnx_model)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Convert
          model to onnx'', description='''')\n_parser.add_argument(\"--tf-model\",
          dest=\"tf_model\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--onnx-model\",
          dest=\"onnx_model\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = convert_model_to_onnx(**_parsed_args)\n"], "image": "quay.io/ntlawrence/demo-workflow:3.0.0"}},
          "inputs": [{"name": "tf_model", "type": "String"}], "name": "Convert model
          to onnx", "outputs": [{"name": "onnx_model", "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: create-pvc-for-tensorboard
    resource:
      action: create
      setOwnerReference: true
      manifest: |
        apiVersion: v1
        kind: PersistentVolumeClaim
        metadata:
          name: '{{workflow.name}}-tensorboard'
        spec:
          accessModes:
          - ReadWriteMany
          resources:
            requests:
              storage: 4G
    outputs:
      parameters:
      - name: create-pvc-for-tensorboard-manifest
        valueFrom: {jsonPath: '{}'}
      - name: create-pvc-for-tensorboard-name
        valueFrom: {jsonPath: '{.metadata.name}'}
      - name: create-pvc-for-tensorboard-size
        valueFrom: {jsonPath: '{.status.capacity.storage}'}
    metadata:
      annotations: {pipelines.kubeflow.org/max_cache_staleness: P0D}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: credit-risk
    dag:
      tasks:
      - name: configure-tensorboard
        template: configure-tensorboard
        dependencies: [create-pvc-for-tensorboard]
        arguments:
          parameters:
          - {name: create-pvc-for-tensorboard-name, value: '{{tasks.create-pvc-for-tensorboard.outputs.parameters.create-pvc-for-tensorboard-name}}'}
      - name: convert-model-to-onnx
        template: convert-model-to-onnx
        dependencies: [train]
        arguments:
          artifacts:
          - {name: train-model, from: '{{tasks.train.outputs.artifacts.train-model}}'}
      - {name: create-pvc-for-tensorboard, template: create-pvc-for-tensorboard}
      - name: data-quality-report
        template: data-quality-report
        dependencies: [load-df-from-postgresql]
        arguments:
          artifacts:
          - {name: load-df-from-postgresql-data_frame_pkl, from: '{{tasks.load-df-from-postgresql.outputs.artifacts.load-df-from-postgresql-data_frame_pkl}}'}
      - name: deploy-inference-service
        template: deploy-inference-service
        dependencies: [train, upload-artifacts]
        arguments:
          parameters:
          - {name: upload-artifacts-s3_address, value: '{{tasks.upload-artifacts.outputs.parameters.upload-artifacts-s3_address}}'}
          artifacts:
          - {name: train-target_processing_config, from: '{{tasks.train.outputs.artifacts.train-target_processing_config}}'}
      - name: evaluate
        template: evaluate
        dependencies: [convert-model-to-onnx, fit-preprocessor, load-df-from-postgresql-2,
          train]
        arguments:
          artifacts:
          - {name: convert-model-to-onnx-onnx_model, from: '{{tasks.convert-model-to-onnx.outputs.artifacts.convert-model-to-onnx-onnx_model}}'}
          - {name: fit-preprocessor-preprocessor_pkl, from: '{{tasks.fit-preprocessor.outputs.artifacts.fit-preprocessor-preprocessor_pkl}}'}
          - {name: load-df-from-postgresql-2-data_frame_pkl, from: '{{tasks.load-df-from-postgresql-2.outputs.artifacts.load-df-from-postgresql-2-data_frame_pkl}}'}
          - {name: train-target_processing_config, from: '{{tasks.train.outputs.artifacts.train-target_processing_config}}'}
      - name: fit-preprocessor
        template: fit-preprocessor
        dependencies: [load-df-from-postgresql]
        arguments:
          artifacts:
          - {name: load-df-from-postgresql-data_frame_pkl, from: '{{tasks.load-df-from-postgresql.outputs.artifacts.load-df-from-postgresql-data_frame_pkl}}'}
      - {name: load-df-from-postgresql, template: load-df-from-postgresql}
      - {name: load-df-from-postgresql-2, template: load-df-from-postgresql-2}
      - name: train
        template: train
        dependencies: [configure-tensorboard, create-pvc-for-tensorboard, fit-preprocessor,
          load-df-from-postgresql]
        arguments:
          parameters:
          - {name: create-pvc-for-tensorboard-name, value: '{{tasks.create-pvc-for-tensorboard.outputs.parameters.create-pvc-for-tensorboard-name}}'}
          artifacts:
          - {name: fit-preprocessor-preprocessor_pkl, from: '{{tasks.fit-preprocessor.outputs.artifacts.fit-preprocessor-preprocessor_pkl}}'}
          - {name: load-df-from-postgresql-data_frame_pkl, from: '{{tasks.load-df-from-postgresql.outputs.artifacts.load-df-from-postgresql-data_frame_pkl}}'}
      - name: train-explainer
        template: train-explainer
        dependencies: [convert-model-to-onnx, fit-preprocessor, load-df-from-postgresql,
          train]
        arguments:
          artifacts:
          - {name: convert-model-to-onnx-onnx_model, from: '{{tasks.convert-model-to-onnx.outputs.artifacts.convert-model-to-onnx-onnx_model}}'}
          - {name: fit-preprocessor-preprocessor_pkl, from: '{{tasks.fit-preprocessor.outputs.artifacts.fit-preprocessor-preprocessor_pkl}}'}
          - {name: load-df-from-postgresql-data_frame_pkl, from: '{{tasks.load-df-from-postgresql.outputs.artifacts.load-df-from-postgresql-data_frame_pkl}}'}
          - {name: train-target_processing_config, from: '{{tasks.train.outputs.artifacts.train-target_processing_config}}'}
      - name: upload-artifacts
        template: upload-artifacts
        dependencies: [convert-model-to-onnx, evaluate, fit-preprocessor, train-explainer]
        arguments:
          artifacts:
          - {name: convert-model-to-onnx-onnx_model, from: '{{tasks.convert-model-to-onnx.outputs.artifacts.convert-model-to-onnx-onnx_model}}'}
          - {name: fit-preprocessor-preprocessor_pkl, from: '{{tasks.fit-preprocessor.outputs.artifacts.fit-preprocessor-preprocessor_pkl}}'}
          - {name: train-explainer-explainer_dll, from: '{{tasks.train-explainer.outputs.artifacts.train-explainer-explainer_dll}}'}
  - name: data-quality-report
    container:
      args: [--df, /tmp/inputs/df/data, --features, '["CheckingStatus", "LoanDuration",
          "CreditHistory", "LoanPurpose", "LoanAmount", "ExistingSavings", "EmploymentDuration",
          "InstallmentPercent", "Sex", "OthersOnLoan", "CurrentResidenceDuration",
          "OwnsProperty", "Age", "InstallmentPlans", "Housing", "ExistingCreditsCount",
          "Job", "Dependents", "Telephone", "ForeignWorker"]', --target, Risk, --mlpipeline-ui-metadata,
        /tmp/outputs/mlpipeline_ui_metadata/data, --output-report, /tmp/outputs/output_report/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def data_quality_report(df,
                                features,
                                mlpipeline_ui_metadata_path,
                                output_report,
                                target = 'Risk'):
            from evidently.metric_preset import DataQualityPreset
            from evidently.report import Report
            from evidently import ColumnMapping
            import pandas as pd
            import os
            from pathlib import Path
            import json

            dataset = pd.read_pickle(df)
            column_info = json.loads(os.environ["COLUMNS"])

            column_mapping = ColumnMapping()
            column_mapping.target = target
            column_mapping.task = "classification"
            feature_set = set(features)
            column_mapping.numerical_features = [
                c
                for c in column_info["int_columns"]
                if c in feature_set
            ]
            column_mapping.categorical_features = [
                c
                for c in column_info["label_columns"]
                if c in feature_set
            ]

            report = Report(
                metrics=[
                    DataQualityPreset(),
                ]
            )

            report.run(
                reference_data=None,
                current_data=dataset,
                column_mapping=column_mapping,
            )

            Path(output_report).parent.mkdir(parents=True, exist_ok=True)
            report.save_html(output_report)
            html_content = open(output_report, "r").read()
            metadata = {
                "outputs": [
                    {
                        "type": "web-app",
                        "storage": "inline",
                        "source": html_content,
                    }
                ]
            }

            with open(mlpipeline_ui_metadata_path, "w") as f:
                json.dump(metadata, f)

        import json
        import argparse
        _parser = argparse.ArgumentParser(prog='Data quality report', description='')
        _parser.add_argument("--df", dest="df", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--features", dest="features", type=json.loads, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--target", dest="target", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--mlpipeline-ui-metadata", dest="mlpipeline_ui_metadata_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--output-report", dest="output_report", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = data_quality_report(**_parsed_args)
      env:
      - name: COLUMNS
        valueFrom:
          configMapKeyRef: {key: columns, name: credit-risk-columns}
      image: quay.io/ntlawrence/demo-workflow:3.0.0
    inputs:
      artifacts:
      - {name: load-df-from-postgresql-data_frame_pkl, path: /tmp/inputs/df/data}
    outputs:
      artifacts:
      - {name: mlpipeline-ui-metadata, path: /tmp/outputs/mlpipeline_ui_metadata/data}
      - {name: data-quality-report-output_report, path: /tmp/outputs/output_report/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--df", {"inputPath": "df"}, "--features", {"inputValue": "features"},
          {"if": {"cond": {"isPresent": "target"}, "then": ["--target", {"inputValue":
          "target"}]}}, "--mlpipeline-ui-metadata", {"outputPath": "mlpipeline_ui_metadata"},
          "--output-report", {"outputPath": "output_report"}], "command": ["sh", "-ec",
          "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef data_quality_report(df,\n                        features,\n                        mlpipeline_ui_metadata_path,\n                        output_report,\n                        target
          = ''Risk''):\n    from evidently.metric_preset import DataQualityPreset\n    from
          evidently.report import Report\n    from evidently import ColumnMapping\n    import
          pandas as pd\n    import os\n    from pathlib import Path\n    import json\n\n    dataset
          = pd.read_pickle(df)\n    column_info = json.loads(os.environ[\"COLUMNS\"])\n\n    column_mapping
          = ColumnMapping()\n    column_mapping.target = target\n    column_mapping.task
          = \"classification\"\n    feature_set = set(features)\n    column_mapping.numerical_features
          = [\n        c\n        for c in column_info[\"int_columns\"]\n        if
          c in feature_set\n    ]\n    column_mapping.categorical_features = [\n        c\n        for
          c in column_info[\"label_columns\"]\n        if c in feature_set\n    ]\n\n    report
          = Report(\n        metrics=[\n            DataQualityPreset(),\n        ]\n    )\n\n    report.run(\n        reference_data=None,\n        current_data=dataset,\n        column_mapping=column_mapping,\n    )\n\n    Path(output_report).parent.mkdir(parents=True,
          exist_ok=True)\n    report.save_html(output_report)\n    html_content =
          open(output_report, \"r\").read()\n    metadata = {\n        \"outputs\":
          [\n            {\n                \"type\": \"web-app\",\n                \"storage\":
          \"inline\",\n                \"source\": html_content,\n            }\n        ]\n    }\n\n    with
          open(mlpipeline_ui_metadata_path, \"w\") as f:\n        json.dump(metadata,
          f)\n\nimport json\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Data
          quality report'', description='''')\n_parser.add_argument(\"--df\", dest=\"df\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--features\",
          dest=\"features\", type=json.loads, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--target\",
          dest=\"target\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlpipeline-ui-metadata\",
          dest=\"mlpipeline_ui_metadata_path\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-report\",
          dest=\"output_report\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = data_quality_report(**_parsed_args)\n"], "image": "quay.io/ntlawrence/demo-workflow:3.0.0"}},
          "inputs": [{"name": "df", "type": "String"}, {"name": "features", "type":
          "typing.List[str]"}, {"default": "Risk", "name": "target", "optional": true,
          "type": "String"}], "name": "Data quality report", "outputs": [{"name":
          "mlpipeline_ui_metadata", "type": "String"}, {"name": "output_report", "type":
          "String"}]}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"features":
          "[\"CheckingStatus\", \"LoanDuration\", \"CreditHistory\", \"LoanPurpose\",
          \"LoanAmount\", \"ExistingSavings\", \"EmploymentDuration\", \"InstallmentPercent\",
          \"Sex\", \"OthersOnLoan\", \"CurrentResidenceDuration\", \"OwnsProperty\",
          \"Age\", \"InstallmentPlans\", \"Housing\", \"ExistingCreditsCount\", \"Job\",
          \"Dependents\", \"Telephone\", \"ForeignWorker\"]", "target": "Risk"}'}
  - name: deploy-inference-service
    container:
      args: [--name, credit-risk, --target-processing-config, /tmp/inputs/target_processing_config/data,
        --model-archive-s3, '{{inputs.parameters.upload-artifacts-s3_address}}', --transformer-image,
        'quay.io/ntlawrence/demo-transformer:3.0.0', --predictor-image, 'quay.io/ntlawrence/demo-predictor:3.0.0',
        --explainer-image, 'quay.io/ntlawrence/demo-explainer:3.0.0', --predictor-max-replicas,
        '4', --predictor-min-replicas, '1', --predictor-concurrency-target, '1', --transformer-max-replicas,
        '4', --transformer-min-replicas, '1', --transformer-concurrency-target, '1',
        --explainer-max-replicas, '1', --explainer-min-replicas, '1']
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def deploy_inference_service(name,\n                             target_processing_config,\n\
        \                             model_archive_s3,\n                        \
        \     transformer_image,\n                             predictor_image,\n\
        \                             explainer_image,\n                         \
        \    predictor_max_replicas = 1,\n                             predictor_min_replicas\
        \ = 1,\n                             predictor_concurrency_target = None,\n\
        \                             transformer_max_replicas = 1,\n            \
        \                 transformer_min_replicas = 1,\n                        \
        \     transformer_concurrency_target = None,\n                           \
        \  explainer_max_replicas = 1,\n                             explainer_min_replicas\
        \ = 1,\n                             explainer_concurrency_target = None\n\
        \                            ):\n    import kserve\n    from kubernetes import\
        \ client, config\n    from kubernetes.client import (V1ServiceAccount, \n\
        \                                   V1Container, \n                      \
        \             V1EnvVar, \n                                   V1ObjectMeta,\
        \ \n                                   V1ContainerPort, \n               \
        \                    V1ObjectReference,\n                                \
        \   V1ResourceRequirements\n                                  )\n    from\
        \ kserve import KServeClient\n    from kserve import constants\n    from kserve\
        \ import V1beta1PredictorSpec\n    from kserve import V1beta1ExplainerSpec\n\
        \    from kserve import V1beta1TransformerSpec\n    from kserve import V1beta1InferenceServiceSpec\n\
        \    from kserve import V1beta1InferenceService\n    import json\n    from\
        \ http import HTTPStatus\n    import logging\n    import yaml\n    from time\
        \ import sleep\n\n    with open(target_processing_config, \"r\") as f:\n \
        \       target_processing = json.load(f)\n\n    prediction_threshold = target_processing[\"\
        threshold\"]\n    target_names = json.dumps(\n        [target_processing[\"\
        target_names\"].get(str(idx),\"?\")\n         for idx in range(len(target_processing[\"\
        target_names\"]))]\n    )\n\n    config.load_incluster_config()\n    SERVICE_ACCOUNT\
        \ = \"credit-risk-inference-sa\"\n\n    sa = V1ServiceAccount(\n        api_version=\"\
        v1\",\n        kind=\"ServiceAccount\",\n        metadata=V1ObjectMeta(name=SERVICE_ACCOUNT,\
        \ \n                              namespace=\"{{workflow.namespace}}\"),\n\
        \        secrets=[V1ObjectReference(name=\"minio-credentials\")]\n    )\n\
        \    corev1 = client.CoreV1Api()\n    try:\n        corev1.create_namespaced_service_account(namespace=\"\
        {{workflow.namespace}}\",\n                                              \
        \   body=sa)\n    except client.exceptions.ApiException as e:\n        if\
        \ e.status==HTTPStatus.CONFLICT:\n            corev1.patch_namespaced_service_account(name=SERVICE_ACCOUNT,\n\
        \                                                    namespace=\"{{workflow.namespace}}\"\
        ,\n                                                    body=sa)\n        else:\n\
        \            raise\n\n    predictor_spec = V1beta1PredictorSpec(\n       \
        \ max_replicas=predictor_max_replicas,\n        min_replicas=predictor_min_replicas,\n\
        \        scale_target=predictor_concurrency_target,\n        scale_metric=\"\
        concurrency\",\n        containers=[\n            V1Container(\n         \
        \       name=\"kserve-container\",\n                image=predictor_image,\n\
        \                args=[\"--grpc_port=8081\", f\"--model_name={name}\"],\n\
        \                ports=[V1ContainerPort(\n                    container_port=8081,\n\
        \                    name=\"h2c\",\n                    protocol=\"TCP\"\n\
        \                )],\n                resources=V1ResourceRequirements(\n\
        \                    limits={\"memory\": \"10Gi\"},\n                    requests={\"\
        memory\": \"2Gi\"},\n                ),\n                env=[\n         \
        \        V1EnvVar(\n                     name=\"STORAGE_URI\", value=model_archive_s3\n\
        \                 ),\n                 V1EnvVar(\n                     name=\"\
        THRESHOLD\",\n                     value=str(prediction_threshold)\n     \
        \            )\n                ],\n            )\n        ],\n        service_account_name=SERVICE_ACCOUNT\n\
        \    )\n\n    transformer_spec = V1beta1TransformerSpec(\n        max_replicas=transformer_max_replicas,\n\
        \        min_replicas=transformer_min_replicas,\n        scale_target=transformer_concurrency_target,\n\
        \        scale_metric=\"concurrency\",\n        containers=[\n           \
        \ V1Container(\n                name=\"kserve-container\",\n             \
        \   image=transformer_image,\n                args=[\"--protocol=grpc-v2\"\
        , f\"--model_name={name}\"],\n                resources=V1ResourceRequirements(\n\
        \                    limits={\"memory\": \"10Gi\"},\n                    requests={\"\
        memory\": \"2Gi\"},\n                ),\n                #ports=[V1ContainerPort(\n\
        \                #    container_port=8080,\n                #    name=\"h2c\"\
        ,\n                #    protocol=\"TCP\"\n                #)],\n         \
        \       env=[\n                 V1EnvVar(\n                     name=\"STORAGE_URI\"\
        , value=model_archive_s3\n                 ),\n                V1EnvVar(\n\
        \                    name=\"TARGET_NAMES\", value=target_names\n         \
        \       )\n                ],\n            )\n        ],\n        service_account_name=SERVICE_ACCOUNT\n\
        \    )\n\n    explainer_spec=V1beta1ExplainerSpec(\n        max_replicas=explainer_max_replicas,\n\
        \        min_replicas=explainer_min_replicas,\n        scale_target=explainer_concurrency_target,\n\
        \        scale_metric=\"concurrency\",\n        containers=[\n           \
        \ V1Container(\n                name=\"kserve-container\",\n             \
        \   image=explainer_image,\n                args=[\"--protocol=grpc-v2\",\
        \ f\"--model_name={name}\"],\n                resources=V1ResourceRequirements(\n\
        \                    limits={\"memory\": \"10Gi\"},\n                    requests={\"\
        memory\": \"4Gi\"},\n                ),\n                #ports=[V1ContainerPort(\n\
        \                #    container_port=8080,\n                #    name=\"h2c\"\
        ,\n                #    protocol=\"TCP\"\n                #)],\n         \
        \       env=[\n                 V1EnvVar(\n                     name=\"STORAGE_URI\"\
        , value=model_archive_s3\n                 ),\n                 V1EnvVar(\n\
        \                     name=\"EXPLAIN_MIN_SAMPLES_START\", value=\"15000\"\n\
        \                 )\n                ],\n            )\n        ],\n     \
        \   service_account_name=SERVICE_ACCOUNT\n    )\n\n    inference_service=V1beta1InferenceService(\n\
        \        api_version=constants.KSERVE_V1BETA1,\n        kind=constants.KSERVE_KIND,\n\
        \        metadata=V1ObjectMeta(name=name, \n                             \
        \ namespace=\"{{workflow.namespace}}\",\n                              annotations={\"\
        sidecar.istio.io/inject\": \"false\",\n                                  \
        \         \"serving.kserve.io/enable-prometheus-scraping\" : \"true\"}),\n\
        \        spec=V1beta1InferenceServiceSpec(predictor=predictor_spec,\n    \
        \                                     transformer=transformer_spec,\n    \
        \                                     explainer=explainer_spec)\n    )\n \
        \   # serving.kserve.io/inferenceservice: credit-risk\n    logging.info(\n\
        \        yaml.dump(\n            client.ApiClient().sanitize_for_serialization(inference_service)\n\
        \        )\n    )\n\n    # KServeClient doesn't throw ApiException for CONFLICT\n\
        \    # Using the k8s API directly for the create\n    api_instance = client.CustomObjectsApi()\n\
        \    while True:\n        try:\n            api_instance.create_namespaced_custom_object(\n\
        \                    group=constants.KSERVE_GROUP,\n                    version=inference_service.api_version.split(\"\
        /\")[1],\n                    namespace=\"{{workflow.namespace}}\",\n    \
        \                plural=constants.KSERVE_PLURAL,\n                    body=inference_service)\n\
        \            break\n        except client.exceptions.ApiException as api_exception:\n\
        \            if api_exception.status==HTTPStatus.CONFLICT:\n             \
        \   try:\n                    api_instance.delete_namespaced_custom_object(\n\
        \                        group=constants.KSERVE_GROUP,\n                 \
        \       version=inference_service.api_version.split(\"/\")[1],\n         \
        \               namespace=\"{{workflow.namespace}}\",\n                  \
        \      plural=constants.KSERVE_PLURAL,\n                        name=name)\n\
        \                    sleep(15)\n                except client.exceptions.ApiException\
        \ as api_exception2:\n                    if api_exception2.status in {HTTPStatus.NOT_FOUND,\
        \ HTTPStatus.GONE}:\n                        pass\n                    else:\n\
        \                        raise\n\n            else:\n                raise\n\
        \n    kclient = KServeClient()\n    kclient.wait_isvc_ready(name=name, namespace=\"\
        {{workflow.namespace}}\")\n\n    if not kclient.is_isvc_ready(name=name, namespace=\"\
        {{workflow.namespace}}\"):\n        raise RuntimeError(f\"The inference service\
        \ {name} is not ready!\")\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Deploy\
        \ inference service', description='')\n_parser.add_argument(\"--name\", dest=\"\
        name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --target-processing-config\", dest=\"target_processing_config\", type=str,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-archive-s3\"\
        , dest=\"model_archive_s3\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--transformer-image\", dest=\"transformer_image\",\
        \ type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --predictor-image\", dest=\"predictor_image\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--explainer-image\", dest=\"explainer_image\", type=str,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictor-max-replicas\"\
        , dest=\"predictor_max_replicas\", type=int, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--predictor-min-replicas\", dest=\"predictor_min_replicas\"\
        , type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --predictor-concurrency-target\", dest=\"predictor_concurrency_target\", type=int,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--transformer-max-replicas\"\
        , dest=\"transformer_max_replicas\", type=int, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--transformer-min-replicas\", dest=\"transformer_min_replicas\"\
        , type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --transformer-concurrency-target\", dest=\"transformer_concurrency_target\"\
        , type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --explainer-max-replicas\", dest=\"explainer_max_replicas\", type=int, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--explainer-min-replicas\"\
        , dest=\"explainer_min_replicas\", type=int, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--explainer-concurrency-target\", dest=\"explainer_concurrency_target\"\
        , type=int, required=False, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
        \n_outputs = deploy_inference_service(**_parsed_args)\n"
      env:
      - name: COLUMNS
        valueFrom:
          configMapKeyRef: {key: columns, name: credit-risk-columns}
      image: quay.io/ntlawrence/demo-kserve:3.0.0
    inputs:
      parameters:
      - {name: upload-artifacts-s3_address}
      artifacts:
      - {name: train-target_processing_config, path: /tmp/inputs/target_processing_config/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--name", {"inputValue": "name"}, "--target-processing-config",
          {"inputPath": "target_processing_config"}, "--model-archive-s3", {"inputValue":
          "model_archive_s3"}, "--transformer-image", {"inputValue": "transformer_image"},
          "--predictor-image", {"inputValue": "predictor_image"}, "--explainer-image",
          {"inputValue": "explainer_image"}, {"if": {"cond": {"isPresent": "predictor_max_replicas"},
          "then": ["--predictor-max-replicas", {"inputValue": "predictor_max_replicas"}]}},
          {"if": {"cond": {"isPresent": "predictor_min_replicas"}, "then": ["--predictor-min-replicas",
          {"inputValue": "predictor_min_replicas"}]}}, {"if": {"cond": {"isPresent":
          "predictor_concurrency_target"}, "then": ["--predictor-concurrency-target",
          {"inputValue": "predictor_concurrency_target"}]}}, {"if": {"cond": {"isPresent":
          "transformer_max_replicas"}, "then": ["--transformer-max-replicas", {"inputValue":
          "transformer_max_replicas"}]}}, {"if": {"cond": {"isPresent": "transformer_min_replicas"},
          "then": ["--transformer-min-replicas", {"inputValue": "transformer_min_replicas"}]}},
          {"if": {"cond": {"isPresent": "transformer_concurrency_target"}, "then":
          ["--transformer-concurrency-target", {"inputValue": "transformer_concurrency_target"}]}},
          {"if": {"cond": {"isPresent": "explainer_max_replicas"}, "then": ["--explainer-max-replicas",
          {"inputValue": "explainer_max_replicas"}]}}, {"if": {"cond": {"isPresent":
          "explainer_min_replicas"}, "then": ["--explainer-min-replicas", {"inputValue":
          "explainer_min_replicas"}]}}, {"if": {"cond": {"isPresent": "explainer_concurrency_target"},
          "then": ["--explainer-concurrency-target", {"inputValue": "explainer_concurrency_target"}]}}],
          "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" >
          \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def deploy_inference_service(name,\n                             target_processing_config,\n                             model_archive_s3,\n                             transformer_image,\n                             predictor_image,\n                             explainer_image,\n                             predictor_max_replicas
          = 1,\n                             predictor_min_replicas = 1,\n                             predictor_concurrency_target
          = None,\n                             transformer_max_replicas = 1,\n                             transformer_min_replicas
          = 1,\n                             transformer_concurrency_target = None,\n                             explainer_max_replicas
          = 1,\n                             explainer_min_replicas = 1,\n                             explainer_concurrency_target
          = None\n                            ):\n    import kserve\n    from kubernetes
          import client, config\n    from kubernetes.client import (V1ServiceAccount,
          \n                                   V1Container, \n                                   V1EnvVar,
          \n                                   V1ObjectMeta, \n                                   V1ContainerPort,
          \n                                   V1ObjectReference,\n                                   V1ResourceRequirements\n                                  )\n    from
          kserve import KServeClient\n    from kserve import constants\n    from kserve
          import V1beta1PredictorSpec\n    from kserve import V1beta1ExplainerSpec\n    from
          kserve import V1beta1TransformerSpec\n    from kserve import V1beta1InferenceServiceSpec\n    from
          kserve import V1beta1InferenceService\n    import json\n    from http import
          HTTPStatus\n    import logging\n    import yaml\n    from time import sleep\n\n    with
          open(target_processing_config, \"r\") as f:\n        target_processing =
          json.load(f)\n\n    prediction_threshold = target_processing[\"threshold\"]\n    target_names
          = json.dumps(\n        [target_processing[\"target_names\"].get(str(idx),\"?\")\n         for
          idx in range(len(target_processing[\"target_names\"]))]\n    )\n\n    config.load_incluster_config()\n    SERVICE_ACCOUNT
          = \"credit-risk-inference-sa\"\n\n    sa = V1ServiceAccount(\n        api_version=\"v1\",\n        kind=\"ServiceAccount\",\n        metadata=V1ObjectMeta(name=SERVICE_ACCOUNT,
          \n                              namespace=\"{{workflow.namespace}}\"),\n        secrets=[V1ObjectReference(name=\"minio-credentials\")]\n    )\n    corev1
          = client.CoreV1Api()\n    try:\n        corev1.create_namespaced_service_account(namespace=\"{{workflow.namespace}}\",\n                                                 body=sa)\n    except
          client.exceptions.ApiException as e:\n        if e.status==HTTPStatus.CONFLICT:\n            corev1.patch_namespaced_service_account(name=SERVICE_ACCOUNT,\n                                                    namespace=\"{{workflow.namespace}}\",\n                                                    body=sa)\n        else:\n            raise\n\n    predictor_spec
          = V1beta1PredictorSpec(\n        max_replicas=predictor_max_replicas,\n        min_replicas=predictor_min_replicas,\n        scale_target=predictor_concurrency_target,\n        scale_metric=\"concurrency\",\n        containers=[\n            V1Container(\n                name=\"kserve-container\",\n                image=predictor_image,\n                args=[\"--grpc_port=8081\",
          f\"--model_name={name}\"],\n                ports=[V1ContainerPort(\n                    container_port=8081,\n                    name=\"h2c\",\n                    protocol=\"TCP\"\n                )],\n                resources=V1ResourceRequirements(\n                    limits={\"memory\":
          \"10Gi\"},\n                    requests={\"memory\": \"2Gi\"},\n                ),\n                env=[\n                 V1EnvVar(\n                     name=\"STORAGE_URI\",
          value=model_archive_s3\n                 ),\n                 V1EnvVar(\n                     name=\"THRESHOLD\",\n                     value=str(prediction_threshold)\n                 )\n                ],\n            )\n        ],\n        service_account_name=SERVICE_ACCOUNT\n    )\n\n    transformer_spec
          = V1beta1TransformerSpec(\n        max_replicas=transformer_max_replicas,\n        min_replicas=transformer_min_replicas,\n        scale_target=transformer_concurrency_target,\n        scale_metric=\"concurrency\",\n        containers=[\n            V1Container(\n                name=\"kserve-container\",\n                image=transformer_image,\n                args=[\"--protocol=grpc-v2\",
          f\"--model_name={name}\"],\n                resources=V1ResourceRequirements(\n                    limits={\"memory\":
          \"10Gi\"},\n                    requests={\"memory\": \"2Gi\"},\n                ),\n                #ports=[V1ContainerPort(\n                #    container_port=8080,\n                #    name=\"h2c\",\n                #    protocol=\"TCP\"\n                #)],\n                env=[\n                 V1EnvVar(\n                     name=\"STORAGE_URI\",
          value=model_archive_s3\n                 ),\n                V1EnvVar(\n                    name=\"TARGET_NAMES\",
          value=target_names\n                )\n                ],\n            )\n        ],\n        service_account_name=SERVICE_ACCOUNT\n    )\n\n    explainer_spec=V1beta1ExplainerSpec(\n        max_replicas=explainer_max_replicas,\n        min_replicas=explainer_min_replicas,\n        scale_target=explainer_concurrency_target,\n        scale_metric=\"concurrency\",\n        containers=[\n            V1Container(\n                name=\"kserve-container\",\n                image=explainer_image,\n                args=[\"--protocol=grpc-v2\",
          f\"--model_name={name}\"],\n                resources=V1ResourceRequirements(\n                    limits={\"memory\":
          \"10Gi\"},\n                    requests={\"memory\": \"4Gi\"},\n                ),\n                #ports=[V1ContainerPort(\n                #    container_port=8080,\n                #    name=\"h2c\",\n                #    protocol=\"TCP\"\n                #)],\n                env=[\n                 V1EnvVar(\n                     name=\"STORAGE_URI\",
          value=model_archive_s3\n                 ),\n                 V1EnvVar(\n                     name=\"EXPLAIN_MIN_SAMPLES_START\",
          value=\"15000\"\n                 )\n                ],\n            )\n        ],\n        service_account_name=SERVICE_ACCOUNT\n    )\n\n    inference_service=V1beta1InferenceService(\n        api_version=constants.KSERVE_V1BETA1,\n        kind=constants.KSERVE_KIND,\n        metadata=V1ObjectMeta(name=name,
          \n                              namespace=\"{{workflow.namespace}}\",\n                              annotations={\"sidecar.istio.io/inject\":
          \"false\",\n                                           \"serving.kserve.io/enable-prometheus-scraping\"
          : \"true\"}),\n        spec=V1beta1InferenceServiceSpec(predictor=predictor_spec,\n                                         transformer=transformer_spec,\n                                         explainer=explainer_spec)\n    )\n    #
          serving.kserve.io/inferenceservice: credit-risk\n    logging.info(\n        yaml.dump(\n            client.ApiClient().sanitize_for_serialization(inference_service)\n        )\n    )\n\n    #
          KServeClient doesn''t throw ApiException for CONFLICT\n    # Using the k8s
          API directly for the create\n    api_instance = client.CustomObjectsApi()\n    while
          True:\n        try:\n            api_instance.create_namespaced_custom_object(\n                    group=constants.KSERVE_GROUP,\n                    version=inference_service.api_version.split(\"/\")[1],\n                    namespace=\"{{workflow.namespace}}\",\n                    plural=constants.KSERVE_PLURAL,\n                    body=inference_service)\n            break\n        except
          client.exceptions.ApiException as api_exception:\n            if api_exception.status==HTTPStatus.CONFLICT:\n                try:\n                    api_instance.delete_namespaced_custom_object(\n                        group=constants.KSERVE_GROUP,\n                        version=inference_service.api_version.split(\"/\")[1],\n                        namespace=\"{{workflow.namespace}}\",\n                        plural=constants.KSERVE_PLURAL,\n                        name=name)\n                    sleep(15)\n                except
          client.exceptions.ApiException as api_exception2:\n                    if
          api_exception2.status in {HTTPStatus.NOT_FOUND, HTTPStatus.GONE}:\n                        pass\n                    else:\n                        raise\n\n            else:\n                raise\n\n    kclient
          = KServeClient()\n    kclient.wait_isvc_ready(name=name, namespace=\"{{workflow.namespace}}\")\n\n    if
          not kclient.is_isvc_ready(name=name, namespace=\"{{workflow.namespace}}\"):\n        raise
          RuntimeError(f\"The inference service {name} is not ready!\")\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Deploy inference service'',
          description='''')\n_parser.add_argument(\"--name\", dest=\"name\", type=str,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--target-processing-config\",
          dest=\"target_processing_config\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-archive-s3\",
          dest=\"model_archive_s3\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--transformer-image\",
          dest=\"transformer_image\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictor-image\",
          dest=\"predictor_image\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--explainer-image\",
          dest=\"explainer_image\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictor-max-replicas\",
          dest=\"predictor_max_replicas\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictor-min-replicas\",
          dest=\"predictor_min_replicas\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictor-concurrency-target\",
          dest=\"predictor_concurrency_target\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--transformer-max-replicas\",
          dest=\"transformer_max_replicas\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--transformer-min-replicas\",
          dest=\"transformer_min_replicas\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--transformer-concurrency-target\",
          dest=\"transformer_concurrency_target\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--explainer-max-replicas\",
          dest=\"explainer_max_replicas\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--explainer-min-replicas\",
          dest=\"explainer_min_replicas\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--explainer-concurrency-target\",
          dest=\"explainer_concurrency_target\", type=int, required=False, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = deploy_inference_service(**_parsed_args)\n"],
          "image": "quay.io/ntlawrence/demo-kserve:3.0.0"}}, "inputs": [{"name": "name",
          "type": "String"}, {"name": "target_processing_config", "type": "String"},
          {"name": "model_archive_s3", "type": "String"}, {"name": "transformer_image",
          "type": "String"}, {"name": "predictor_image", "type": "String"}, {"name":
          "explainer_image", "type": "String"}, {"default": "1", "name": "predictor_max_replicas",
          "optional": true, "type": "Integer"}, {"default": "1", "name": "predictor_min_replicas",
          "optional": true, "type": "Integer"}, {"name": "predictor_concurrency_target",
          "optional": true, "type": "Integer"}, {"default": "1", "name": "transformer_max_replicas",
          "optional": true, "type": "Integer"}, {"default": "1", "name": "transformer_min_replicas",
          "optional": true, "type": "Integer"}, {"name": "transformer_concurrency_target",
          "optional": true, "type": "Integer"}, {"default": "1", "name": "explainer_max_replicas",
          "optional": true, "type": "Integer"}, {"default": "1", "name": "explainer_min_replicas",
          "optional": true, "type": "Integer"}, {"name": "explainer_concurrency_target",
          "optional": true, "type": "Integer"}], "name": "Deploy inference service"}',
        pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"explainer_image":
          "quay.io/ntlawrence/demo-explainer:3.0.0", "explainer_max_replicas": "1",
          "explainer_min_replicas": "1", "model_archive_s3": "{{inputs.parameters.upload-artifacts-s3_address}}",
          "name": "credit-risk", "predictor_concurrency_target": "1", "predictor_image":
          "quay.io/ntlawrence/demo-predictor:3.0.0", "predictor_max_replicas": "4",
          "predictor_min_replicas": "1", "transformer_concurrency_target": "1", "transformer_image":
          "quay.io/ntlawrence/demo-transformer:3.0.0", "transformer_max_replicas":
          "4", "transformer_min_replicas": "1"}'}
  - name: evaluate
    container:
      args: [--df, /tmp/inputs/df/data, --preprocessor, /tmp/inputs/preprocessor/data,
        --onnx-model, /tmp/inputs/onnx_model/data, --target-processing-config, /tmp/inputs/target_processing_config/data,
        --target, Risk, --output-report, /tmp/outputs/output_report/data, --mlpipeline-ui-metadata,
        /tmp/outputs/mlpipeline_ui_metadata/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def evaluate(
            df,
            preprocessor,
            onnx_model,
            target_processing_config,
            output_report,
            mlpipeline_ui_metadata_path,
            target="Risk",
        ):
            import pandas as pd
            import joblib
            import json
            from evidently.metric_preset import ClassificationPreset
            from evidently.report import Report
            from evidently import ColumnMapping
            import os
            from pathlib import Path
            import onnxruntime as ort
            import numpy as np

            dataset = pd.read_pickle(df)
            preprocessor = joblib.load(preprocessor)

            inference_session = ort.InferenceSession(
                    onnx_model, providers=["CPUExecutionProvider"]
            )

            with open(target_processing_config, "r") as f:
                target_processing_config_dict = json.load(f)
                target_processing_config_dict["target_names"] = {int(k):v for k,v in target_processing_config_dict["target_names"].items()}

            column_info = json.loads(os.environ["COLUMNS"])

            X = preprocessor.transform(dataset).astype(np.float32)
            y_prob = np.array(
                inference_session.run(
                    [], {"input_1": X}
                )).flatten()

            dataset["Prediction"] = pd.Series(y_prob).apply(
                lambda p: 1 if p > target_processing_config_dict["threshold"] else 0
            )
            dataset["Actual"] = dataset.loc[:, target].apply(
                lambda v: 1 if v == target_processing_config_dict["target_names"][1] else 0
            )

            column_mapping = ColumnMapping()
            column_mapping.target_names = target_processing_config_dict["target_names"]
            column_mapping.target = "Actual"
            column_mapping.prediction = "Prediction"
            column_mapping.task = "classification"
            column_mapping.numerical_features = [
                c
                for c in column_info["int_columns"]
                if c in set(preprocessor.feature_names_in_)
            ]
            column_mapping.categorical_features = [
                c
                for c in column_info["label_columns"]
                if c in set(preprocessor.feature_names_in_)
            ]

            report = Report(
                metrics=[
                    ClassificationPreset(),
                ]
            )

            report.run(
                reference_data=None,
                current_data=dataset,
                column_mapping=column_mapping,
            )

            Path(output_report).parent.mkdir(parents=True, exist_ok=True)
            report.save_html(output_report)
            html_content = open(output_report, "r").read()
            metadata = {
                "outputs": [
                    {
                        "type": "web-app",
                        "storage": "inline",
                        "source": html_content,
                    }
                ]
            }

            with open(mlpipeline_ui_metadata_path, "w") as f:
                json.dump(metadata, f)

        import argparse
        _parser = argparse.ArgumentParser(prog='Evaluate', description='')
        _parser.add_argument("--df", dest="df", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--preprocessor", dest="preprocessor", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--onnx-model", dest="onnx_model", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--target-processing-config", dest="target_processing_config", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--target", dest="target", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--output-report", dest="output_report", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--mlpipeline-ui-metadata", dest="mlpipeline_ui_metadata_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = evaluate(**_parsed_args)
      env:
      - name: COLUMNS
        valueFrom:
          configMapKeyRef: {key: columns, name: credit-risk-columns}
      image: quay.io/ntlawrence/demo-workflow:3.0.0
    inputs:
      artifacts:
      - {name: load-df-from-postgresql-2-data_frame_pkl, path: /tmp/inputs/df/data}
      - {name: convert-model-to-onnx-onnx_model, path: /tmp/inputs/onnx_model/data}
      - {name: fit-preprocessor-preprocessor_pkl, path: /tmp/inputs/preprocessor/data}
      - {name: train-target_processing_config, path: /tmp/inputs/target_processing_config/data}
    outputs:
      artifacts:
      - {name: mlpipeline-ui-metadata, path: /tmp/outputs/mlpipeline_ui_metadata/data}
      - {name: evaluate-output_report, path: /tmp/outputs/output_report/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--df", {"inputPath": "df"}, "--preprocessor", {"inputPath": "preprocessor"},
          "--onnx-model", {"inputPath": "onnx_model"}, "--target-processing-config",
          {"inputPath": "target_processing_config"}, {"if": {"cond": {"isPresent":
          "target"}, "then": ["--target", {"inputValue": "target"}]}}, "--output-report",
          {"outputPath": "output_report"}, "--mlpipeline-ui-metadata", {"outputPath":
          "mlpipeline_ui_metadata"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef evaluate(\n    df,\n    preprocessor,\n    onnx_model,\n    target_processing_config,\n    output_report,\n    mlpipeline_ui_metadata_path,\n    target=\"Risk\",\n):\n    import
          pandas as pd\n    import joblib\n    import json\n    from evidently.metric_preset
          import ClassificationPreset\n    from evidently.report import Report\n    from
          evidently import ColumnMapping\n    import os\n    from pathlib import Path\n    import
          onnxruntime as ort\n    import numpy as np\n\n    dataset = pd.read_pickle(df)\n    preprocessor
          = joblib.load(preprocessor)\n\n    inference_session = ort.InferenceSession(\n            onnx_model,
          providers=[\"CPUExecutionProvider\"]\n    )\n\n    with open(target_processing_config,
          \"r\") as f:\n        target_processing_config_dict = json.load(f)\n        target_processing_config_dict[\"target_names\"]
          = {int(k):v for k,v in target_processing_config_dict[\"target_names\"].items()}\n\n    column_info
          = json.loads(os.environ[\"COLUMNS\"])\n\n    X = preprocessor.transform(dataset).astype(np.float32)\n    y_prob
          = np.array(\n        inference_session.run(\n            [], {\"input_1\":
          X}\n        )).flatten()\n\n    dataset[\"Prediction\"] = pd.Series(y_prob).apply(\n        lambda
          p: 1 if p > target_processing_config_dict[\"threshold\"] else 0\n    )\n    dataset[\"Actual\"]
          = dataset.loc[:, target].apply(\n        lambda v: 1 if v == target_processing_config_dict[\"target_names\"][1]
          else 0\n    )\n\n    column_mapping = ColumnMapping()\n    column_mapping.target_names
          = target_processing_config_dict[\"target_names\"]\n    column_mapping.target
          = \"Actual\"\n    column_mapping.prediction = \"Prediction\"\n    column_mapping.task
          = \"classification\"\n    column_mapping.numerical_features = [\n        c\n        for
          c in column_info[\"int_columns\"]\n        if c in set(preprocessor.feature_names_in_)\n    ]\n    column_mapping.categorical_features
          = [\n        c\n        for c in column_info[\"label_columns\"]\n        if
          c in set(preprocessor.feature_names_in_)\n    ]\n\n    report = Report(\n        metrics=[\n            ClassificationPreset(),\n        ]\n    )\n\n    report.run(\n        reference_data=None,\n        current_data=dataset,\n        column_mapping=column_mapping,\n    )\n\n    Path(output_report).parent.mkdir(parents=True,
          exist_ok=True)\n    report.save_html(output_report)\n    html_content =
          open(output_report, \"r\").read()\n    metadata = {\n        \"outputs\":
          [\n            {\n                \"type\": \"web-app\",\n                \"storage\":
          \"inline\",\n                \"source\": html_content,\n            }\n        ]\n    }\n\n    with
          open(mlpipeline_ui_metadata_path, \"w\") as f:\n        json.dump(metadata,
          f)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Evaluate'',
          description='''')\n_parser.add_argument(\"--df\", dest=\"df\", type=str,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--preprocessor\",
          dest=\"preprocessor\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--onnx-model\",
          dest=\"onnx_model\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--target-processing-config\",
          dest=\"target_processing_config\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--target\",
          dest=\"target\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-report\",
          dest=\"output_report\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlpipeline-ui-metadata\",
          dest=\"mlpipeline_ui_metadata_path\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = evaluate(**_parsed_args)\n"], "image": "quay.io/ntlawrence/demo-workflow:3.0.0"}},
          "inputs": [{"name": "df", "type": "String"}, {"name": "preprocessor", "type":
          "String"}, {"name": "onnx_model", "type": "String"}, {"name": "target_processing_config",
          "type": "String"}, {"default": "Risk", "name": "target", "optional": true}],
          "name": "Evaluate", "outputs": [{"name": "output_report", "type": "String"},
          {"name": "mlpipeline_ui_metadata", "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"target": "Risk"}'}
  - name: fit-preprocessor
    container:
      args: [--training-df, /tmp/inputs/training_df/data, --features, '["CheckingStatus",
          "LoanDuration", "CreditHistory", "LoanPurpose", "LoanAmount", "ExistingSavings",
          "EmploymentDuration", "InstallmentPercent", "Sex", "OthersOnLoan", "CurrentResidenceDuration",
          "OwnsProperty", "Age", "InstallmentPlans", "Housing", "ExistingCreditsCount",
          "Job", "Dependents", "Telephone", "ForeignWorker"]', --preprocessor-pkl,
        /tmp/outputs/preprocessor_pkl/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def fit_preprocessor(
            training_df,
            preprocessor_pkl,
            features,
        ):
            import pandas as pd
            import json
            import joblib
            import os

            from sklearn.compose import ColumnTransformer
            from sklearn.preprocessing import OneHotEncoder
            from sklearn.pipeline import Pipeline

            feature_set = set(features)
            column_info = json.loads(os.environ["COLUMNS"])

            ohe_labels = [
                (
                    "ohe_" + label,
                    OneHotEncoder(
                        handle_unknown="ignore", sparse_output=False, categories=[levels]
                    ),
                    [label],
                )
                for label, levels in column_info["label_columns"].items()
                if label in feature_set
            ]

            int_cols = [
                (
                    "passthrough",
                    "passthrough",
                    [col for col in column_info["int_columns"] if col in feature_set],
                )
            ]

            pipe = Pipeline(
                steps=[
                    ("Preprocess", ColumnTransformer(ohe_labels + int_cols, remainder="drop")),
                ]
            )

            print(pipe)
            train = pd.read_pickle(training_df)
            print(train.dtypes)
            pipe.fit(train)
            joblib.dump(pipe, preprocessor_pkl)

        import json
        import argparse
        _parser = argparse.ArgumentParser(prog='Fit preprocessor', description='')
        _parser.add_argument("--training-df", dest="training_df", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--features", dest="features", type=json.loads, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--preprocessor-pkl", dest="preprocessor_pkl", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = fit_preprocessor(**_parsed_args)
      env:
      - name: COLUMNS
        valueFrom:
          configMapKeyRef: {key: columns, name: credit-risk-columns}
      image: quay.io/ntlawrence/demo-workflow:3.0.0
    inputs:
      artifacts:
      - {name: load-df-from-postgresql-data_frame_pkl, path: /tmp/inputs/training_df/data}
    outputs:
      artifacts:
      - {name: fit-preprocessor-preprocessor_pkl, path: /tmp/outputs/preprocessor_pkl/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--training-df", {"inputPath": "training_df"}, "--features", {"inputValue":
          "features"}, "--preprocessor-pkl", {"outputPath": "preprocessor_pkl"}],
          "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" >
          \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef fit_preprocessor(\n    training_df,\n    preprocessor_pkl,\n    features,\n):\n    import
          pandas as pd\n    import json\n    import joblib\n    import os\n\n    from
          sklearn.compose import ColumnTransformer\n    from sklearn.preprocessing
          import OneHotEncoder\n    from sklearn.pipeline import Pipeline\n\n    feature_set
          = set(features)\n    column_info = json.loads(os.environ[\"COLUMNS\"])\n\n    ohe_labels
          = [\n        (\n            \"ohe_\" + label,\n            OneHotEncoder(\n                handle_unknown=\"ignore\",
          sparse_output=False, categories=[levels]\n            ),\n            [label],\n        )\n        for
          label, levels in column_info[\"label_columns\"].items()\n        if label
          in feature_set\n    ]\n\n    int_cols = [\n        (\n            \"passthrough\",\n            \"passthrough\",\n            [col
          for col in column_info[\"int_columns\"] if col in feature_set],\n        )\n    ]\n\n    pipe
          = Pipeline(\n        steps=[\n            (\"Preprocess\", ColumnTransformer(ohe_labels
          + int_cols, remainder=\"drop\")),\n        ]\n    )\n\n    print(pipe)\n    train
          = pd.read_pickle(training_df)\n    print(train.dtypes)\n    pipe.fit(train)\n    joblib.dump(pipe,
          preprocessor_pkl)\n\nimport json\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Fit
          preprocessor'', description='''')\n_parser.add_argument(\"--training-df\",
          dest=\"training_df\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--features\",
          dest=\"features\", type=json.loads, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--preprocessor-pkl\",
          dest=\"preprocessor_pkl\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = fit_preprocessor(**_parsed_args)\n"], "image": "quay.io/ntlawrence/demo-workflow:3.0.0"}},
          "inputs": [{"name": "training_df", "type": "String"}, {"name": "features",
          "type": "typing.List[str]"}], "name": "Fit preprocessor", "outputs": [{"name":
          "preprocessor_pkl", "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"features": "[\"CheckingStatus\",
          \"LoanDuration\", \"CreditHistory\", \"LoanPurpose\", \"LoanAmount\", \"ExistingSavings\",
          \"EmploymentDuration\", \"InstallmentPercent\", \"Sex\", \"OthersOnLoan\",
          \"CurrentResidenceDuration\", \"OwnsProperty\", \"Age\", \"InstallmentPlans\",
          \"Housing\", \"ExistingCreditsCount\", \"Job\", \"Dependents\", \"Telephone\",
          \"ForeignWorker\"]"}'}
  - name: load-df-from-postgresql
    container:
      args: [--table-name, TRAIN, --data-frame-pkl, /tmp/outputs/data_frame_pkl/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'psycopg[binary,pool]' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
        --quiet --no-warn-script-location 'psycopg[binary,pool]' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def load_df_from_postgresql(table_name,
                                   data_frame_pkl):
            import os
            import json
            import pandas as pd
            import pickle
            from typing import Dict, Any
            import psycopg
            from psycopg import sql
            import yaml

            def get_pg_conn():
                host, dbname, username, password, port = (
                    os.environ.get('PG_HOST'),
                    os.environ.get('PG_DB_NAME'),
                    os.environ.get('PG_USER'),
                    os.environ.get('PG_PWD'),
                    int(os.environ.get('PG_PORT'))
                )

                conn_str = f"postgresql://{username}:{password}@{host}:{port}/{dbname}?connect_timeout=10&application_name=mlpipeline"
                print(conn_str)
                conn = psycopg.connect(conn_str)

                return conn

            def assign_categories_to_df(df, column_info):
                for col_name, levels in column_info["label_columns"].items():
                    if col_name in df.columns:
                        ctype = pd.CategoricalDtype(categories=levels, ordered=False)
                        df[col_name] = df[col_name].astype(ctype)

            def df_from_sql(
                name,
                db,
                column_info,
            ):
                with db.cursor() as cur:
                    cur.execute(sql.SQL('SELECT * FROM {} ORDER BY "ACCOUNT_ID"').format(sql.Identifier(table_name)))
                    df = pd.DataFrame(cur.fetchall(), columns=[desc[0] for desc in cur.description])
                assign_categories_to_df(df, column_info)

                return df

            conn = get_pg_conn()
            column_info = json.loads(os.environ["COLUMNS"])
            df = df_from_sql(table_name, conn, column_info)
            df.to_pickle(data_frame_pkl)

        import argparse
        _parser = argparse.ArgumentParser(prog='Load df from postgresql', description='')
        _parser.add_argument("--table-name", dest="table_name", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--data-frame-pkl", dest="data_frame_pkl", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = load_df_from_postgresql(**_parsed_args)
      env:
      - {name: PG_HOST, value: 'postgresql.{{workflow.namespace}}.svc'}
      - name: PG_DB_NAME
        valueFrom:
          secretKeyRef: {key: database-name, name: postgresql}
      - name: PG_USER
        valueFrom:
          secretKeyRef: {key: database-user, name: postgresql}
      - name: PG_PWD
        valueFrom:
          secretKeyRef: {key: database-password, name: postgresql}
      - {name: PG_PORT, value: '5432'}
      - name: COLUMNS
        valueFrom:
          configMapKeyRef: {key: columns, name: credit-risk-columns}
      image: quay.io/ntlawrence/demo-workflow:3.0.0
    outputs:
      artifacts:
      - {name: load-df-from-postgresql-data_frame_pkl, path: /tmp/outputs/data_frame_pkl/data}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: Load_Training_Data,
        pipelines.kubeflow.org/component_spec: '{"implementation": {"container": {"args":
          ["--table-name", {"inputValue": "table_name"}, "--data-frame-pkl", {"outputPath":
          "data_frame_pkl"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''psycopg[binary,pool]''
          || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''psycopg[binary,pool]'' --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef load_df_from_postgresql(table_name,\n                           data_frame_pkl):\n    import
          os\n    import json\n    import pandas as pd\n    import pickle\n    from
          typing import Dict, Any\n    import psycopg\n    from psycopg import sql\n    import
          yaml\n\n    def get_pg_conn():\n        host, dbname, username, password,
          port = (\n            os.environ.get(''PG_HOST''),\n            os.environ.get(''PG_DB_NAME''),\n            os.environ.get(''PG_USER''),\n            os.environ.get(''PG_PWD''),\n            int(os.environ.get(''PG_PORT''))\n        )\n\n        conn_str
          = f\"postgresql://{username}:{password}@{host}:{port}/{dbname}?connect_timeout=10&application_name=mlpipeline\"\n        print(conn_str)\n        conn
          = psycopg.connect(conn_str)\n\n        return conn\n\n    def assign_categories_to_df(df,
          column_info):\n        for col_name, levels in column_info[\"label_columns\"].items():\n            if
          col_name in df.columns:\n                ctype = pd.CategoricalDtype(categories=levels,
          ordered=False)\n                df[col_name] = df[col_name].astype(ctype)\n\n    def
          df_from_sql(\n        name,\n        db,\n        column_info,\n    ):\n        with
          db.cursor() as cur:\n            cur.execute(sql.SQL(''SELECT * FROM {}
          ORDER BY \"ACCOUNT_ID\"'').format(sql.Identifier(table_name)))\n            df
          = pd.DataFrame(cur.fetchall(), columns=[desc[0] for desc in cur.description])\n        assign_categories_to_df(df,
          column_info)\n\n        return df\n\n    conn = get_pg_conn()\n    column_info
          = json.loads(os.environ[\"COLUMNS\"])\n    df = df_from_sql(table_name,
          conn, column_info)\n    df.to_pickle(data_frame_pkl)\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Load df from postgresql'', description='''')\n_parser.add_argument(\"--table-name\",
          dest=\"table_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--data-frame-pkl\",
          dest=\"data_frame_pkl\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = load_df_from_postgresql(**_parsed_args)\n"], "image": "quay.io/ntlawrence/demo-workflow:3.0.0"}},
          "inputs": [{"name": "table_name", "type": "String"}], "name": "Load df from
          postgresql", "outputs": [{"name": "data_frame_pkl", "type": "String"}]}',
        pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"table_name":
          "TRAIN"}', pipelines.kubeflow.org/max_cache_staleness: P0D}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: load-df-from-postgresql-2
    container:
      args: [--table-name, TEST, --data-frame-pkl, /tmp/outputs/data_frame_pkl/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'psycopg[binary,pool]' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
        --quiet --no-warn-script-location 'psycopg[binary,pool]' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def load_df_from_postgresql(table_name,
                                   data_frame_pkl):
            import os
            import json
            import pandas as pd
            import pickle
            from typing import Dict, Any
            import psycopg
            from psycopg import sql
            import yaml

            def get_pg_conn():
                host, dbname, username, password, port = (
                    os.environ.get('PG_HOST'),
                    os.environ.get('PG_DB_NAME'),
                    os.environ.get('PG_USER'),
                    os.environ.get('PG_PWD'),
                    int(os.environ.get('PG_PORT'))
                )

                conn_str = f"postgresql://{username}:{password}@{host}:{port}/{dbname}?connect_timeout=10&application_name=mlpipeline"
                print(conn_str)
                conn = psycopg.connect(conn_str)

                return conn

            def assign_categories_to_df(df, column_info):
                for col_name, levels in column_info["label_columns"].items():
                    if col_name in df.columns:
                        ctype = pd.CategoricalDtype(categories=levels, ordered=False)
                        df[col_name] = df[col_name].astype(ctype)

            def df_from_sql(
                name,
                db,
                column_info,
            ):
                with db.cursor() as cur:
                    cur.execute(sql.SQL('SELECT * FROM {} ORDER BY "ACCOUNT_ID"').format(sql.Identifier(table_name)))
                    df = pd.DataFrame(cur.fetchall(), columns=[desc[0] for desc in cur.description])
                assign_categories_to_df(df, column_info)

                return df

            conn = get_pg_conn()
            column_info = json.loads(os.environ["COLUMNS"])
            df = df_from_sql(table_name, conn, column_info)
            df.to_pickle(data_frame_pkl)

        import argparse
        _parser = argparse.ArgumentParser(prog='Load df from postgresql', description='')
        _parser.add_argument("--table-name", dest="table_name", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--data-frame-pkl", dest="data_frame_pkl", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = load_df_from_postgresql(**_parsed_args)
      env:
      - {name: PG_HOST, value: 'postgresql.{{workflow.namespace}}.svc'}
      - name: PG_DB_NAME
        valueFrom:
          secretKeyRef: {key: database-name, name: postgresql}
      - name: PG_USER
        valueFrom:
          secretKeyRef: {key: database-user, name: postgresql}
      - name: PG_PWD
        valueFrom:
          secretKeyRef: {key: database-password, name: postgresql}
      - {name: PG_PORT, value: '5432'}
      - name: COLUMNS
        valueFrom:
          configMapKeyRef: {key: columns, name: credit-risk-columns}
      image: quay.io/ntlawrence/demo-workflow:3.0.0
    outputs:
      artifacts:
      - {name: load-df-from-postgresql-2-data_frame_pkl, path: /tmp/outputs/data_frame_pkl/data}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: Load_Test_Data, pipelines.kubeflow.org/component_spec: '{"implementation":
          {"container": {"args": ["--table-name", {"inputValue": "table_name"}, "--data-frame-pkl",
          {"outputPath": "data_frame_pkl"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''psycopg[binary,pool]''
          || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''psycopg[binary,pool]'' --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef load_df_from_postgresql(table_name,\n                           data_frame_pkl):\n    import
          os\n    import json\n    import pandas as pd\n    import pickle\n    from
          typing import Dict, Any\n    import psycopg\n    from psycopg import sql\n    import
          yaml\n\n    def get_pg_conn():\n        host, dbname, username, password,
          port = (\n            os.environ.get(''PG_HOST''),\n            os.environ.get(''PG_DB_NAME''),\n            os.environ.get(''PG_USER''),\n            os.environ.get(''PG_PWD''),\n            int(os.environ.get(''PG_PORT''))\n        )\n\n        conn_str
          = f\"postgresql://{username}:{password}@{host}:{port}/{dbname}?connect_timeout=10&application_name=mlpipeline\"\n        print(conn_str)\n        conn
          = psycopg.connect(conn_str)\n\n        return conn\n\n    def assign_categories_to_df(df,
          column_info):\n        for col_name, levels in column_info[\"label_columns\"].items():\n            if
          col_name in df.columns:\n                ctype = pd.CategoricalDtype(categories=levels,
          ordered=False)\n                df[col_name] = df[col_name].astype(ctype)\n\n    def
          df_from_sql(\n        name,\n        db,\n        column_info,\n    ):\n        with
          db.cursor() as cur:\n            cur.execute(sql.SQL(''SELECT * FROM {}
          ORDER BY \"ACCOUNT_ID\"'').format(sql.Identifier(table_name)))\n            df
          = pd.DataFrame(cur.fetchall(), columns=[desc[0] for desc in cur.description])\n        assign_categories_to_df(df,
          column_info)\n\n        return df\n\n    conn = get_pg_conn()\n    column_info
          = json.loads(os.environ[\"COLUMNS\"])\n    df = df_from_sql(table_name,
          conn, column_info)\n    df.to_pickle(data_frame_pkl)\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Load df from postgresql'', description='''')\n_parser.add_argument(\"--table-name\",
          dest=\"table_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--data-frame-pkl\",
          dest=\"data_frame_pkl\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = load_df_from_postgresql(**_parsed_args)\n"], "image": "quay.io/ntlawrence/demo-workflow:3.0.0"}},
          "inputs": [{"name": "table_name", "type": "String"}], "name": "Load df from
          postgresql", "outputs": [{"name": "data_frame_pkl", "type": "String"}]}',
        pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"table_name":
          "TEST"}', pipelines.kubeflow.org/max_cache_staleness: P0D}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: train
    container:
      args: [--training-df, /tmp/inputs/training_df/data, --preprocessor, /tmp/inputs/preprocessor/data,
        --target, Risk, --tensorboard-dir, /tensorboard, --model, /tmp/outputs/model/data,
        --target-processing-config, /tmp/outputs/target_processing_config/data, --mlpipeline-ui-metadata,
        /tmp/outputs/mlpipeline_ui_metadata/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'minio' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'minio' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef train(\n    training_df,\n    preprocessor,\n    model,\n    target_processing_config,\n\
        \    mlpipeline_ui_metadata_path,\n    target = \"Risk\",\n    tensorboard_dir\
        \ = None,\n):\n    import pandas as pd\n    import json\n    import joblib\n\
        \    import tensorflow as tf\n    from keras import Sequential\n    from keras.layers\
        \ import Dense, Dropout, BatchNormalization, Input\n    from keras.callbacks\
        \ import EarlyStopping, ReduceLROnPlateau, TensorBoard\n    from sklearn.metrics\
        \ import precision_recall_curve\n    import numpy as np\n    import os\n \
        \   from sklearn.metrics import PrecisionRecallDisplay\n    import base64\n\
        \n    tf.keras.utils.set_random_seed(42)\n    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.DEBUG)\n\
        \    target_processing_config_dict = {\n        \"threshold\" : 0.5,\n   \
        \     \"target_names\" : {0: \"No Risk\", 1: \"Risk\"}\n    }\n\n    def get_tf_model(num_features):\n\
        \n        tf_model = Sequential(\n            [\n                Input(shape=(num_features,)),\n\
        \                BatchNormalization(),\n                Dense(64, activation=\"\
        sigmoid\", name=\"layer1\"),\n                BatchNormalization(),\n    \
        \            Dropout(0.3, name=\"dropout1\"),\n                Dense(32, activation=\"\
        sigmoid\", name=\"layer2\"),\n                BatchNormalization(),\n    \
        \            Dropout(0.3, name=\"dropout2\"),\n                Dense(16, activation=\"\
        sigmoid\", name=\"layer3\"),\n                Dropout(0.3, name=\"dropout3\"\
        ),\n                Dense(\n                    1,\n                    activation=\"\
        sigmoid\",\n                    name=\"output\",\n                ),\n   \
        \         ]\n        )\n\n        tf_model.compile(optimizer=\"adam\", loss=\"\
        binary_crossentropy\")\n\n        callbacks = [\n            EarlyStopping(\n\
        \                monitor=\"val_loss\",\n                patience=20,\n   \
        \             verbose=0,\n                mode=\"min\",\n                restore_best_weights=True,\n\
        \            ),\n            ReduceLROnPlateau(\n                monitor=\"\
        val_loss\",\n                factor=0.1,\n                patience=10,\n \
        \               verbose=1,\n                min_delta=0.0001,\n          \
        \      mode=\"min\",\n            )\n        ]\n        print(f\"constructing\
        \ tensorborad with log dir {tensorboard_dir}...\")\n        if tensorboard_dir:\n\
        \            callbacks.append(TensorBoard(\n                log_dir=tensorboard_dir\n\
        \            ))\n\n        return tf_model, callbacks\n\n    print(\"loading\
        \ training data...\")\n    train = pd.read_pickle(training_df)\n    print(\"\
        loading preprocessor...\")\n    preprocessor = joblib.load(preprocessor)\n\
        \n    X = tf.convert_to_tensor(preprocessor.transform(train))\n    y = tf.convert_to_tensor(\n\
        \        train.loc[:, target].apply(lambda v: 1 if v == target_processing_config_dict[\"\
        target_names\"][1] else 0)\n    )\n    print(\"obtaining model....\")\n  \
        \  tf_model, callbacks = get_tf_model(num_features=X.shape[1])\n    print(\"\
        Training...\")\n    tf_model.fit(\n        X,\n        y,\n        validation_split=0.2,\n\
        \        epochs=500,\n        callbacks=callbacks,\n        class_weight={0:\
        \ 1, 1: 2},\n    )\n\n    # calculate best threshold of highest f1 score\n\
        \    predictions = tf_model.predict(X)\n    precision, recall, thresholds\
        \ = precision_recall_curve(\n        y_true=y.numpy(), probas_pred=predictions.flatten()\n\
        \    )\n    f1s = 2 * (precision * recall) / (precision + recall)\n    threshold_index\
        \ = np.argmax(f1s)\n    target_processing_config_dict[\"threshold\"] = float(thresholds[threshold_index])\n\
        \n    # Save model and threshold config\n    tf_model.save(model, save_format=\"\
        h5\")\n    with open(target_processing_config, \"w\") as f:\n        json.dump(target_processing_config_dict,\
        \ f)\n\n    # Plot precision recall curve\n    plt = PrecisionRecallDisplay.from_predictions(\n\
        \        y_true=y.numpy(), y_pred=predictions.flatten(),\n    )\n    plt.ax_.plot(recall[threshold_index],\
        \ precision[threshold_index], \n                     marker=\"o\", markersize=10,\
        \ markeredgecolor=\"black\", markerfacecolor=\"red\")\n    plt.ax_.text(recall[threshold_index]\
        \ + .03, precision[threshold_index] + .03,\n                  (f\"({recall[threshold_index]:.3f},{precision[threshold_index]:.3f})\\\
        n\" +\n                   f\"threshold={thresholds[threshold_index]:.3f}\\\
        n\" +\n                   f\"f1={f1s[threshold_index]:.3f}\")\n          \
        \      )\n    plt.ax_.set_title(\"Precision Recall - Risk (Training)\")\n\n\
        \    plt.figure_.savefig(\"pr.jpg\")\n    with open(\"pr.jpg\", \"rb\") as\
        \ f:\n        jpg = base64.b64encode(f.read())\n    html = f'<img src=\"data:image/jpg;base64,{jpg.decode(\"\
        utf-8\")}\"/>'\n    metadata = {\"outputs\": [{\"type\": \"markdown\", \"\
        storage\": \"inline\", \"source\": html}]}\n    with open(mlpipeline_ui_metadata_path,\
        \ \"w\") as metadata_file:\n        json.dump(metadata, metadata_file)\n\n\
        import argparse\n_parser = argparse.ArgumentParser(prog='Train', description='')\n\
        _parser.add_argument(\"--training-df\", dest=\"training_df\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--preprocessor\", dest=\"\
        preprocessor\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --target\", dest=\"target\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--tensorboard-dir\", dest=\"tensorboard_dir\", type=str,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model\"\
        , dest=\"model\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--target-processing-config\", dest=\"target_processing_config\"\
        , type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--mlpipeline-ui-metadata\", dest=\"mlpipeline_ui_metadata_path\"\
        , type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n\
        _parsed_args = vars(_parser.parse_args())\n\n_outputs = train(**_parsed_args)\n"
      env:
      - name: COLUMNS
        valueFrom:
          configMapKeyRef: {key: columns, name: credit-risk-columns}
      image: quay.io/ntlawrence/demo-workflow:3.0.0
      volumeMounts:
      - {mountPath: /tensorboard, name: create-pvc-for-tensorboard}
    inputs:
      parameters:
      - {name: create-pvc-for-tensorboard-name}
      artifacts:
      - {name: fit-preprocessor-preprocessor_pkl, path: /tmp/inputs/preprocessor/data}
      - {name: load-df-from-postgresql-data_frame_pkl, path: /tmp/inputs/training_df/data}
    outputs:
      artifacts:
      - {name: mlpipeline-ui-metadata, path: /tmp/outputs/mlpipeline_ui_metadata/data}
      - {name: train-model, path: /tmp/outputs/model/data}
      - {name: train-target_processing_config, path: /tmp/outputs/target_processing_config/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--training-df", {"inputPath": "training_df"}, "--preprocessor",
          {"inputPath": "preprocessor"}, {"if": {"cond": {"isPresent": "target"},
          "then": ["--target", {"inputValue": "target"}]}}, {"if": {"cond": {"isPresent":
          "tensorboard_dir"}, "then": ["--tensorboard-dir", {"inputValue": "tensorboard_dir"}]}},
          "--model", {"outputPath": "model"}, "--target-processing-config", {"outputPath":
          "target_processing_config"}, "--mlpipeline-ui-metadata", {"outputPath":
          "mlpipeline_ui_metadata"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''minio'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''minio'' --user)
          && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef train(\n    training_df,\n    preprocessor,\n    model,\n    target_processing_config,\n    mlpipeline_ui_metadata_path,\n    target
          = \"Risk\",\n    tensorboard_dir = None,\n):\n    import pandas as pd\n    import
          json\n    import joblib\n    import tensorflow as tf\n    from keras import
          Sequential\n    from keras.layers import Dense, Dropout, BatchNormalization,
          Input\n    from keras.callbacks import EarlyStopping, ReduceLROnPlateau,
          TensorBoard\n    from sklearn.metrics import precision_recall_curve\n    import
          numpy as np\n    import os\n    from sklearn.metrics import PrecisionRecallDisplay\n    import
          base64\n\n    tf.keras.utils.set_random_seed(42)\n    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.DEBUG)\n    target_processing_config_dict
          = {\n        \"threshold\" : 0.5,\n        \"target_names\" : {0: \"No Risk\",
          1: \"Risk\"}\n    }\n\n    def get_tf_model(num_features):\n\n        tf_model
          = Sequential(\n            [\n                Input(shape=(num_features,)),\n                BatchNormalization(),\n                Dense(64,
          activation=\"sigmoid\", name=\"layer1\"),\n                BatchNormalization(),\n                Dropout(0.3,
          name=\"dropout1\"),\n                Dense(32, activation=\"sigmoid\", name=\"layer2\"),\n                BatchNormalization(),\n                Dropout(0.3,
          name=\"dropout2\"),\n                Dense(16, activation=\"sigmoid\", name=\"layer3\"),\n                Dropout(0.3,
          name=\"dropout3\"),\n                Dense(\n                    1,\n                    activation=\"sigmoid\",\n                    name=\"output\",\n                ),\n            ]\n        )\n\n        tf_model.compile(optimizer=\"adam\",
          loss=\"binary_crossentropy\")\n\n        callbacks = [\n            EarlyStopping(\n                monitor=\"val_loss\",\n                patience=20,\n                verbose=0,\n                mode=\"min\",\n                restore_best_weights=True,\n            ),\n            ReduceLROnPlateau(\n                monitor=\"val_loss\",\n                factor=0.1,\n                patience=10,\n                verbose=1,\n                min_delta=0.0001,\n                mode=\"min\",\n            )\n        ]\n        print(f\"constructing
          tensorborad with log dir {tensorboard_dir}...\")\n        if tensorboard_dir:\n            callbacks.append(TensorBoard(\n                log_dir=tensorboard_dir\n            ))\n\n        return
          tf_model, callbacks\n\n    print(\"loading training data...\")\n    train
          = pd.read_pickle(training_df)\n    print(\"loading preprocessor...\")\n    preprocessor
          = joblib.load(preprocessor)\n\n    X = tf.convert_to_tensor(preprocessor.transform(train))\n    y
          = tf.convert_to_tensor(\n        train.loc[:, target].apply(lambda v: 1
          if v == target_processing_config_dict[\"target_names\"][1] else 0)\n    )\n    print(\"obtaining
          model....\")\n    tf_model, callbacks = get_tf_model(num_features=X.shape[1])\n    print(\"Training...\")\n    tf_model.fit(\n        X,\n        y,\n        validation_split=0.2,\n        epochs=500,\n        callbacks=callbacks,\n        class_weight={0:
          1, 1: 2},\n    )\n\n    # calculate best threshold of highest f1 score\n    predictions
          = tf_model.predict(X)\n    precision, recall, thresholds = precision_recall_curve(\n        y_true=y.numpy(),
          probas_pred=predictions.flatten()\n    )\n    f1s = 2 * (precision * recall)
          / (precision + recall)\n    threshold_index = np.argmax(f1s)\n    target_processing_config_dict[\"threshold\"]
          = float(thresholds[threshold_index])\n\n    # Save model and threshold config\n    tf_model.save(model,
          save_format=\"h5\")\n    with open(target_processing_config, \"w\") as f:\n        json.dump(target_processing_config_dict,
          f)\n\n    # Plot precision recall curve\n    plt = PrecisionRecallDisplay.from_predictions(\n        y_true=y.numpy(),
          y_pred=predictions.flatten(),\n    )\n    plt.ax_.plot(recall[threshold_index],
          precision[threshold_index], \n                     marker=\"o\", markersize=10,
          markeredgecolor=\"black\", markerfacecolor=\"red\")\n    plt.ax_.text(recall[threshold_index]
          + .03, precision[threshold_index] + .03,\n                  (f\"({recall[threshold_index]:.3f},{precision[threshold_index]:.3f})\\n\"
          +\n                   f\"threshold={thresholds[threshold_index]:.3f}\\n\"
          +\n                   f\"f1={f1s[threshold_index]:.3f}\")\n                )\n    plt.ax_.set_title(\"Precision
          Recall - Risk (Training)\")\n\n    plt.figure_.savefig(\"pr.jpg\")\n    with
          open(\"pr.jpg\", \"rb\") as f:\n        jpg = base64.b64encode(f.read())\n    html
          = f''<img src=\"data:image/jpg;base64,{jpg.decode(\"utf-8\")}\"/>''\n    metadata
          = {\"outputs\": [{\"type\": \"markdown\", \"storage\": \"inline\", \"source\":
          html}]}\n    with open(mlpipeline_ui_metadata_path, \"w\") as metadata_file:\n        json.dump(metadata,
          metadata_file)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Train'',
          description='''')\n_parser.add_argument(\"--training-df\", dest=\"training_df\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--preprocessor\",
          dest=\"preprocessor\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--target\",
          dest=\"target\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--tensorboard-dir\",
          dest=\"tensorboard_dir\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model\",
          dest=\"model\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--target-processing-config\",
          dest=\"target_processing_config\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlpipeline-ui-metadata\",
          dest=\"mlpipeline_ui_metadata_path\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = train(**_parsed_args)\n"], "image": "quay.io/ntlawrence/demo-workflow:3.0.0"}},
          "inputs": [{"name": "training_df", "type": "String"}, {"name": "preprocessor",
          "type": "String"}, {"default": "Risk", "name": "target", "optional": true,
          "type": "String"}, {"name": "tensorboard_dir", "optional": true, "type":
          "String"}], "name": "Train", "outputs": [{"name": "model", "type": "String"},
          {"name": "target_processing_config", "type": "String"}, {"name": "mlpipeline_ui_metadata",
          "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"target":
          "Risk", "tensorboard_dir": "/tensorboard"}'}
    volumes:
    - name: create-pvc-for-tensorboard
      persistentVolumeClaim: {claimName: '{{inputs.parameters.create-pvc-for-tensorboard-name}}'}
  - name: train-explainer
    container:
      args: [--train-df, /tmp/inputs/train_df/data, --preprocessor, /tmp/inputs/preprocessor/data,
        --onnx-model, /tmp/inputs/onnx_model/data, --target-processing-config, /tmp/inputs/target_processing_config/data,
        --explainer-dll, /tmp/outputs/explainer_dll/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def train_explainer(train_df,
                            preprocessor,
                            onnx_model,
                            target_processing_config,
                            explainer_dll):
            from alibi.explainers.anchors import anchor_tabular
            from alibi.utils import gen_category_map
            import joblib
            from sklearn.pipeline import Pipeline
            import re
            import numpy as np
            import dill
            from functools import partial
            from typing import Tuple, List, Dict
            import json
            import os
            import pandas as pd
            import onnxruntime as ort
            import logging

            logging.basicConfig()

            def generate_category_map(preprocessor_pipeline):
                features = []
                seen_features = set()
                category_map = dict()

                for out_col_name in preprocessor_pipeline.get_feature_names_out():
                    parts = re.search("(.*)__([^_]+)(_([A-Za-z0-9_-]+))?", out_col_name)
                    if parts:
                        if parts.group(2) not in seen_features:
                            features.append(parts.group(2))
                            seen_features.add(parts.group(2))

                        if parts.group(1) == "categorical" or parts.group(1).startswith("ohe_"):
                            levels = category_map.get(len(features) - 1, [])
                            levels.append(parts.group(4))
                            category_map[len(features) - 1] = levels
                    else:
                        raise ValueError("Could not parse column " + out_col_name)

                return features, category_map

            with open(target_processing_config, "r") as f:
                target_processing_config_dict = json.load(f)
            column_info = json.loads(os.environ["COLUMNS"])

            inference_session = ort.InferenceSession(
                    onnx_model, providers=["CPUExecutionProvider"]
                )
            threshold:float = float(target_processing_config_dict["threshold"])
            def predict(X):
                scores= np.array(inference_session.run(
                    [], {"input_1": X}
                )).flatten()
                predictions = pd.Series(scores).apply(lambda p: 1 if p > threshold else 0).to_numpy()
                return predictions

            preprocessor_pipeline = joblib.load(preprocessor)
            features, category_map = generate_category_map(preprocessor_pipeline)

            dataset = pd.read_pickle(train_df)
            X = preprocessor_pipeline.transform(dataset).astype(np.float32)

            logging.info(f"Training explainer: features={features} categories={category_map} X.shape = {X.shape}")
            explainer = anchor_tabular.AnchorTabular(
                predictor=predict, feature_names=features, categorical_names=category_map, ohe=True, seed=42
            )
            explainer.fit(X, disc_prec=[10,25,33,50,66,75,90])

            explainer.reset_predictor(None)   # Clear explainer predict_fn as its a lambda and will be reset when loaded
            with open(explainer_dll, "wb") as f:
                dill.dump(explainer, f)

        import argparse
        _parser = argparse.ArgumentParser(prog='Train explainer', description='')
        _parser.add_argument("--train-df", dest="train_df", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--preprocessor", dest="preprocessor", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--onnx-model", dest="onnx_model", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--target-processing-config", dest="target_processing_config", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--explainer-dll", dest="explainer_dll", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = train_explainer(**_parsed_args)
      env:
      - name: COLUMNS
        valueFrom:
          configMapKeyRef: {key: columns, name: credit-risk-columns}
      image: quay.io/ntlawrence/demo-workflow:3.0.0
    inputs:
      artifacts:
      - {name: convert-model-to-onnx-onnx_model, path: /tmp/inputs/onnx_model/data}
      - {name: fit-preprocessor-preprocessor_pkl, path: /tmp/inputs/preprocessor/data}
      - {name: train-target_processing_config, path: /tmp/inputs/target_processing_config/data}
      - {name: load-df-from-postgresql-data_frame_pkl, path: /tmp/inputs/train_df/data}
    outputs:
      artifacts:
      - {name: train-explainer-explainer_dll, path: /tmp/outputs/explainer_dll/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--train-df", {"inputPath": "train_df"}, "--preprocessor", {"inputPath":
          "preprocessor"}, "--onnx-model", {"inputPath": "onnx_model"}, "--target-processing-config",
          {"inputPath": "target_processing_config"}, "--explainer-dll", {"outputPath":
          "explainer_dll"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef train_explainer(train_df,\n                    preprocessor,\n                    onnx_model,\n                    target_processing_config,\n                    explainer_dll):\n    from
          alibi.explainers.anchors import anchor_tabular\n    from alibi.utils import
          gen_category_map\n    import joblib\n    from sklearn.pipeline import Pipeline\n    import
          re\n    import numpy as np\n    import dill\n    from functools import partial\n    from
          typing import Tuple, List, Dict\n    import json\n    import os\n    import
          pandas as pd\n    import onnxruntime as ort\n    import logging\n\n    logging.basicConfig()\n\n    def
          generate_category_map(preprocessor_pipeline):\n        features = []\n        seen_features
          = set()\n        category_map = dict()\n\n        for out_col_name in preprocessor_pipeline.get_feature_names_out():\n            parts
          = re.search(\"(.*)__([^_]+)(_([A-Za-z0-9_-]+))?\", out_col_name)\n            if
          parts:\n                if parts.group(2) not in seen_features:\n                    features.append(parts.group(2))\n                    seen_features.add(parts.group(2))\n\n                if
          parts.group(1) == \"categorical\" or parts.group(1).startswith(\"ohe_\"):\n                    levels
          = category_map.get(len(features) - 1, [])\n                    levels.append(parts.group(4))\n                    category_map[len(features)
          - 1] = levels\n            else:\n                raise ValueError(\"Could
          not parse column \" + out_col_name)\n\n        return features, category_map\n\n    with
          open(target_processing_config, \"r\") as f:\n        target_processing_config_dict
          = json.load(f)\n    column_info = json.loads(os.environ[\"COLUMNS\"])\n\n    inference_session
          = ort.InferenceSession(\n            onnx_model, providers=[\"CPUExecutionProvider\"]\n        )\n    threshold:float
          = float(target_processing_config_dict[\"threshold\"])\n    def predict(X):\n        scores=
          np.array(inference_session.run(\n            [], {\"input_1\": X}\n        )).flatten()\n        predictions
          = pd.Series(scores).apply(lambda p: 1 if p > threshold else 0).to_numpy()\n        return
          predictions\n\n    preprocessor_pipeline = joblib.load(preprocessor)\n    features,
          category_map = generate_category_map(preprocessor_pipeline)\n\n    dataset
          = pd.read_pickle(train_df)\n    X = preprocessor_pipeline.transform(dataset).astype(np.float32)\n\n    logging.info(f\"Training
          explainer: features={features} categories={category_map} X.shape = {X.shape}\")\n    explainer
          = anchor_tabular.AnchorTabular(\n        predictor=predict, feature_names=features,
          categorical_names=category_map, ohe=True, seed=42\n    )\n    explainer.fit(X,
          disc_prec=[10,25,33,50,66,75,90])\n\n    explainer.reset_predictor(None)   #
          Clear explainer predict_fn as its a lambda and will be reset when loaded\n    with
          open(explainer_dll, \"wb\") as f:\n        dill.dump(explainer, f)\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Train explainer'', description='''')\n_parser.add_argument(\"--train-df\",
          dest=\"train_df\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--preprocessor\",
          dest=\"preprocessor\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--onnx-model\",
          dest=\"onnx_model\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--target-processing-config\",
          dest=\"target_processing_config\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--explainer-dll\",
          dest=\"explainer_dll\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = train_explainer(**_parsed_args)\n"], "image": "quay.io/ntlawrence/demo-workflow:3.0.0"}},
          "inputs": [{"name": "train_df", "type": "String"}, {"name": "preprocessor",
          "type": "String"}, {"name": "onnx_model", "type": "String"}, {"name": "target_processing_config",
          "type": "String"}], "name": "Train explainer", "outputs": [{"name": "explainer_dll",
          "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: upload-artifacts
    container:
      args: [--onnx-model, /tmp/inputs/onnx_model/data, --preprocessor, /tmp/inputs/preprocessor/data,
        --explainer, /tmp/inputs/explainer/data, --archive-name, credit-risk.tar,
        --minio-url, 'minio-service.kubeflow:9000', --version, '1', '----output-paths',
        /tmp/outputs/s3_address/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'minio==7.1.13' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
        --quiet --no-warn-script-location 'minio==7.1.13' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def upload_artifacts(\n    onnx_model,\n    preprocessor,\n    explainer,\n\
        \    archive_name,\n    minio_url = \"minio-service.kubeflow:9000\",\n   \
        \ version = \"1\"\n):\n    \"\"\"Uploads a model file to MinIO artifact store.\"\
        \"\"\n\n    from collections import namedtuple\n    import logging\n    from\
        \ minio import Minio\n    import sys\n    import tarfile\n    import os\n\n\
        \    logging.basicConfig(\n        stream=sys.stdout,\n        level=logging.INFO,\n\
        \        format=\"%(levelname)s %(asctime)s: %(message)s\",\n    )\n    logger\
        \ = logging.getLogger()\n\n    ARCHIVE_FILE = f\"/tmp/{archive_name}\"\n \
        \   with tarfile.open(ARCHIVE_FILE, \"w\") as f:\n        f.add(onnx_model,\
        \ arcname=\"model.onnx\")\n        f.add(preprocessor, arcname=\"preprocessor.joblib\"\
        )\n        f.add(explainer, arcname=\"explainer.dll\")\n\n    minio_client\
        \ = Minio(\n            minio_url, \n            access_key=os.environ[\"\
        MINIO_ID\"], \n            secret_key=os.environ[\"MINIO_PWD\"], secure=False\n\
        \        )\n\n    # Create export bucket if it does not yet exist\n    export_bucket=\"\
        {{workflow.namespace}}\"\n    existing_bucket = next(filter(lambda bucket:\
        \ bucket.name == export_bucket, minio_client.list_buckets()), None)\n\n  \
        \  if not existing_bucket:\n        logger.info(f\"Creating bucket '{export_bucket}'...\"\
        )\n        minio_client.make_bucket(bucket_name=export_bucket)\n\n    path\
        \ = f\"tar/{version}/{archive_name}\"\n    s3_address = f\"s3://{export_bucket}/tar\"\
        \n\n    logger.info(f\"Saving onnx file to MinIO (s3 address: {s3_address})...\"\
        )\n    minio_client.fput_object(\n        bucket_name=export_bucket,  # bucket\
        \ name in Minio\n        object_name=path,  # file name in bucket of Minio\
        \ \n        file_path=ARCHIVE_FILE,  # file path / name in local system\n\
        \    )\n\n    logger.info(\"Finished.\")\n    out_tuple = namedtuple(\"UploadOutput\"\
        , [\"s3_address\"])\n    return out_tuple(s3_address)\n\ndef _serialize_str(str_value:\
        \ str) -> str:\n    if not isinstance(str_value, str):\n        raise TypeError('Value\
        \ \"{}\" has type \"{}\" instead of str.'.format(\n            str(str_value),\
        \ str(type(str_value))))\n    return str_value\n\nimport argparse\n_parser\
        \ = argparse.ArgumentParser(prog='Upload artifacts', description='Uploads\
        \ a model file to MinIO artifact store.')\n_parser.add_argument(\"--onnx-model\"\
        , dest=\"onnx_model\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--preprocessor\", dest=\"preprocessor\", type=str,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--explainer\"\
        , dest=\"explainer\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--archive-name\", dest=\"archive_name\", type=str,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--minio-url\"\
        , dest=\"minio_url\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--version\", dest=\"version\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\", dest=\"\
        _output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
        _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = upload_artifacts(**_parsed_args)\n\
        \n_output_serializers = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx,\
        \ output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
        \    except OSError:\n        pass\n    with open(output_file, 'w') as f:\n\
        \        f.write(_output_serializers[idx](_outputs[idx]))\n"
      env:
      - name: MINIO_ID
        valueFrom:
          secretKeyRef: {key: accesskey, name: mlpipeline-minio-artifact}
      - name: MINIO_PWD
        valueFrom:
          secretKeyRef: {key: secretkey, name: mlpipeline-minio-artifact}
      - name: COLUMNS
        valueFrom:
          configMapKeyRef: {key: columns, name: credit-risk-columns}
      image: quay.io/ntlawrence/demo-workflow:3.0.0
    inputs:
      artifacts:
      - {name: train-explainer-explainer_dll, path: /tmp/inputs/explainer/data}
      - {name: convert-model-to-onnx-onnx_model, path: /tmp/inputs/onnx_model/data}
      - {name: fit-preprocessor-preprocessor_pkl, path: /tmp/inputs/preprocessor/data}
    outputs:
      parameters:
      - name: upload-artifacts-s3_address
        valueFrom: {path: /tmp/outputs/s3_address/data}
      artifacts:
      - {name: upload-artifacts-s3_address, path: /tmp/outputs/s3_address/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Uploads
          a model file to MinIO artifact store.", "implementation": {"container":
          {"args": ["--onnx-model", {"inputPath": "onnx_model"}, "--preprocessor",
          {"inputPath": "preprocessor"}, "--explainer", {"inputPath": "explainer"},
          "--archive-name", {"inputValue": "archive_name"}, {"if": {"cond": {"isPresent":
          "minio_url"}, "then": ["--minio-url", {"inputValue": "minio_url"}]}}, {"if":
          {"cond": {"isPresent": "version"}, "then": ["--version", {"inputValue":
          "version"}]}}, "----output-paths", {"outputPath": "s3_address"}], "command":
          ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location ''minio==7.1.13'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''minio==7.1.13''
          --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def upload_artifacts(\n    onnx_model,\n    preprocessor,\n    explainer,\n    archive_name,\n    minio_url
          = \"minio-service.kubeflow:9000\",\n    version = \"1\"\n):\n    \"\"\"Uploads
          a model file to MinIO artifact store.\"\"\"\n\n    from collections import
          namedtuple\n    import logging\n    from minio import Minio\n    import
          sys\n    import tarfile\n    import os\n\n    logging.basicConfig(\n        stream=sys.stdout,\n        level=logging.INFO,\n        format=\"%(levelname)s
          %(asctime)s: %(message)s\",\n    )\n    logger = logging.getLogger()\n\n    ARCHIVE_FILE
          = f\"/tmp/{archive_name}\"\n    with tarfile.open(ARCHIVE_FILE, \"w\") as
          f:\n        f.add(onnx_model, arcname=\"model.onnx\")\n        f.add(preprocessor,
          arcname=\"preprocessor.joblib\")\n        f.add(explainer, arcname=\"explainer.dll\")\n\n    minio_client
          = Minio(\n            minio_url, \n            access_key=os.environ[\"MINIO_ID\"],
          \n            secret_key=os.environ[\"MINIO_PWD\"], secure=False\n        )\n\n    #
          Create export bucket if it does not yet exist\n    export_bucket=\"{{workflow.namespace}}\"\n    existing_bucket
          = next(filter(lambda bucket: bucket.name == export_bucket, minio_client.list_buckets()),
          None)\n\n    if not existing_bucket:\n        logger.info(f\"Creating bucket
          ''{export_bucket}''...\")\n        minio_client.make_bucket(bucket_name=export_bucket)\n\n    path
          = f\"tar/{version}/{archive_name}\"\n    s3_address = f\"s3://{export_bucket}/tar\"\n\n    logger.info(f\"Saving
          onnx file to MinIO (s3 address: {s3_address})...\")\n    minio_client.fput_object(\n        bucket_name=export_bucket,  #
          bucket name in Minio\n        object_name=path,  # file name in bucket of
          Minio \n        file_path=ARCHIVE_FILE,  # file path / name in local system\n    )\n\n    logger.info(\"Finished.\")\n    out_tuple
          = namedtuple(\"UploadOutput\", [\"s3_address\"])\n    return out_tuple(s3_address)\n\ndef
          _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,
          str):\n        raise TypeError(''Value \"{}\" has type \"{}\" instead of
          str.''.format(\n            str(str_value), str(type(str_value))))\n    return
          str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Upload
          artifacts'', description=''Uploads a model file to MinIO artifact store.'')\n_parser.add_argument(\"--onnx-model\",
          dest=\"onnx_model\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--preprocessor\",
          dest=\"preprocessor\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--explainer\",
          dest=\"explainer\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--archive-name\",
          dest=\"archive_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--minio-url\",
          dest=\"minio_url\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--version\",
          dest=\"version\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = upload_artifacts(**_parsed_args)\n\n_output_serializers
          = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "quay.io/ntlawrence/demo-workflow:3.0.0"}}, "inputs": [{"name":
          "onnx_model", "type": "String"}, {"name": "preprocessor", "type": "String"},
          {"name": "explainer", "type": "String"}, {"name": "archive_name", "type":
          "String"}, {"default": "minio-service.kubeflow:9000", "name": "minio_url",
          "optional": true, "type": "String"}, {"default": "1", "name": "version",
          "optional": true, "type": "String"}], "name": "Upload artifacts", "outputs":
          [{"name": "s3_address", "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"archive_name": "credit-risk.tar",
          "minio_url": "minio-service.kubeflow:9000", "version": "1"}'}
  arguments:
    parameters: []
  serviceAccountName: pipeline-runner
