{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dca6eebb-74b5-4d39-b3dd-e47656bbd2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp.components import InputPath, OutputPath\n",
    "from kfp import dsl\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "69a64e6a-4b89-4a45-a950-9ef464ff4259",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_IMAGE = \"quay.io/ntlawrence/explain-demo:0.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbbef71-a79f-4472-b9d6-3969b2c3bb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df_from_db2(table_name: str,\n",
    "                     data_frame_pkl: OutputPath):\n",
    "    import warnings\n",
    "    import ibm_db\n",
    "    import ibm_db_dbi\n",
    "    import os\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    from typing import Dict, Any\n",
    "    \n",
    "    def assign_categories_to_df(df: pd.DataFrame, column_info: Dict[str, any]) -> None:\n",
    "        for col_name, levels in column_info[\"label_columns\"].items():\n",
    "            if col_name in df.columns:\n",
    "                ctype = pd.CategoricalDtype(categories=levels, ordered=False)\n",
    "                df[col_name] = df[col_name].astype(ctype)\n",
    "\n",
    "    def df_from_sql(\n",
    "        name: str,\n",
    "        conn: ibm_db.IBM_DBConnection,\n",
    "        column_info: Dict[str, Any],\n",
    "    ) -> pd.DataFrame:\n",
    "        sql_safe_name = name.replace('\"', \"\")\n",
    "\n",
    "        rStmtColsSql = \",\".join([f'\"{col}\"' for col in column_info[\"columns\"]])\n",
    "        rSql = f'SELECT {rStmtColsSql} FROM \"{sql_safe_name}\"'\n",
    "\n",
    "        read_conn = ibm_db_dbi.Connection(conn)\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", message=\"pandas only support SQLAlchemy\")\n",
    "            df = pd.read_sql(rSql, read_conn)\n",
    "\n",
    "        assign_categories_to_df(df, column_info)\n",
    "        return df\n",
    "    \n",
    "    conn_str = (\n",
    "    \"DRIVER={IBM DB2 ODBC DRIVER};\"\n",
    "    f\"DATABASE=BLUDB;HOSTNAME={os.environ['db2_host']};PORT={os.environ['db2_port']};PROTOCOL=TCPIP;UID={os.environ['db2_user']};Pwd={os.environ['db2_pwd']};SECURITY=SSL;\"\n",
    "    )\n",
    "        \n",
    "    conn = ibm_db.connect(conn_str, \"\", \"\")\n",
    "\n",
    "    column_info = json.loads(os.environ[\"COLUMNS\"])\n",
    "    df = df_from_sql(table_name, conn, column_info)\n",
    "    df.to_pickle(data_frame_pkl)\n",
    "    \n",
    "    \n",
    "load_df_from_db2_comp = kfp.components.create_component_from_func(\n",
    "    func=load_df_from_db2, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4bd8a481-de19-4837-94cf-bcf805617245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_preprocessor(\n",
    "    training_df: InputPath,\n",
    "    preprocessor_pkl: OutputPath,\n",
    "    features: List[str],\n",
    "):\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import joblib\n",
    "    import os\n",
    "\n",
    "    from sklearn.preprocessing import ColumnTransformer\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    from sklearn.pipeline import Pipeline\n",
    "\n",
    "    feature_set = set(features)\n",
    "    column_info = json.loads(os.environ[\"COLUMNS\"])\n",
    "\n",
    "    ohe_labels = [\n",
    "        (\n",
    "            \"ohe_\" + label,\n",
    "            OneHotEncoder(\n",
    "                handle_unknown=\"ignore\", sparse_output=False, categories=levels\n",
    "            ),\n",
    "            label,\n",
    "        )\n",
    "        for label, levels in column_info[\"label_columns\"].items()\n",
    "        if label in feature_set\n",
    "    ]\n",
    "\n",
    "    int_cols = [\n",
    "        (\n",
    "            \"passthrough\",\n",
    "            \"passthrough\",\n",
    "            [col for col in column_info[\"int_columns\"] if col in feature_set],\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"Preprocess\", ColumnTransformer(ohe_labels + int_cols, remainder=\"drop\")),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train = pd.read_pickle(training_df)\n",
    "    pipe.fit(train)\n",
    "    joblib.dump(pipe, preprocessor_pkl)\n",
    "\n",
    "\n",
    "fit_preprocessor_comp = kfp.components.create_component_from_func(\n",
    "    func=fit_preprocessor, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2af502ad-b08b-40e6-a8d2-e75fd3374826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    training_df: InputPath,\n",
    "    preprocessor: InputPath,\n",
    "    model: OutputPath,\n",
    "    target_processing_config: OutputPath,\n",
    "    target_col: str = \"Risk\"\n",
    "):\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import joblib\n",
    "    import tensorflow as tf\n",
    "    from keras import Sequential\n",
    "    from keras.layers import Dense, Dropout, BatchNormalization, Input\n",
    "    from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    import numpy as np\n",
    "\n",
    "    target_processing_config_dict = {\n",
    "        \"threshold\" : 0.5,\n",
    "        \"target_names\" : {0: \"No Risk\", 1: \"Risk\"}\n",
    "    }\n",
    "\n",
    "    def get_tf_model(num_features: int) -> Tuple[tf.keras.Model, List[tf.keras.callbacks.Callback]]:\n",
    "\n",
    "        tf_model = Sequential(\n",
    "            [\n",
    "                Input(shape=(num_features,)),\n",
    "                BatchNormalization(),\n",
    "                Dense(30, activation=\"relu\", name=\"layer1\"),\n",
    "                Dropout(0.3, name=\"dropout1\"),\n",
    "                Dense(30, activation=\"relu\", name=\"layer2\"),\n",
    "                Dropout(0.3, name=\"dropout2\"),\n",
    "                Dense(30, activation=\"relu\", name=\"layer3\"),\n",
    "                Dropout(0.3, name=\"dropout3\"),\n",
    "                Dense(\n",
    "                    1,\n",
    "                    activation=\"sigmoid\",\n",
    "                    name=\"output\",\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        tf_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "\n",
    "        callbacks = [\n",
    "            EarlyStopping(\n",
    "                monitor=\"val_loss\",\n",
    "                patience=50,\n",
    "                verbose=0,\n",
    "                mode=\"min\",\n",
    "                restore_best_weights=True,\n",
    "            ),\n",
    "            ReduceLROnPlateau(\n",
    "                monitor=\"val_loss\",\n",
    "                factor=0.1,\n",
    "                patience=7,\n",
    "                verbose=1,\n",
    "                min_delta=0.0001,\n",
    "                mode=\"min\",\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        return tf_model, callbacks\n",
    "\n",
    "    train = pd.read_pickle(training_df)\n",
    "    preprocessor = joblib.load(preprocessor)\n",
    "\n",
    "    X = tf.convert_to_tensor(preprocessor.transform(train))\n",
    "    y = tf.convert_to_tensor(\n",
    "        train.loc[:, target_col].apply(lambda v: 1 if v == target_processing_config_dict[\"target_names\"][1] else 0)\n",
    "    )                                                        \n",
    "    \n",
    "    tf_model, callbacks = get_tf_model(num_features=X.shape[1])\n",
    "    tf_model.fit(\n",
    "        X,\n",
    "        y,\n",
    "        validation_split=0.2,\n",
    "        epochs=500,\n",
    "        callbacks=callbacks,\n",
    "        class_weight={0: 1, 1: 2},\n",
    "    )\n",
    "\n",
    "    # calculate best threshold of highest f1 score\n",
    "    predictions = model.predict(X)\n",
    "    precision, recall, thresholds = precision_recall_curve(\n",
    "        y_true=y.numpy(), probas_pred=predictions.flatten()\n",
    "    )\n",
    "    f1s = 2 * (precision * recall) / (precision + recall)\n",
    "    target_processing_config_dict[\"threshold\"] = thresholds[np.argmax(f1s)]\n",
    "    \n",
    "    # Save model and threshold config\n",
    "    tf_model.save(model)\n",
    "    with open(target_processing_config, \"w\") as f:\n",
    "        json.dump(target_processing_config_dict, f)\n",
    "\n",
    "\n",
    "train_comp = kfp.components.create_component_from_func(\n",
    "    func=train, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8937ccc-1de2-42cf-a4c2-5bed40cfb022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    df: InputPath,\n",
    "    preprocessor: InputPath,\n",
    "    model: InputPath,\n",
    "    target_processing_config: InputPath,\n",
    "    output_report: OutputPath(str),\n",
    "    mlpipeline_ui_metadata_path: OutputPath(),\n",
    "    target=\"Risk\",\n",
    "):\n",
    "    import pandas as pd\n",
    "    import joblib\n",
    "    import tensorflow as tf\n",
    "    import json\n",
    "    from evidently.metric_preset import ClassificationPreset\n",
    "    from evidently.report import Report\n",
    "    from evidently import ColumnMapping\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "\n",
    "    dataset = pd.read_pickle(df)\n",
    "    preprocessor = joblib.load(preprocessor)\n",
    "    tf_model = tf.keras.models.load_model(model)\n",
    "    with open(target_processing_config, \"r\") as f:\n",
    "        target_processing_config_dict = json.load(f)\n",
    "\n",
    "    column_info = json.loads(os.environ[\"COLUMNS\"])\n",
    "\n",
    "    X = preprocessor.transform(dataset)\n",
    "    y_prob = tf_model.predict(X)\n",
    "\n",
    "    dataset[\"Prediction\"] = pd.Series(y_prob).apply(\n",
    "        lambda p: 1 if p > target_processing_config_dict[\"threshold\"] else 0\n",
    "    )\n",
    "    dataset[\"Actual\"] = train.loc[:, target].apply(\n",
    "        lambda v: 1 if v == target_processing_config_dict[\"target_names\"][1] else 0\n",
    "    )\n",
    "\n",
    "    column_mapping = ColumnMapping()\n",
    "    column_mapping.target_names = target_processing_config_dict[\"target_names\"]\n",
    "    column_mapping.target = \"Actual\"\n",
    "    column_mapping.prediction = \"Prediction\"\n",
    "    column_mapping.task = \"classification\"\n",
    "    column_mapping.numerical_features = [\n",
    "        c\n",
    "        for c in column_info[\"int_columns\"]\n",
    "        if c in set(preprocessor.feature_names_in_)\n",
    "    ]\n",
    "    column_mapping.categorical_features = [\n",
    "        c\n",
    "        for c in column_info[\"label_columns\"]\n",
    "        if c in set(preprocessor.feature_names_in_)\n",
    "    ]\n",
    "\n",
    "    report = Report(\n",
    "        metrics=[\n",
    "            ClassificationPreset(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    report.run(\n",
    "        reference_data=None,\n",
    "        current_data=dataset,\n",
    "        column_mapping=column_mapping,\n",
    "    )\n",
    "\n",
    "    Path(output_report).parent.mkdir(parents=True, exist_ok=True)\n",
    "    report.save_html(output_report)\n",
    "    html_content = open(output_report, \"r\").read()\n",
    "    metadata = {\n",
    "        \"outputs\": [\n",
    "            {\n",
    "                \"type\": \"web-app\",\n",
    "                \"storage\": \"inline\",\n",
    "                \"source\": html_content,\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    with open(mlpipeline_ui_metadata_path, \"w\") as f:\n",
    "        json.dump(metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25df93f6-5134-4178-bcef-3458ff25ca3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"Credit Risk\",\n",
    "    description=\"An example pipeline that builds and deploys a credit risk model\",\n",
    ")\n",
    "def credit_model_pipeline():\n",
    "    load_training_data_task = load_df_from_db2(\"TRAIN\")\n",
    "    load_test_data_task = load_df_from_db2(\"TEST\")\n",
    "\n",
    "    fit_preprocessor_task = fit_preprocessor(\n",
    "        load_training_data_task.outputs[\"data_frame_pkl\"],\n",
    "        features=[\n",
    "            \"CheckingStatus\",\n",
    "            \"LoanDuration\",\n",
    "            \"CreditHistory\",\n",
    "            \"LoanPurpose\",\n",
    "            \"LoanAmount\",\n",
    "            \"ExistingSavings\",\n",
    "            \"EmploymentDuration\",\n",
    "            \"InstallmentPercent\",\n",
    "            \"Sex\",\n",
    "            \"OthersOnLoan\",\n",
    "            \"CurrentResidenceDuration\",\n",
    "            \"OwnsProperty\",\n",
    "            \"Age\",\n",
    "            \"InstallmentPlans\",\n",
    "            \"Housing\",\n",
    "            \"ExistingCreditsCount\",\n",
    "            \"Job\",\n",
    "            \"Dependents\",\n",
    "            \"Telephone\",\n",
    "            \"ForeignWorker\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    train_model_task = train(\n",
    "        training_df=load_training_data_task.outputs[\"data_frame_pkl\"],\n",
    "        preprocessor=fit_preprocessor_task.outputs[\"preprocessor_pkl\"],\n",
    "    )\n",
    "\n",
    "    evaluate_model_task = evaluate(\n",
    "        load_test_data_task.outputs[\"data_frame_pkl\"],\n",
    "        preprocessor=fit_preprocessor_task.outputs[\"preprocessor_pkl\"],\n",
    "        model=train_model_task.outputs[\"model\"],\n",
    "        target_processing_config=train_model_task.outputs[\"target_processing_config\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76074abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pipeline(pipeline_name: str):\n",
    "    \"\"\"Delete's a pipeline with the specified name\"\"\"\n",
    "\n",
    "    client = kfp.Client()\n",
    "    existing_pipelines = client.list_pipelines(page_size=999).pipelines\n",
    "    matches = (\n",
    "        [ep.id for ep in existing_pipelines if ep.name == pipeline_name]\n",
    "        if existing_pipelines\n",
    "        else []\n",
    "    )\n",
    "    for id in matches:\n",
    "        client.delete_pipeline(id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c6a023f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_id(experiment_name: str) -> str:\n",
    "    \"\"\"Returns the id for the experiment, creating the experiment if needed\"\"\"\n",
    "    client = kfp.Client()\n",
    "    existing_experiments = client.list_experiments(page_size=999).experiments\n",
    "    matches = (\n",
    "        [ex.id for ex in existing_experiments if ex.name == experiment_name]\n",
    "        if existing_experiments\n",
    "        else []\n",
    "    )\n",
    "\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "\n",
    "    exp = client.create_experiment(experiment_name)\n",
    "    return exp.id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a2f50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_NAME = \"Build_Credit_Risk_Model\"\n",
    "# Pipeline names need to be unique, so before we upload,\n",
    "# check for and delete any pipeline with the same name\n",
    "delete_pipeline(PIPELINE_NAME)\n",
    "\n",
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=credit_model_pipeline,\n",
    "    package_path=f\"{PIPELINE_NAME}.yaml\",\n",
    ")\n",
    "\n",
    "# upload\n",
    "client = kfp.Client()\n",
    "uploaded_pipeline = client.upload_pipeline(f\"{PIPELINE_NAME}.yaml\", PIPELINE_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3139f4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = client.run_pipeline(\n",
    "    experiment_id=get_experiment_id(\"monkey-classification-exp\"),\n",
    "    job_name=\"monkey-classification-pipeline\",\n",
    "    pipeline_id=uploaded_pipeline.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dce21c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TWENTY_MIN = 20 * 60\n",
    "result = client.wait_for_run_completion(run.id, timeout=TWENTY_MIN)\n",
    "{\n",
    "    \"status\": result.run.status,\n",
    "    \"error\": result.run.error,\n",
    "    \"time\": str(result.run.finished_at - result.run.created_at),\n",
    "    \"metrics\": result.run.metrics,\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
