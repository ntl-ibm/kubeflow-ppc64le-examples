apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: monitor-credit-risk-ai-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.18, pipelines.kubeflow.org/pipeline_compilation_time: '2023-12-05T19:31:27.146760',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "An example pipeline that
      monitors the behavior of the AI model within the application", "inputs": [{"default":
      "10080", "name": "time_window_minutes", "optional": true, "type": "Integer"},
      {"default": "5", "name": "min_rows", "optional": true, "type": "Integer"}],
      "name": "Monitor Credit Risk AI"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.18}
spec:
  entrypoint: monitor-credit-risk-ai
  templates:
  - name: check-metrics
    container:
      args: [--classification-report, /tmp/inputs/classification_report/data, --data-drift-report,
        /tmp/inputs/data_drift_report/data, --target-drift-report, /tmp/inputs/target_drift_report/data,
        '----output-paths', /tmp/outputs/mlpipeline_metrics/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def check_metrics(classification_report,
                          data_drift_report,
                          target_drift_report):
            import json
            from collections import namedtuple

            with open(classification_report) as class_f:
                 classification = json.load(class_f)

            ClassificationQualityMetric = next(filter(lambda m: m["metric"] == "ClassificationQualityMetric", classification["metrics"]))

            metrics = {
                "metrics": [
                    {"name": "f1", "numberValue": ClassificationQualityMetric["result"]["current"]["f1"], "format": "RAW"}
                ]
            }

            out_tuple = namedtuple("EvaluationOutput", ["mlpipeline_metrics"])
            return out_tuple(json.dumps(metrics))

        import argparse
        _parser = argparse.ArgumentParser(prog='Check metrics', description='')
        _parser.add_argument("--classification-report", dest="classification_report", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--data-drift-report", dest="data_drift_report", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--target-drift-report", dest="target_drift_report", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = check_metrics(**_parsed_args)

        _output_serializers = [
            str,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      env:
      - name: COLUMNS
        valueFrom:
          configMapKeyRef: {key: columns, name: credit-risk-columns}
      image: quay.io/ntlawrence/demo-workflow:3.0.0
    inputs:
      artifacts:
      - {name: evidently-classification-report-output_json, path: /tmp/inputs/classification_report/data}
      - {name: evidently-report-output_json, path: /tmp/inputs/data_drift_report/data}
      - {name: evidently-report-2-output_json, path: /tmp/inputs/target_drift_report/data}
    outputs:
      artifacts:
      - {name: mlpipeline-metrics, path: /tmp/outputs/mlpipeline_metrics/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--classification-report", {"inputPath": "classification_report"},
          "--data-drift-report", {"inputPath": "data_drift_report"}, "--target-drift-report",
          {"inputPath": "target_drift_report"}, "----output-paths", {"outputPath":
          "mlpipeline_metrics"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def check_metrics(classification_report,\n                  data_drift_report,\n                  target_drift_report):\n    import
          json\n    from collections import namedtuple\n\n    with open(classification_report)
          as class_f:\n         classification = json.load(class_f)\n\n    ClassificationQualityMetric
          = next(filter(lambda m: m[\"metric\"] == \"ClassificationQualityMetric\",
          classification[\"metrics\"]))\n\n    metrics = {\n        \"metrics\": [\n            {\"name\":
          \"f1\", \"numberValue\": ClassificationQualityMetric[\"result\"][\"current\"][\"f1\"],
          \"format\": \"RAW\"}\n        ]\n    }\n\n    out_tuple = namedtuple(\"EvaluationOutput\",
          [\"mlpipeline_metrics\"])\n    return out_tuple(json.dumps(metrics))\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Check metrics'', description='''')\n_parser.add_argument(\"--classification-report\",
          dest=\"classification_report\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--data-drift-report\",
          dest=\"data_drift_report\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--target-drift-report\",
          dest=\"target_drift_report\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = check_metrics(**_parsed_args)\n\n_output_serializers
          = [\n    str,\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "quay.io/ntlawrence/demo-workflow:3.0.0"}}, "inputs": [{"name":
          "classification_report", "type": "String"}, {"name": "data_drift_report",
          "type": "String"}, {"name": "target_drift_report", "type": "String"}], "name":
          "Check metrics", "outputs": [{"name": "mlpipeline_metrics", "type": "Metrics"}]}',
        pipelines.kubeflow.org/component_ref: '{}'}
  - name: condition-1
    inputs:
      artifacts:
      - {name: load-df-from-postgresql-2-data_frame_pkl}
      - {name: load-df-from-postgresql-data_frame_pkl}
    dag:
      tasks:
      - name: check-metrics
        template: check-metrics
        dependencies: [evidently-classification-report, evidently-report, evidently-report-2]
        arguments:
          artifacts:
          - {name: evidently-classification-report-output_json, from: '{{tasks.evidently-classification-report.outputs.artifacts.evidently-classification-report-output_json}}'}
          - {name: evidently-report-2-output_json, from: '{{tasks.evidently-report-2.outputs.artifacts.evidently-report-2-output_json}}'}
          - {name: evidently-report-output_json, from: '{{tasks.evidently-report.outputs.artifacts.evidently-report-output_json}}'}
      - name: evidently-classification-report
        template: evidently-classification-report
        arguments:
          artifacts:
          - {name: load-df-from-postgresql-2-data_frame_pkl, from: '{{inputs.artifacts.load-df-from-postgresql-2-data_frame_pkl}}'}
      - name: evidently-report
        template: evidently-report
        arguments:
          artifacts:
          - {name: load-df-from-postgresql-2-data_frame_pkl, from: '{{inputs.artifacts.load-df-from-postgresql-2-data_frame_pkl}}'}
          - {name: load-df-from-postgresql-data_frame_pkl, from: '{{inputs.artifacts.load-df-from-postgresql-data_frame_pkl}}'}
      - name: evidently-report-2
        template: evidently-report-2
        arguments:
          artifacts:
          - {name: load-df-from-postgresql-2-data_frame_pkl, from: '{{inputs.artifacts.load-df-from-postgresql-2-data_frame_pkl}}'}
          - {name: load-df-from-postgresql-data_frame_pkl, from: '{{inputs.artifacts.load-df-from-postgresql-data_frame_pkl}}'}
  - name: evidently-classification-report
    container:
      args: [--production-df, /tmp/inputs/production_df/data, --target, Risk, --predictions,
        PredictedRisk, --pos-class, Risk, --mlpipeline-ui-metadata, /tmp/outputs/mlpipeline_ui_metadata/data,
        --output-report, /tmp/outputs/output_report/data, --output-json, /tmp/outputs/output_json/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def evidently_classification_report(production_df,
                              mlpipeline_ui_metadata_path,
                              output_report,
                              output_json,
                              target = 'Risk',
                              predictions = 'PredictedRisk',
                              pos_class = 'Risk'
                             ):
            from evidently.metric_preset import (
            ClassificationPreset,
            )
            from evidently.report import Report
            from evidently import ColumnMapping
            import pandas as pd
            import os
            from pathlib import Path
            import json

            production_dataset = pd.read_pickle(production_df)

            production_dataset.dropna(subset=[target, predictions], inplace=True)
            column_info = json.loads(os.environ["COLUMNS"])

            column_mapping = ColumnMapping()
            #column_mapping.target_names = ['No Risk', 'Risk']
            column_mapping.target = 'Risk' #'Actual_Int'
            column_mapping.prediction = 'PredictedRisk' #'Predicted_Int'
            column_mapping.pos_label = 'Risk'
            column_mapping.task = "classification"

            column_mapping.numerical_features = [
                c
                for c in column_info["int_columns"]
                if c != target
            ]
            column_mapping.categorical_features = [
                c
                for c in column_info["label_columns"]
                if c != target
            ]

            report = Report(
                metrics=[
                    ClassificationPreset()
                ]
            )

            #production_dataset['Actual_Int'] = production_dataset[target].apply(lambda v: 1 if v == pos_class else 0)
            #production_dataset['Predicted_Int'] = production_dataset[predictions].apply(lambda v: 1 if v == pos_class else 0)

            report.run(
                reference_data=None,
                current_data=production_dataset,
                column_mapping=column_mapping,
            )

            Path(output_report).parent.mkdir(parents=True, exist_ok=True)
            report.save_html(output_report)
            html_content = open(output_report, "r").read()
            metadata = {
                "outputs": [
                    {
                        "type": "web-app",
                        "storage": "inline",
                        "source": html_content,
                    }
                ]
            }

            with open(mlpipeline_ui_metadata_path, "w") as f:
                json.dump(metadata, f)

            with open(output_json, "w") as json_f:
                json_f.write(report.json())

        import argparse
        _parser = argparse.ArgumentParser(prog='Evidently classification report', description='')
        _parser.add_argument("--production-df", dest="production_df", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--target", dest="target", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--predictions", dest="predictions", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--pos-class", dest="pos_class", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--mlpipeline-ui-metadata", dest="mlpipeline_ui_metadata_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--output-report", dest="output_report", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--output-json", dest="output_json", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = evidently_classification_report(**_parsed_args)
      env:
      - name: COLUMNS
        valueFrom:
          configMapKeyRef: {key: columns, name: credit-risk-columns}
      image: quay.io/ntlawrence/demo-workflow:3.0.0
    inputs:
      artifacts:
      - {name: load-df-from-postgresql-2-data_frame_pkl, path: /tmp/inputs/production_df/data}
    outputs:
      artifacts:
      - {name: mlpipeline-ui-metadata, path: /tmp/outputs/mlpipeline_ui_metadata/data}
      - {name: evidently-classification-report-output_json, path: /tmp/outputs/output_json/data}
      - {name: evidently-classification-report-output_report, path: /tmp/outputs/output_report/data}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: Produce classification
          Report, pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--production-df", {"inputPath": "production_df"}, {"if": {"cond":
          {"isPresent": "target"}, "then": ["--target", {"inputValue": "target"}]}},
          {"if": {"cond": {"isPresent": "predictions"}, "then": ["--predictions",
          {"inputValue": "predictions"}]}}, {"if": {"cond": {"isPresent": "pos_class"},
          "then": ["--pos-class", {"inputValue": "pos_class"}]}}, "--mlpipeline-ui-metadata",
          {"outputPath": "mlpipeline_ui_metadata"}, "--output-report", {"outputPath":
          "output_report"}, "--output-json", {"outputPath": "output_json"}], "command":
          ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef evidently_classification_report(production_df,\n                      mlpipeline_ui_metadata_path,\n                      output_report,\n                      output_json,\n                      target
          = ''Risk'',\n                      predictions = ''PredictedRisk'',\n                      pos_class
          = ''Risk''\n                     ):\n    from evidently.metric_preset import
          (\n    ClassificationPreset,\n    )\n    from evidently.report import Report\n    from
          evidently import ColumnMapping\n    import pandas as pd\n    import os\n    from
          pathlib import Path\n    import json\n\n    production_dataset = pd.read_pickle(production_df)\n\n    production_dataset.dropna(subset=[target,
          predictions], inplace=True)\n    column_info = json.loads(os.environ[\"COLUMNS\"])\n\n    column_mapping
          = ColumnMapping()\n    #column_mapping.target_names = [''No Risk'', ''Risk'']\n    column_mapping.target
          = ''Risk'' #''Actual_Int''\n    column_mapping.prediction = ''PredictedRisk''
          #''Predicted_Int''\n    column_mapping.pos_label = ''Risk''\n    column_mapping.task
          = \"classification\"\n\n    column_mapping.numerical_features = [\n        c\n        for
          c in column_info[\"int_columns\"]\n        if c != target\n    ]\n    column_mapping.categorical_features
          = [\n        c\n        for c in column_info[\"label_columns\"]\n        if
          c != target\n    ]\n\n    report = Report(\n        metrics=[\n            ClassificationPreset()\n        ]\n    )\n\n    #production_dataset[''Actual_Int'']
          = production_dataset[target].apply(lambda v: 1 if v == pos_class else 0)\n    #production_dataset[''Predicted_Int'']
          = production_dataset[predictions].apply(lambda v: 1 if v == pos_class else
          0)\n\n    report.run(\n        reference_data=None,\n        current_data=production_dataset,\n        column_mapping=column_mapping,\n    )\n\n    Path(output_report).parent.mkdir(parents=True,
          exist_ok=True)\n    report.save_html(output_report)\n    html_content =
          open(output_report, \"r\").read()\n    metadata = {\n        \"outputs\":
          [\n            {\n                \"type\": \"web-app\",\n                \"storage\":
          \"inline\",\n                \"source\": html_content,\n            }\n        ]\n    }\n\n    with
          open(mlpipeline_ui_metadata_path, \"w\") as f:\n        json.dump(metadata,
          f)\n\n    with open(output_json, \"w\") as json_f:\n        json_f.write(report.json())\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Evidently classification
          report'', description='''')\n_parser.add_argument(\"--production-df\", dest=\"production_df\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--target\",
          dest=\"target\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictions\",
          dest=\"predictions\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--pos-class\",
          dest=\"pos_class\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlpipeline-ui-metadata\",
          dest=\"mlpipeline_ui_metadata_path\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-report\",
          dest=\"output_report\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-json\", dest=\"output_json\",
          type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = evidently_classification_report(**_parsed_args)\n"],
          "image": "quay.io/ntlawrence/demo-workflow:3.0.0"}}, "inputs": [{"name":
          "production_df", "type": "String"}, {"default": "Risk", "name": "target",
          "optional": true, "type": "String"}, {"default": "PredictedRisk", "name":
          "predictions", "optional": true, "type": "String"}, {"default": "Risk",
          "name": "pos_class", "optional": true, "type": "String"}], "name": "Evidently
          classification report", "outputs": [{"name": "mlpipeline_ui_metadata", "type":
          "String"}, {"name": "output_report", "type": "String"}, {"name": "output_json",
          "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"pos_class":
          "Risk", "predictions": "PredictedRisk", "target": "Risk"}'}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: evidently-report
    container:
      args: [--reference-df, /tmp/inputs/reference_df/data, --production-df, /tmp/inputs/production_df/data,
        --target, Risk, --report-type, drift, --mlpipeline-ui-metadata, /tmp/outputs/mlpipeline_ui_metadata/data,
        --output-report, /tmp/outputs/output_report/data, --output-json, /tmp/outputs/output_json/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def evidently_report(reference_df,
                              production_df,
                              mlpipeline_ui_metadata_path,
                              output_report,
                              output_json,
                              target = 'Risk',
                              report_type = 'drift'
                             ):
            from evidently.metric_preset import (
            DataDriftPreset,
            TargetDriftPreset,
            ClassificationPreset,
            )
            from evidently.report import Report
            from evidently import ColumnMapping
            import pandas as pd
            import os
            from pathlib import Path
            import json

            reference_dataset = pd.read_pickle(reference_df)
            production_dataset = pd.read_pickle(production_df)
            column_info = json.loads(os.environ["COLUMNS"])

            column_mapping = ColumnMapping()
            column_mapping.target = target
            column_mapping.task = "classification"

            column_mapping.numerical_features = [
                c
                for c in column_info["int_columns"]
                if c != target
            ]
            column_mapping.categorical_features = [
                c
                for c in column_info["label_columns"]
                if c != target
            ]

            if report_type.lower() == "drift":
                report = Report(
                    metrics=[
                        DataDriftPreset(),
                    ]
                )
            elif report_type.lower() == "target_drift":
                report = Report(
                    metrics=[
                        TargetDriftPreset(),
                    ]
                )
            else:
                raise NotImplementedError()

            report.run(
                reference_data=reference_dataset,
                current_data=production_dataset,
                column_mapping=column_mapping,
            )

            Path(output_report).parent.mkdir(parents=True, exist_ok=True)
            report.save_html(output_report)
            html_content = open(output_report, "r").read()
            metadata = {
                "outputs": [
                    {
                        "type": "web-app",
                        "storage": "inline",
                        "source": html_content,
                    }
                ]
            }

            with open(mlpipeline_ui_metadata_path, "w") as f:
                json.dump(metadata, f)

            with open(output_json, "w") as json_f:
                json_f.write(report.json())

        import argparse
        _parser = argparse.ArgumentParser(prog='Evidently report', description='')
        _parser.add_argument("--reference-df", dest="reference_df", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--production-df", dest="production_df", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--target", dest="target", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--report-type", dest="report_type", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--mlpipeline-ui-metadata", dest="mlpipeline_ui_metadata_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--output-report", dest="output_report", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--output-json", dest="output_json", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = evidently_report(**_parsed_args)
      env:
      - name: COLUMNS
        valueFrom:
          configMapKeyRef: {key: columns, name: credit-risk-columns}
      image: quay.io/ntlawrence/demo-workflow:3.0.0
    inputs:
      artifacts:
      - {name: load-df-from-postgresql-2-data_frame_pkl, path: /tmp/inputs/production_df/data}
      - {name: load-df-from-postgresql-data_frame_pkl, path: /tmp/inputs/reference_df/data}
    outputs:
      artifacts:
      - {name: mlpipeline-ui-metadata, path: /tmp/outputs/mlpipeline_ui_metadata/data}
      - {name: evidently-report-output_json, path: /tmp/outputs/output_json/data}
      - {name: evidently-report-output_report, path: /tmp/outputs/output_report/data}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: Produce Data Drift Report,
        pipelines.kubeflow.org/component_spec: '{"implementation": {"container": {"args":
          ["--reference-df", {"inputPath": "reference_df"}, "--production-df", {"inputPath":
          "production_df"}, {"if": {"cond": {"isPresent": "target"}, "then": ["--target",
          {"inputValue": "target"}]}}, {"if": {"cond": {"isPresent": "report_type"},
          "then": ["--report-type", {"inputValue": "report_type"}]}}, "--mlpipeline-ui-metadata",
          {"outputPath": "mlpipeline_ui_metadata"}, "--output-report", {"outputPath":
          "output_report"}, "--output-json", {"outputPath": "output_json"}], "command":
          ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef evidently_report(reference_df,\n                      production_df,\n                      mlpipeline_ui_metadata_path,\n                      output_report,\n                      output_json,\n                      target
          = ''Risk'',\n                      report_type = ''drift''\n                     ):\n    from
          evidently.metric_preset import (\n    DataDriftPreset,\n    TargetDriftPreset,\n    ClassificationPreset,\n    )\n    from
          evidently.report import Report\n    from evidently import ColumnMapping\n    import
          pandas as pd\n    import os\n    from pathlib import Path\n    import json\n\n    reference_dataset
          = pd.read_pickle(reference_df)\n    production_dataset = pd.read_pickle(production_df)\n    column_info
          = json.loads(os.environ[\"COLUMNS\"])\n\n    column_mapping = ColumnMapping()\n    column_mapping.target
          = target\n    column_mapping.task = \"classification\"\n\n    column_mapping.numerical_features
          = [\n        c\n        for c in column_info[\"int_columns\"]\n        if
          c != target\n    ]\n    column_mapping.categorical_features = [\n        c\n        for
          c in column_info[\"label_columns\"]\n        if c != target\n    ]\n\n    if
          report_type.lower() == \"drift\":\n        report = Report(\n            metrics=[\n                DataDriftPreset(),\n            ]\n        )\n    elif
          report_type.lower() == \"target_drift\":\n        report = Report(\n            metrics=[\n                TargetDriftPreset(),\n            ]\n        )\n    else:\n        raise
          NotImplementedError()\n\n    report.run(\n        reference_data=reference_dataset,\n        current_data=production_dataset,\n        column_mapping=column_mapping,\n    )\n\n    Path(output_report).parent.mkdir(parents=True,
          exist_ok=True)\n    report.save_html(output_report)\n    html_content =
          open(output_report, \"r\").read()\n    metadata = {\n        \"outputs\":
          [\n            {\n                \"type\": \"web-app\",\n                \"storage\":
          \"inline\",\n                \"source\": html_content,\n            }\n        ]\n    }\n\n    with
          open(mlpipeline_ui_metadata_path, \"w\") as f:\n        json.dump(metadata,
          f)\n\n    with open(output_json, \"w\") as json_f:\n        json_f.write(report.json())\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Evidently report'', description='''')\n_parser.add_argument(\"--reference-df\",
          dest=\"reference_df\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--production-df\",
          dest=\"production_df\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--target\",
          dest=\"target\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--report-type\",
          dest=\"report_type\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlpipeline-ui-metadata\",
          dest=\"mlpipeline_ui_metadata_path\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-report\",
          dest=\"output_report\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-json\", dest=\"output_json\",
          type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = evidently_report(**_parsed_args)\n"],
          "image": "quay.io/ntlawrence/demo-workflow:3.0.0"}}, "inputs": [{"name":
          "reference_df", "type": "String"}, {"name": "production_df", "type": "String"},
          {"default": "Risk", "name": "target", "optional": true, "type": "String"},
          {"default": "drift", "name": "report_type", "optional": true, "type": "String"}],
          "name": "Evidently report", "outputs": [{"name": "mlpipeline_ui_metadata",
          "type": "String"}, {"name": "output_report", "type": "String"}, {"name":
          "output_json", "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"report_type": "drift", "target":
          "Risk"}'}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: evidently-report-2
    container:
      args: [--reference-df, /tmp/inputs/reference_df/data, --production-df, /tmp/inputs/production_df/data,
        --target, Risk, --report-type, target_drift, --mlpipeline-ui-metadata, /tmp/outputs/mlpipeline_ui_metadata/data,
        --output-report, /tmp/outputs/output_report/data, --output-json, /tmp/outputs/output_json/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def evidently_report(reference_df,
                              production_df,
                              mlpipeline_ui_metadata_path,
                              output_report,
                              output_json,
                              target = 'Risk',
                              report_type = 'drift'
                             ):
            from evidently.metric_preset import (
            DataDriftPreset,
            TargetDriftPreset,
            ClassificationPreset,
            )
            from evidently.report import Report
            from evidently import ColumnMapping
            import pandas as pd
            import os
            from pathlib import Path
            import json

            reference_dataset = pd.read_pickle(reference_df)
            production_dataset = pd.read_pickle(production_df)
            column_info = json.loads(os.environ["COLUMNS"])

            column_mapping = ColumnMapping()
            column_mapping.target = target
            column_mapping.task = "classification"

            column_mapping.numerical_features = [
                c
                for c in column_info["int_columns"]
                if c != target
            ]
            column_mapping.categorical_features = [
                c
                for c in column_info["label_columns"]
                if c != target
            ]

            if report_type.lower() == "drift":
                report = Report(
                    metrics=[
                        DataDriftPreset(),
                    ]
                )
            elif report_type.lower() == "target_drift":
                report = Report(
                    metrics=[
                        TargetDriftPreset(),
                    ]
                )
            else:
                raise NotImplementedError()

            report.run(
                reference_data=reference_dataset,
                current_data=production_dataset,
                column_mapping=column_mapping,
            )

            Path(output_report).parent.mkdir(parents=True, exist_ok=True)
            report.save_html(output_report)
            html_content = open(output_report, "r").read()
            metadata = {
                "outputs": [
                    {
                        "type": "web-app",
                        "storage": "inline",
                        "source": html_content,
                    }
                ]
            }

            with open(mlpipeline_ui_metadata_path, "w") as f:
                json.dump(metadata, f)

            with open(output_json, "w") as json_f:
                json_f.write(report.json())

        import argparse
        _parser = argparse.ArgumentParser(prog='Evidently report', description='')
        _parser.add_argument("--reference-df", dest="reference_df", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--production-df", dest="production_df", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--target", dest="target", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--report-type", dest="report_type", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--mlpipeline-ui-metadata", dest="mlpipeline_ui_metadata_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--output-report", dest="output_report", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--output-json", dest="output_json", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = evidently_report(**_parsed_args)
      env:
      - name: COLUMNS
        valueFrom:
          configMapKeyRef: {key: columns, name: credit-risk-columns}
      image: quay.io/ntlawrence/demo-workflow:3.0.0
    inputs:
      artifacts:
      - {name: load-df-from-postgresql-2-data_frame_pkl, path: /tmp/inputs/production_df/data}
      - {name: load-df-from-postgresql-data_frame_pkl, path: /tmp/inputs/reference_df/data}
    outputs:
      artifacts:
      - {name: mlpipeline-ui-metadata, path: /tmp/outputs/mlpipeline_ui_metadata/data}
      - {name: evidently-report-2-output_json, path: /tmp/outputs/output_json/data}
      - {name: evidently-report-2-output_report, path: /tmp/outputs/output_report/data}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: Produce Target Drift
          Report, pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--reference-df", {"inputPath": "reference_df"}, "--production-df",
          {"inputPath": "production_df"}, {"if": {"cond": {"isPresent": "target"},
          "then": ["--target", {"inputValue": "target"}]}}, {"if": {"cond": {"isPresent":
          "report_type"}, "then": ["--report-type", {"inputValue": "report_type"}]}},
          "--mlpipeline-ui-metadata", {"outputPath": "mlpipeline_ui_metadata"}, "--output-report",
          {"outputPath": "output_report"}, "--output-json", {"outputPath": "output_json"}],
          "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" >
          \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef evidently_report(reference_df,\n                      production_df,\n                      mlpipeline_ui_metadata_path,\n                      output_report,\n                      output_json,\n                      target
          = ''Risk'',\n                      report_type = ''drift''\n                     ):\n    from
          evidently.metric_preset import (\n    DataDriftPreset,\n    TargetDriftPreset,\n    ClassificationPreset,\n    )\n    from
          evidently.report import Report\n    from evidently import ColumnMapping\n    import
          pandas as pd\n    import os\n    from pathlib import Path\n    import json\n\n    reference_dataset
          = pd.read_pickle(reference_df)\n    production_dataset = pd.read_pickle(production_df)\n    column_info
          = json.loads(os.environ[\"COLUMNS\"])\n\n    column_mapping = ColumnMapping()\n    column_mapping.target
          = target\n    column_mapping.task = \"classification\"\n\n    column_mapping.numerical_features
          = [\n        c\n        for c in column_info[\"int_columns\"]\n        if
          c != target\n    ]\n    column_mapping.categorical_features = [\n        c\n        for
          c in column_info[\"label_columns\"]\n        if c != target\n    ]\n\n    if
          report_type.lower() == \"drift\":\n        report = Report(\n            metrics=[\n                DataDriftPreset(),\n            ]\n        )\n    elif
          report_type.lower() == \"target_drift\":\n        report = Report(\n            metrics=[\n                TargetDriftPreset(),\n            ]\n        )\n    else:\n        raise
          NotImplementedError()\n\n    report.run(\n        reference_data=reference_dataset,\n        current_data=production_dataset,\n        column_mapping=column_mapping,\n    )\n\n    Path(output_report).parent.mkdir(parents=True,
          exist_ok=True)\n    report.save_html(output_report)\n    html_content =
          open(output_report, \"r\").read()\n    metadata = {\n        \"outputs\":
          [\n            {\n                \"type\": \"web-app\",\n                \"storage\":
          \"inline\",\n                \"source\": html_content,\n            }\n        ]\n    }\n\n    with
          open(mlpipeline_ui_metadata_path, \"w\") as f:\n        json.dump(metadata,
          f)\n\n    with open(output_json, \"w\") as json_f:\n        json_f.write(report.json())\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Evidently report'', description='''')\n_parser.add_argument(\"--reference-df\",
          dest=\"reference_df\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--production-df\",
          dest=\"production_df\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--target\",
          dest=\"target\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--report-type\",
          dest=\"report_type\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlpipeline-ui-metadata\",
          dest=\"mlpipeline_ui_metadata_path\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-report\",
          dest=\"output_report\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-json\", dest=\"output_json\",
          type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = evidently_report(**_parsed_args)\n"],
          "image": "quay.io/ntlawrence/demo-workflow:3.0.0"}}, "inputs": [{"name":
          "reference_df", "type": "String"}, {"name": "production_df", "type": "String"},
          {"default": "Risk", "name": "target", "optional": true, "type": "String"},
          {"default": "drift", "name": "report_type", "optional": true, "type": "String"}],
          "name": "Evidently report", "outputs": [{"name": "mlpipeline_ui_metadata",
          "type": "String"}, {"name": "output_report", "type": "String"}, {"name":
          "output_json", "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"report_type": "target_drift",
          "target": "Risk"}'}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: load-df-from-postgresql
    container:
      args: [--table-name, TRAIN, --target-column, Risk, --predictions-column, '',
        --data-frame-pkl, /tmp/outputs/data_frame_pkl/data, '----output-paths', /tmp/outputs/num_rows/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'psycopg[binary,pool]' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
        --quiet --no-warn-script-location 'psycopg[binary,pool]' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef load_df_from_postgresql(table_name,\n                     data_frame_pkl,\n\
        \                     target_column = 'Risk',\n                     predictions_column\
        \ = '',\n                     time_window_minutes = None):\n    import warnings\n\
        \    import psycopg\n    from psycopg import sql\n    import os\n    import\
        \ json\n    import pandas as pd\n    import pickle\n    from typing import\
        \ Dict, Any\n    from collections import namedtuple\n\n    def assign_categories_to_df(df,\
        \ column_info):\n        for col_name, levels in column_info[\"label_columns\"\
        ].items():\n            if col_name in df.columns:\n                ctype\
        \ = pd.CategoricalDtype(categories=levels, ordered=False)\n              \
        \  df[col_name] = df[col_name].astype(ctype)\n\n    def get_pg_conn():\n \
        \       host, dbname, username, password, port = (\n            os.environ.get('PG_HOST'),\n\
        \            os.environ.get('PG_DB_NAME'),\n            os.environ.get('PG_USER'),\n\
        \            os.environ.get('PG_PWD'),\n            int(os.environ.get('PG_PORT'))\n\
        \        )\n\n        conn_str = f\"postgresql://{username}:{password}@{host}:{port}/{dbname}?connect_timeout=10&application_name=mlpipeline\"\
        \n        conn = psycopg.connect(conn_str)\n\n        return conn\n\n    def\
        \ df_from_sql(\n        name,\n        conn,\n        column_info,\n     \
        \   target_col = 'Risk',\n        predictions_col = '',\n        time_window_minutes\
        \ = 0\n    ):\n\n        column_list = [sql.Identifier(col) for col in \n\
        \                       (column_info[\"columns\"] + ([] if not predictions_col\
        \ else [predictions_col]))\n                      ]\n\n        if time_window_minutes:\
        \        \n            query = sql.SQL(\n                'SELECT {} FROM {}\
        \ WHERE {} IS NOT NULL AND EXTRACT(EPOCH FROM (CURRENT_TIMESTAMP - \"LastChangeTimestamp\"\
        )) / 60 < {}'\n            ).format(\n                sql.SQL(\", \").join(column_list),\n\
        \                sql.Identifier(name),\n                sql.Identifier(target_column),\n\
        \                sql.Literal(time_window_minutes)\n            )\n       \
        \ else:\n             query = sql.SQL(\n                'SELECT {} FROM {}\
        \ WHERE {} IS NOT NULL'\n            ).format(\n                sql.SQL(\"\
        , \").join(column_list),\n                sql.Identifier(name),\n        \
        \        sql.Identifier(target_column)\n            )\n\n        with conn.cursor()\
        \ as cur:\n            print(query.as_string(cur))\n            cur.execute(query)\n\
        \            df = pd.DataFrame(cur.fetchall(), columns=[desc[0] for desc in\
        \ cur.description])\n\n        assign_categories_to_df(df, column_info)\n\
        \        if predictions_col:\n            df[predictions_col] = df[predictions_col].astype(df[target_col].dtype)\n\
        \n        return df\n\n    column_info = json.loads(os.environ[\"COLUMNS\"\
        ])\n    conn=get_pg_conn()\n    df = df_from_sql(table_name, conn, column_info,\
        \ target_column, predictions_column, time_window_minutes)\n    df.to_pickle(data_frame_pkl)\n\
        \n    out_tuple = namedtuple(\"LoadOutput\", [\"num_rows\"])\n    return out_tuple(df.shape[0])\n\
        \ndef _serialize_int(int_value: int) -> str:\n    if isinstance(int_value,\
        \ str):\n        return int_value\n    if not isinstance(int_value, int):\n\
        \        raise TypeError('Value \"{}\" has type \"{}\" instead of int.'.format(\n\
        \            str(int_value), str(type(int_value))))\n    return str(int_value)\n\
        \nimport argparse\n_parser = argparse.ArgumentParser(prog='Load df from postgresql',\
        \ description='')\n_parser.add_argument(\"--table-name\", dest=\"table_name\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --target-column\", dest=\"target_column\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--predictions-column\", dest=\"predictions_column\"\
        , type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --time-window-minutes\", dest=\"time_window_minutes\", type=int, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--data-frame-pkl\", dest=\"\
        data_frame_pkl\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str,\
        \ nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"\
        _output_paths\", [])\n\n_outputs = load_df_from_postgresql(**_parsed_args)\n\
        \n_output_serializers = [\n    _serialize_int,\n\n]\n\nimport os\nfor idx,\
        \ output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
        \    except OSError:\n        pass\n    with open(output_file, 'w') as f:\n\
        \        f.write(_output_serializers[idx](_outputs[idx]))\n"
      env:
      - {name: PG_HOST, value: 'postgresql.{{workflow.namespace}}.svc'}
      - name: PG_DB_NAME
        valueFrom:
          secretKeyRef: {key: database-name, name: postgresql}
      - name: PG_USER
        valueFrom:
          secretKeyRef: {key: database-user, name: postgresql}
      - name: PG_PWD
        valueFrom:
          secretKeyRef: {key: database-password, name: postgresql}
      - {name: PG_PORT, value: '5432'}
      - name: COLUMNS
        valueFrom:
          configMapKeyRef: {key: columns, name: credit-risk-columns}
      image: quay.io/ntlawrence/demo-workflow:3.0.0
    inputs:
      artifacts:
      - name: time_window_minutes
        path: /tmp/inputs/time_window_minutes/data
        raw: {data: None}
    outputs:
      artifacts:
      - {name: load-df-from-postgresql-data_frame_pkl, path: /tmp/outputs/data_frame_pkl/data}
      - {name: load-df-from-postgresql-num_rows, path: /tmp/outputs/num_rows/data}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: Load_Reference_Data,
        pipelines.kubeflow.org/component_spec: '{"implementation": {"container": {"args":
          ["--table-name", {"inputValue": "table_name"}, {"if": {"cond": {"isPresent":
          "target_column"}, "then": ["--target-column", {"inputValue": "target_column"}]}},
          {"if": {"cond": {"isPresent": "predictions_column"}, "then": ["--predictions-column",
          {"inputValue": "predictions_column"}]}}, {"if": {"cond": {"isPresent": "time_window_minutes"},
          "then": ["--time-window-minutes", {"inputValue": "time_window_minutes"}]}},
          "--data-frame-pkl", {"outputPath": "data_frame_pkl"}, "----output-paths",
          {"outputPath": "num_rows"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''psycopg[binary,pool]''
          || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''psycopg[binary,pool]'' --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef load_df_from_postgresql(table_name,\n                     data_frame_pkl,\n                     target_column
          = ''Risk'',\n                     predictions_column = '''',\n                     time_window_minutes
          = None):\n    import warnings\n    import psycopg\n    from psycopg import
          sql\n    import os\n    import json\n    import pandas as pd\n    import
          pickle\n    from typing import Dict, Any\n    from collections import namedtuple\n\n    def
          assign_categories_to_df(df, column_info):\n        for col_name, levels
          in column_info[\"label_columns\"].items():\n            if col_name in df.columns:\n                ctype
          = pd.CategoricalDtype(categories=levels, ordered=False)\n                df[col_name]
          = df[col_name].astype(ctype)\n\n    def get_pg_conn():\n        host, dbname,
          username, password, port = (\n            os.environ.get(''PG_HOST''),\n            os.environ.get(''PG_DB_NAME''),\n            os.environ.get(''PG_USER''),\n            os.environ.get(''PG_PWD''),\n            int(os.environ.get(''PG_PORT''))\n        )\n\n        conn_str
          = f\"postgresql://{username}:{password}@{host}:{port}/{dbname}?connect_timeout=10&application_name=mlpipeline\"\n        conn
          = psycopg.connect(conn_str)\n\n        return conn\n\n    def df_from_sql(\n        name,\n        conn,\n        column_info,\n        target_col
          = ''Risk'',\n        predictions_col = '''',\n        time_window_minutes
          = 0\n    ):\n\n        column_list = [sql.Identifier(col) for col in \n                       (column_info[\"columns\"]
          + ([] if not predictions_col else [predictions_col]))\n                      ]\n\n        if
          time_window_minutes:        \n            query = sql.SQL(\n                ''SELECT
          {} FROM {} WHERE {} IS NOT NULL AND EXTRACT(EPOCH FROM (CURRENT_TIMESTAMP
          - \"LastChangeTimestamp\")) / 60 < {}''\n            ).format(\n                sql.SQL(\",
          \").join(column_list),\n                sql.Identifier(name),\n                sql.Identifier(target_column),\n                sql.Literal(time_window_minutes)\n            )\n        else:\n             query
          = sql.SQL(\n                ''SELECT {} FROM {} WHERE {} IS NOT NULL''\n            ).format(\n                sql.SQL(\",
          \").join(column_list),\n                sql.Identifier(name),\n                sql.Identifier(target_column)\n            )\n\n        with
          conn.cursor() as cur:\n            print(query.as_string(cur))\n            cur.execute(query)\n            df
          = pd.DataFrame(cur.fetchall(), columns=[desc[0] for desc in cur.description])\n\n        assign_categories_to_df(df,
          column_info)\n        if predictions_col:\n            df[predictions_col]
          = df[predictions_col].astype(df[target_col].dtype)\n\n        return df\n\n    column_info
          = json.loads(os.environ[\"COLUMNS\"])\n    conn=get_pg_conn()\n    df =
          df_from_sql(table_name, conn, column_info, target_column, predictions_column,
          time_window_minutes)\n    df.to_pickle(data_frame_pkl)\n\n    out_tuple
          = namedtuple(\"LoadOutput\", [\"num_rows\"])\n    return out_tuple(df.shape[0])\n\ndef
          _serialize_int(int_value: int) -> str:\n    if isinstance(int_value, str):\n        return
          int_value\n    if not isinstance(int_value, int):\n        raise TypeError(''Value
          \"{}\" has type \"{}\" instead of int.''.format(\n            str(int_value),
          str(type(int_value))))\n    return str(int_value)\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Load df from postgresql'', description='''')\n_parser.add_argument(\"--table-name\",
          dest=\"table_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--target-column\",
          dest=\"target_column\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictions-column\",
          dest=\"predictions_column\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--time-window-minutes\",
          dest=\"time_window_minutes\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--data-frame-pkl\",
          dest=\"data_frame_pkl\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\", dest=\"_output_paths\",
          type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = load_df_from_postgresql(**_parsed_args)\n\n_output_serializers
          = [\n    _serialize_int,\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "quay.io/ntlawrence/demo-workflow:3.0.0"}}, "inputs": [{"name":
          "table_name", "type": "String"}, {"default": "Risk", "name": "target_column",
          "optional": true, "type": "String"}, {"default": "", "name": "predictions_column",
          "optional": true, "type": "String"}, {"name": "time_window_minutes", "optional":
          true, "type": "Integer"}], "name": "Load df from postgresql", "outputs":
          [{"name": "data_frame_pkl", "type": "String"}, {"name": "num_rows", "type":
          "int"}]}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"predictions_column":
          "", "table_name": "TRAIN", "target_column": "Risk"}', pipelines.kubeflow.org/max_cache_staleness: P0D}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: load-df-from-postgresql-2
    container:
      args: [--table-name, CLIENT_DATA, --target-column, Risk, --predictions-column,
        PredictedRisk, --time-window-minutes, '{{inputs.parameters.time_window_minutes}}',
        --data-frame-pkl, /tmp/outputs/data_frame_pkl/data, '----output-paths', /tmp/outputs/num_rows/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'psycopg[binary,pool]' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
        --quiet --no-warn-script-location 'psycopg[binary,pool]' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef load_df_from_postgresql(table_name,\n                     data_frame_pkl,\n\
        \                     target_column = 'Risk',\n                     predictions_column\
        \ = '',\n                     time_window_minutes = None):\n    import warnings\n\
        \    import psycopg\n    from psycopg import sql\n    import os\n    import\
        \ json\n    import pandas as pd\n    import pickle\n    from typing import\
        \ Dict, Any\n    from collections import namedtuple\n\n    def assign_categories_to_df(df,\
        \ column_info):\n        for col_name, levels in column_info[\"label_columns\"\
        ].items():\n            if col_name in df.columns:\n                ctype\
        \ = pd.CategoricalDtype(categories=levels, ordered=False)\n              \
        \  df[col_name] = df[col_name].astype(ctype)\n\n    def get_pg_conn():\n \
        \       host, dbname, username, password, port = (\n            os.environ.get('PG_HOST'),\n\
        \            os.environ.get('PG_DB_NAME'),\n            os.environ.get('PG_USER'),\n\
        \            os.environ.get('PG_PWD'),\n            int(os.environ.get('PG_PORT'))\n\
        \        )\n\n        conn_str = f\"postgresql://{username}:{password}@{host}:{port}/{dbname}?connect_timeout=10&application_name=mlpipeline\"\
        \n        conn = psycopg.connect(conn_str)\n\n        return conn\n\n    def\
        \ df_from_sql(\n        name,\n        conn,\n        column_info,\n     \
        \   target_col = 'Risk',\n        predictions_col = '',\n        time_window_minutes\
        \ = 0\n    ):\n\n        column_list = [sql.Identifier(col) for col in \n\
        \                       (column_info[\"columns\"] + ([] if not predictions_col\
        \ else [predictions_col]))\n                      ]\n\n        if time_window_minutes:\
        \        \n            query = sql.SQL(\n                'SELECT {} FROM {}\
        \ WHERE {} IS NOT NULL AND EXTRACT(EPOCH FROM (CURRENT_TIMESTAMP - \"LastChangeTimestamp\"\
        )) / 60 < {}'\n            ).format(\n                sql.SQL(\", \").join(column_list),\n\
        \                sql.Identifier(name),\n                sql.Identifier(target_column),\n\
        \                sql.Literal(time_window_minutes)\n            )\n       \
        \ else:\n             query = sql.SQL(\n                'SELECT {} FROM {}\
        \ WHERE {} IS NOT NULL'\n            ).format(\n                sql.SQL(\"\
        , \").join(column_list),\n                sql.Identifier(name),\n        \
        \        sql.Identifier(target_column)\n            )\n\n        with conn.cursor()\
        \ as cur:\n            print(query.as_string(cur))\n            cur.execute(query)\n\
        \            df = pd.DataFrame(cur.fetchall(), columns=[desc[0] for desc in\
        \ cur.description])\n\n        assign_categories_to_df(df, column_info)\n\
        \        if predictions_col:\n            df[predictions_col] = df[predictions_col].astype(df[target_col].dtype)\n\
        \n        return df\n\n    column_info = json.loads(os.environ[\"COLUMNS\"\
        ])\n    conn=get_pg_conn()\n    df = df_from_sql(table_name, conn, column_info,\
        \ target_column, predictions_column, time_window_minutes)\n    df.to_pickle(data_frame_pkl)\n\
        \n    out_tuple = namedtuple(\"LoadOutput\", [\"num_rows\"])\n    return out_tuple(df.shape[0])\n\
        \ndef _serialize_int(int_value: int) -> str:\n    if isinstance(int_value,\
        \ str):\n        return int_value\n    if not isinstance(int_value, int):\n\
        \        raise TypeError('Value \"{}\" has type \"{}\" instead of int.'.format(\n\
        \            str(int_value), str(type(int_value))))\n    return str(int_value)\n\
        \nimport argparse\n_parser = argparse.ArgumentParser(prog='Load df from postgresql',\
        \ description='')\n_parser.add_argument(\"--table-name\", dest=\"table_name\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --target-column\", dest=\"target_column\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--predictions-column\", dest=\"predictions_column\"\
        , type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --time-window-minutes\", dest=\"time_window_minutes\", type=int, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--data-frame-pkl\", dest=\"\
        data_frame_pkl\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str,\
        \ nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"\
        _output_paths\", [])\n\n_outputs = load_df_from_postgresql(**_parsed_args)\n\
        \n_output_serializers = [\n    _serialize_int,\n\n]\n\nimport os\nfor idx,\
        \ output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
        \    except OSError:\n        pass\n    with open(output_file, 'w') as f:\n\
        \        f.write(_output_serializers[idx](_outputs[idx]))\n"
      env:
      - {name: PG_HOST, value: 'postgresql.{{workflow.namespace}}.svc'}
      - name: PG_DB_NAME
        valueFrom:
          secretKeyRef: {key: database-name, name: postgresql}
      - name: PG_USER
        valueFrom:
          secretKeyRef: {key: database-user, name: postgresql}
      - name: PG_PWD
        valueFrom:
          secretKeyRef: {key: database-password, name: postgresql}
      - {name: PG_PORT, value: '5432'}
      - name: COLUMNS
        valueFrom:
          configMapKeyRef: {key: columns, name: credit-risk-columns}
      image: quay.io/ntlawrence/demo-workflow:3.0.0
    inputs:
      parameters:
      - {name: time_window_minutes}
    outputs:
      parameters:
      - name: load-df-from-postgresql-2-num_rows
        valueFrom: {path: /tmp/outputs/num_rows/data}
      artifacts:
      - {name: load-df-from-postgresql-2-data_frame_pkl, path: /tmp/outputs/data_frame_pkl/data}
      - {name: load-df-from-postgresql-2-num_rows, path: /tmp/outputs/num_rows/data}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: Load_Production_Data,
        pipelines.kubeflow.org/component_spec: '{"implementation": {"container": {"args":
          ["--table-name", {"inputValue": "table_name"}, {"if": {"cond": {"isPresent":
          "target_column"}, "then": ["--target-column", {"inputValue": "target_column"}]}},
          {"if": {"cond": {"isPresent": "predictions_column"}, "then": ["--predictions-column",
          {"inputValue": "predictions_column"}]}}, {"if": {"cond": {"isPresent": "time_window_minutes"},
          "then": ["--time-window-minutes", {"inputValue": "time_window_minutes"}]}},
          "--data-frame-pkl", {"outputPath": "data_frame_pkl"}, "----output-paths",
          {"outputPath": "num_rows"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''psycopg[binary,pool]''
          || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''psycopg[binary,pool]'' --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef load_df_from_postgresql(table_name,\n                     data_frame_pkl,\n                     target_column
          = ''Risk'',\n                     predictions_column = '''',\n                     time_window_minutes
          = None):\n    import warnings\n    import psycopg\n    from psycopg import
          sql\n    import os\n    import json\n    import pandas as pd\n    import
          pickle\n    from typing import Dict, Any\n    from collections import namedtuple\n\n    def
          assign_categories_to_df(df, column_info):\n        for col_name, levels
          in column_info[\"label_columns\"].items():\n            if col_name in df.columns:\n                ctype
          = pd.CategoricalDtype(categories=levels, ordered=False)\n                df[col_name]
          = df[col_name].astype(ctype)\n\n    def get_pg_conn():\n        host, dbname,
          username, password, port = (\n            os.environ.get(''PG_HOST''),\n            os.environ.get(''PG_DB_NAME''),\n            os.environ.get(''PG_USER''),\n            os.environ.get(''PG_PWD''),\n            int(os.environ.get(''PG_PORT''))\n        )\n\n        conn_str
          = f\"postgresql://{username}:{password}@{host}:{port}/{dbname}?connect_timeout=10&application_name=mlpipeline\"\n        conn
          = psycopg.connect(conn_str)\n\n        return conn\n\n    def df_from_sql(\n        name,\n        conn,\n        column_info,\n        target_col
          = ''Risk'',\n        predictions_col = '''',\n        time_window_minutes
          = 0\n    ):\n\n        column_list = [sql.Identifier(col) for col in \n                       (column_info[\"columns\"]
          + ([] if not predictions_col else [predictions_col]))\n                      ]\n\n        if
          time_window_minutes:        \n            query = sql.SQL(\n                ''SELECT
          {} FROM {} WHERE {} IS NOT NULL AND EXTRACT(EPOCH FROM (CURRENT_TIMESTAMP
          - \"LastChangeTimestamp\")) / 60 < {}''\n            ).format(\n                sql.SQL(\",
          \").join(column_list),\n                sql.Identifier(name),\n                sql.Identifier(target_column),\n                sql.Literal(time_window_minutes)\n            )\n        else:\n             query
          = sql.SQL(\n                ''SELECT {} FROM {} WHERE {} IS NOT NULL''\n            ).format(\n                sql.SQL(\",
          \").join(column_list),\n                sql.Identifier(name),\n                sql.Identifier(target_column)\n            )\n\n        with
          conn.cursor() as cur:\n            print(query.as_string(cur))\n            cur.execute(query)\n            df
          = pd.DataFrame(cur.fetchall(), columns=[desc[0] for desc in cur.description])\n\n        assign_categories_to_df(df,
          column_info)\n        if predictions_col:\n            df[predictions_col]
          = df[predictions_col].astype(df[target_col].dtype)\n\n        return df\n\n    column_info
          = json.loads(os.environ[\"COLUMNS\"])\n    conn=get_pg_conn()\n    df =
          df_from_sql(table_name, conn, column_info, target_column, predictions_column,
          time_window_minutes)\n    df.to_pickle(data_frame_pkl)\n\n    out_tuple
          = namedtuple(\"LoadOutput\", [\"num_rows\"])\n    return out_tuple(df.shape[0])\n\ndef
          _serialize_int(int_value: int) -> str:\n    if isinstance(int_value, str):\n        return
          int_value\n    if not isinstance(int_value, int):\n        raise TypeError(''Value
          \"{}\" has type \"{}\" instead of int.''.format(\n            str(int_value),
          str(type(int_value))))\n    return str(int_value)\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Load df from postgresql'', description='''')\n_parser.add_argument(\"--table-name\",
          dest=\"table_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--target-column\",
          dest=\"target_column\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictions-column\",
          dest=\"predictions_column\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--time-window-minutes\",
          dest=\"time_window_minutes\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--data-frame-pkl\",
          dest=\"data_frame_pkl\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\", dest=\"_output_paths\",
          type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = load_df_from_postgresql(**_parsed_args)\n\n_output_serializers
          = [\n    _serialize_int,\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "quay.io/ntlawrence/demo-workflow:3.0.0"}}, "inputs": [{"name":
          "table_name", "type": "String"}, {"default": "Risk", "name": "target_column",
          "optional": true, "type": "String"}, {"default": "", "name": "predictions_column",
          "optional": true, "type": "String"}, {"name": "time_window_minutes", "optional":
          true, "type": "Integer"}], "name": "Load df from postgresql", "outputs":
          [{"name": "data_frame_pkl", "type": "String"}, {"name": "num_rows", "type":
          "int"}]}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"predictions_column":
          "PredictedRisk", "table_name": "CLIENT_DATA", "target_column": "Risk", "time_window_minutes":
          "{{inputs.parameters.time_window_minutes}}"}', pipelines.kubeflow.org/max_cache_staleness: P0D}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: monitor-credit-risk-ai
    inputs:
      parameters:
      - {name: min_rows}
      - {name: time_window_minutes}
    dag:
      tasks:
      - name: condition-1
        template: condition-1
        when: '{{tasks.load-df-from-postgresql-2.outputs.parameters.load-df-from-postgresql-2-num_rows}}
          > {{inputs.parameters.min_rows}}'
        dependencies: [load-df-from-postgresql, load-df-from-postgresql-2]
        arguments:
          artifacts:
          - {name: load-df-from-postgresql-2-data_frame_pkl, from: '{{tasks.load-df-from-postgresql-2.outputs.artifacts.load-df-from-postgresql-2-data_frame_pkl}}'}
          - {name: load-df-from-postgresql-data_frame_pkl, from: '{{tasks.load-df-from-postgresql.outputs.artifacts.load-df-from-postgresql-data_frame_pkl}}'}
      - {name: load-df-from-postgresql, template: load-df-from-postgresql}
      - name: load-df-from-postgresql-2
        template: load-df-from-postgresql-2
        arguments:
          parameters:
          - {name: time_window_minutes, value: '{{inputs.parameters.time_window_minutes}}'}
  arguments:
    parameters:
    - {name: time_window_minutes, value: '10080'}
    - {name: min_rows, value: '5'}
  serviceAccountName: pipeline-runner
