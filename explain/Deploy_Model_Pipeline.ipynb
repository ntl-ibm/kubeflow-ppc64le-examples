{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4c12c69-b1dd-4945-ba52-43644fa3eb2d",
   "metadata": {},
   "source": [
    "# Train and Deploy the Credit Risk Model\n",
    "\n",
    "This pipeline trains and deploys the components of the credit risk application that are related to the model.\n",
    "\n",
    "The components that are trained and deployed are:\n",
    "* Transformer (data preprocessing)\n",
    "* Predictor (ONNX runtimed)\n",
    "* Explainer (Alibi)\n",
    "\n",
    "The pipeline also includes training data quality reports and model quality evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dca6eebb-74b5-4d39-b3dd-e47656bbd2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp.components import InputPath, OutputPath\n",
    "from kfp import dsl\n",
    "from typing import List, Tuple\n",
    "from kfp.dsl import ContainerOp\n",
    "from kubernetes.client.models import V1EnvVar,V1EnvVarSource, V1SecretKeySelector,V1ConfigMapKeySelector\n",
    "from typing import NamedTuple\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9602cd5a-01eb-4805-8749-c38518e7075c",
   "metadata": {},
   "source": [
    "## Images\n",
    "The source for these images is included in the container image subdirectory.\n",
    "\n",
    "This example uses many custom images, so that we can use open-source packages not available in our notebook container images. Using multiple images allows us to layer our builds, so individual components can be built faster. It also allows us to scope packages to only those parts of the application that need them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69a64e6a-4b89-4a45-a950-9ef464ff4259",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_IMAGE = \"quay.io/ntlawrence/demo-workflow:3.0.0\"\n",
    "KSERVE_IMAGE = \"quay.io/ntlawrence/demo-kserve:3.0.0\"\n",
    "TRANSFORMER_IMAGE = \"quay.io/ntlawrence/demo-transformer:3.0.0\"\n",
    "PREDICTOR_IMAGE = \"quay.io/ntlawrence/demo-predictor:3.0.0\"\n",
    "EXPLAINER_IMAGE = \"quay.io/ntlawrence/demo-explainer:3.0.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f953cc24-6f2e-45bc-87bc-6142d6ba2075",
   "metadata": {},
   "source": [
    "## Define the tensorborad component\n",
    "\n",
    "Define a component to start a tensorboard service to monitor training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab52be0e-c807-4d59-bc67-cccbfd695927",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIGURE_TENSORBOARD_COMPONENT = f\"{os.getenv('HOME')}/kubeflow-ppc64le-examples/configure_tensorboard_component/configure_tensorboard_component.yaml\"\n",
    "\n",
    "configure_tensorboard_comp = kfp.components.load_component_from_file(\n",
    "    CONFIGURE_TENSORBOARD_COMPONENT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e407e560-80f0-48eb-b7f4-00b816fc4b84",
   "metadata": {},
   "source": [
    "## Define components to load training data\n",
    "\n",
    "The demo is designed to be able to pull training data from either DB2 or PostgreSQL. The following components are for each source. The actual pipeline will only use one of them.\n",
    "\n",
    "We could imagine defining both components in different files, and pulling in only the one that is needed here. But to keep things simple, they are both defined here in the pipeline creation script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdbbef71-a79f-4472-b9d6-3969b2c3bb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df_from_db2(table_name: str,\n",
    "                     data_frame_pkl: OutputPath(str)):\n",
    "    import warnings\n",
    "    import ibm_db\n",
    "    import ibm_db_dbi\n",
    "    import os\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    from typing import Dict, Any\n",
    "    \n",
    "    def assign_categories_to_df(df: pd.DataFrame, column_info: Dict[str, any]) -> None:\n",
    "        for col_name, levels in column_info[\"label_columns\"].items():\n",
    "            if col_name in df.columns:\n",
    "                ctype = pd.CategoricalDtype(categories=levels, ordered=False)\n",
    "                df[col_name] = df[col_name].astype(ctype)\n",
    "\n",
    "    def df_from_sql(\n",
    "        name: str,\n",
    "        conn: ibm_db.IBM_DBConnection,\n",
    "        column_info: Dict[str, Any],\n",
    "    ) -> pd.DataFrame:\n",
    "        sql_safe_name = name.replace('\"', \"\")\n",
    "\n",
    "        rStmtColsSql = \",\".join([f'\"{col}\"' for col in column_info[\"columns\"]])\n",
    "        rSql = f'SELECT {rStmtColsSql} FROM \"{sql_safe_name}\" ORDER BY \"ACCOUNT_ID\"'\n",
    "\n",
    "        read_conn = ibm_db_dbi.Connection(conn)\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", message=\"pandas only support SQLAlchemy\")\n",
    "            df = pd.read_sql(rSql, read_conn)\n",
    "\n",
    "        assign_categories_to_df(df, column_info)\n",
    "        return df\n",
    "    \n",
    "    conn_str = (\n",
    "    \"DRIVER={IBM DB2 ODBC DRIVER};\"\n",
    "    f\"DATABASE=BLUDB;HOSTNAME={os.environ['db2_host']};PORT={os.environ['db2_port']};PROTOCOL=TCPIP;UID={os.environ['db2_user']};Pwd={os.environ['db2_pwd']};SECURITY=SSL;\"\n",
    "    )\n",
    "        \n",
    "    conn = ibm_db.connect(conn_str, \"\", \"\")\n",
    "\n",
    "    column_info = json.loads(os.environ[\"COLUMNS\"])\n",
    "    df = df_from_sql(table_name, conn, column_info)\n",
    "    df.to_pickle(data_frame_pkl)\n",
    "\n",
    "\n",
    "load_df_from_db2_comp = kfp.components.create_component_from_func(\n",
    "    func=load_df_from_db2, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ccacd54-7107-4165-9c06-497bd8612b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df_from_postgresql(table_name: str,\n",
    "                           data_frame_pkl: OutputPath(str)):\n",
    "    import os\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    from typing import Dict, Any\n",
    "    import psycopg\n",
    "    from psycopg import sql\n",
    "    import yaml\n",
    "    \n",
    "    def get_pg_conn() -> psycopg.Connection:\n",
    "        host, dbname, username, password, port = (\n",
    "            os.environ.get('PG_HOST'),\n",
    "            os.environ.get('PG_DB_NAME'),\n",
    "            os.environ.get('PG_USER'),\n",
    "            os.environ.get('PG_PWD'),\n",
    "            int(os.environ.get('PG_PORT'))\n",
    "        )\n",
    "\n",
    "        conn_str = f\"postgresql://{username}:{password}@{host}:{port}/{dbname}?connect_timeout=10&application_name=mlpipeline\"\n",
    "        print(conn_str)\n",
    "        conn = psycopg.connect(conn_str)\n",
    "\n",
    "        return conn\n",
    "    \n",
    "    def assign_categories_to_df(df: pd.DataFrame, column_info: Dict[str, any]) -> None:\n",
    "        for col_name, levels in column_info[\"label_columns\"].items():\n",
    "            if col_name in df.columns:\n",
    "                ctype = pd.CategoricalDtype(categories=levels, ordered=False)\n",
    "                df[col_name] = df[col_name].astype(ctype)\n",
    "\n",
    "    def df_from_sql(\n",
    "        name: str,\n",
    "        db: psycopg.Connection,\n",
    "        column_info: Dict[str, Any],\n",
    "    ) -> pd.DataFrame:\n",
    "        with db.cursor() as cur:\n",
    "            cur.execute(sql.SQL('SELECT * FROM {} ORDER BY \"ACCOUNT_ID\"').format(sql.Identifier(table_name)))\n",
    "            df = pd.DataFrame(cur.fetchall(), columns=[desc[0] for desc in cur.description])\n",
    "        assign_categories_to_df(df, column_info)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    conn = get_pg_conn()\n",
    "    column_info = json.loads(os.environ[\"COLUMNS\"])\n",
    "    df = df_from_sql(table_name, conn, column_info)\n",
    "    df.to_pickle(data_frame_pkl)\n",
    "\n",
    "\n",
    "load_df_from_postgresql_comp = kfp.components.create_component_from_func(\n",
    "    func=load_df_from_postgresql, base_image=BASE_IMAGE, packages_to_install=[\"psycopg[binary,pool]\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e378e6-4431-447f-8b57-c3462920358d",
   "metadata": {},
   "source": [
    "## Generate a data quality report\n",
    "\n",
    "The report appears in the task's visualization's tab. It helps to understand what bias may exist in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "033b8788-1df5-4675-a303-504f7a71924c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_quality_report(df: InputPath(str),\n",
    "                        features: List[str],\n",
    "                        mlpipeline_ui_metadata_path: OutputPath(str),\n",
    "                        output_report: OutputPath(str),\n",
    "                        target: str = 'Risk'):\n",
    "    from evidently.metric_preset import DataQualityPreset\n",
    "    from evidently.report import Report\n",
    "    from evidently import ColumnMapping\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    import json\n",
    "\n",
    "    \n",
    "    dataset = pd.read_pickle(df)\n",
    "    column_info = json.loads(os.environ[\"COLUMNS\"])\n",
    "\n",
    "    column_mapping = ColumnMapping()\n",
    "    column_mapping.target = target\n",
    "    column_mapping.task = \"classification\"\n",
    "    feature_set = set(features)\n",
    "    column_mapping.numerical_features = [\n",
    "        c\n",
    "        for c in column_info[\"int_columns\"]\n",
    "        if c in feature_set\n",
    "    ]\n",
    "    column_mapping.categorical_features = [\n",
    "        c\n",
    "        for c in column_info[\"label_columns\"]\n",
    "        if c in feature_set\n",
    "    ]\n",
    "\n",
    "    report = Report(\n",
    "        metrics=[\n",
    "            DataQualityPreset(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    report.run(\n",
    "        reference_data=None,\n",
    "        current_data=dataset,\n",
    "        column_mapping=column_mapping,\n",
    "    )\n",
    "\n",
    "    Path(output_report).parent.mkdir(parents=True, exist_ok=True)\n",
    "    report.save_html(output_report)\n",
    "    html_content = open(output_report, \"r\").read()\n",
    "    metadata = {\n",
    "        \"outputs\": [\n",
    "            {\n",
    "                \"type\": \"web-app\",\n",
    "                \"storage\": \"inline\",\n",
    "                \"source\": html_content,\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    with open(mlpipeline_ui_metadata_path, \"w\") as f:\n",
    "        json.dump(metadata, f)\n",
    "        \n",
    "data_quality_report_comp = kfp.components.create_component_from_func(\n",
    "    func=data_quality_report, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67960607-30a6-4c5c-b2f4-4223fa525007",
   "metadata": {},
   "source": [
    "## Define a component to fit the preprocessor\n",
    "\n",
    "Because the input features contain categorical data, we'll need a preprocessor to one-hot encode the cateogrical features. Non-categorical features are passed through, making the result features that are ready to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bd8a481-de19-4837-94cf-bcf805617245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_preprocessor(\n",
    "    training_df: InputPath(str),\n",
    "    preprocessor_pkl: OutputPath(str),\n",
    "    features: List[str],\n",
    "):\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import joblib\n",
    "    import os\n",
    "\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    from sklearn.pipeline import Pipeline\n",
    "\n",
    "    feature_set = set(features)\n",
    "    column_info = json.loads(os.environ[\"COLUMNS\"])\n",
    "\n",
    "    ohe_labels = [\n",
    "        (\n",
    "            \"ohe_\" + label,\n",
    "            OneHotEncoder(\n",
    "                handle_unknown=\"ignore\", sparse_output=False, categories=[levels]\n",
    "            ),\n",
    "            [label],\n",
    "        )\n",
    "        for label, levels in column_info[\"label_columns\"].items()\n",
    "        if label in feature_set\n",
    "    ]\n",
    "\n",
    "    int_cols = [\n",
    "        (\n",
    "            \"passthrough\",\n",
    "            \"passthrough\",\n",
    "            [col for col in column_info[\"int_columns\"] if col in feature_set],\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"Preprocess\", ColumnTransformer(ohe_labels + int_cols, remainder=\"drop\")),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(pipe)\n",
    "    train = pd.read_pickle(training_df)\n",
    "    print(train.dtypes)\n",
    "    pipe.fit(train)\n",
    "    joblib.dump(pipe, preprocessor_pkl)\n",
    "\n",
    "\n",
    "fit_preprocessor_comp = kfp.components.create_component_from_func(\n",
    "    func=fit_preprocessor, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2edfd89-c5ef-42db-8f46-85f2520cc1cf",
   "metadata": {},
   "source": [
    "## Define a component to train the model\n",
    "\n",
    "The model is a shallow neural network of fully connected layers, implemented in TensorFlow. In practice, we might be better model performance using a random forest with Scikit-learn or [SnapML](https://snapml.readthedocs.io/en/latest/installation.html), however a neural network was selected to show off the features of GPUs on IBM Power 9 AC922's and IBM Power 10 with MMA technology.\n",
    "\n",
    "Since a neural network has difficulty providing explainations for its predictions, using one allows us to showcase open-source solutions for explainable AI.\n",
    "\n",
    "In practice, the German Credit Risk dataset is usuallay paired Random Forests in AI demos. SnapML provides that capability and has been reported to have excellent performance on Power 10 systems; this would be a worthwhile alternative to investigate in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2af502ad-b08b-40e6-a8d2-e75fd3374826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    training_df: InputPath(str),\n",
    "    preprocessor: InputPath(str),\n",
    "    model: OutputPath(str),\n",
    "    target_processing_config: OutputPath(str),\n",
    "    mlpipeline_ui_metadata_path: OutputPath(str),\n",
    "    target: str = \"Risk\",\n",
    "    tensorboard_dir: str = None,\n",
    "):\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import joblib\n",
    "    import tensorflow as tf\n",
    "    from keras import Sequential\n",
    "    from keras.layers import Dense, Dropout, BatchNormalization, Input\n",
    "    from keras.callbacks import EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    import numpy as np\n",
    "    import os\n",
    "    from sklearn.metrics import PrecisionRecallDisplay\n",
    "    import base64\n",
    "\n",
    "    tf.keras.utils.set_random_seed(42)\n",
    "    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.DEBUG)\n",
    "    target_processing_config_dict = {\n",
    "        \"threshold\" : 0.5,\n",
    "        \"target_names\" : {0: \"No Risk\", 1: \"Risk\"}\n",
    "    }\n",
    "\n",
    "    def get_tf_model(num_features: int) -> Tuple[tf.keras.Model, List[tf.keras.callbacks.Callback]]:\n",
    "\n",
    "        tf_model = Sequential(\n",
    "            [\n",
    "                Input(shape=(num_features,)),\n",
    "                BatchNormalization(),\n",
    "                Dense(64, activation=\"sigmoid\", name=\"layer1\"),\n",
    "                BatchNormalization(),\n",
    "                Dropout(0.3, name=\"dropout1\"),\n",
    "                Dense(32, activation=\"sigmoid\", name=\"layer2\"),\n",
    "                BatchNormalization(),\n",
    "                Dropout(0.3, name=\"dropout2\"),\n",
    "                Dense(16, activation=\"sigmoid\", name=\"layer3\"),\n",
    "                Dropout(0.3, name=\"dropout3\"),\n",
    "                Dense(\n",
    "                    1,\n",
    "                    activation=\"sigmoid\",\n",
    "                    name=\"output\",\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        tf_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "\n",
    "        callbacks = [\n",
    "            EarlyStopping(\n",
    "                monitor=\"val_loss\",\n",
    "                patience=20,\n",
    "                verbose=0,\n",
    "                mode=\"min\",\n",
    "                restore_best_weights=True,\n",
    "            ),\n",
    "            ReduceLROnPlateau(\n",
    "                monitor=\"val_loss\",\n",
    "                factor=0.1,\n",
    "                patience=10,\n",
    "                verbose=1,\n",
    "                min_delta=0.0001,\n",
    "                mode=\"min\",\n",
    "            )\n",
    "        ]\n",
    "        print(f\"constructing tensorborad with log dir {tensorboard_dir}...\")\n",
    "        if tensorboard_dir:\n",
    "            callbacks.append(TensorBoard(\n",
    "                log_dir=tensorboard_dir\n",
    "            ))\n",
    "\n",
    "        return tf_model, callbacks\n",
    "\n",
    "    print(\"loading training data...\")\n",
    "    train = pd.read_pickle(training_df)\n",
    "    print(\"loading preprocessor...\")\n",
    "    preprocessor = joblib.load(preprocessor)\n",
    "\n",
    "    X = tf.convert_to_tensor(preprocessor.transform(train))\n",
    "    y = tf.convert_to_tensor(\n",
    "        train.loc[:, target].apply(lambda v: 1 if v == target_processing_config_dict[\"target_names\"][1] else 0)\n",
    "    )\n",
    "    print(\"obtaining model....\")\n",
    "    tf_model, callbacks = get_tf_model(num_features=X.shape[1])\n",
    "    print(\"Training...\")\n",
    "    tf_model.fit(\n",
    "        X,\n",
    "        y,\n",
    "        validation_split=0.2,\n",
    "        epochs=500,\n",
    "        callbacks=callbacks,\n",
    "        class_weight={0: 1, 1: 2},\n",
    "    )\n",
    "    \n",
    "    # calculate best threshold of highest f1 score\n",
    "    predictions = tf_model.predict(X)\n",
    "    precision, recall, thresholds = precision_recall_curve(\n",
    "        y_true=y.numpy(), probas_pred=predictions.flatten()\n",
    "    )\n",
    "    f1s = 2 * (precision * recall) / (precision + recall)\n",
    "    threshold_index = np.argmax(f1s)\n",
    "    target_processing_config_dict[\"threshold\"] = float(thresholds[threshold_index])\n",
    "    \n",
    "    # Save model and threshold config\n",
    "    tf_model.save(model, save_format=\"h5\")\n",
    "    with open(target_processing_config, \"w\") as f:\n",
    "        json.dump(target_processing_config_dict, f)\n",
    "        \n",
    "    # Plot precision recall curve\n",
    "    plt = PrecisionRecallDisplay.from_predictions(\n",
    "        y_true=y.numpy(), y_pred=predictions.flatten(),\n",
    "    )\n",
    "    plt.ax_.plot(recall[threshold_index], precision[threshold_index], \n",
    "                     marker=\"o\", markersize=10, markeredgecolor=\"black\", markerfacecolor=\"red\")\n",
    "    plt.ax_.text(recall[threshold_index] + .03, precision[threshold_index] + .03,\n",
    "                  (f\"({recall[threshold_index]:.3f},{precision[threshold_index]:.3f})\\n\" +\n",
    "                   f\"threshold={thresholds[threshold_index]:.3f}\\n\" +\n",
    "                   f\"f1={f1s[threshold_index]:.3f}\")\n",
    "                )\n",
    "    plt.ax_.set_title(\"Precision Recall - Risk (Training)\")\n",
    "\n",
    "    plt.figure_.savefig(\"pr.jpg\")\n",
    "    with open(\"pr.jpg\", \"rb\") as f:\n",
    "        jpg = base64.b64encode(f.read())\n",
    "    html = f'<img src=\"data:image/jpg;base64,{jpg.decode(\"utf-8\")}\"/>'\n",
    "    metadata = {\"outputs\": [{\"type\": \"markdown\", \"storage\": \"inline\", \"source\": html}]}\n",
    "    with open(mlpipeline_ui_metadata_path, \"w\") as metadata_file:\n",
    "        json.dump(metadata, metadata_file)\n",
    "\n",
    "train_comp = kfp.components.create_component_from_func(\n",
    "    func=train, base_image=BASE_IMAGE, packages_to_install=[\"minio\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5f6123-fda4-4791-b37a-96d59e3d440c",
   "metadata": {},
   "source": [
    "## Define a component to evalute the model\n",
    "\n",
    "This component evaluates the model and produces a classification report in the visualizations tab. The output parameter mlpipeline_ui_metadata_path is what defines the visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8937ccc-1de2-42cf-a4c2-5bed40cfb022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    df: InputPath(str),\n",
    "    preprocessor: InputPath(str),\n",
    "    onnx_model: InputPath(str),\n",
    "    target_processing_config: InputPath(str),\n",
    "    output_report: OutputPath(str),\n",
    "    mlpipeline_ui_metadata_path: OutputPath(str),\n",
    "    target=\"Risk\",\n",
    ") -> NamedTuple(\"EvaluationOutput\", [(\"mlpipeline_metrics\", \"Metrics\")]):\n",
    "    import pandas as pd\n",
    "    import joblib\n",
    "    import json\n",
    "    from evidently.metric_preset import ClassificationPreset\n",
    "    from evidently.report import Report\n",
    "    from evidently import ColumnMapping\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    import onnxruntime as ort\n",
    "    import numpy as np\n",
    "    from collections import namedtuple\n",
    "\n",
    "    dataset = pd.read_pickle(df)\n",
    "    preprocessor = joblib.load(preprocessor)\n",
    "    \n",
    "    inference_session = ort.InferenceSession(\n",
    "            onnx_model, providers=[\"CPUExecutionProvider\"]\n",
    "    )\n",
    "    \n",
    "    with open(target_processing_config, \"r\") as f:\n",
    "        target_processing_config_dict = json.load(f)\n",
    "        target_processing_config_dict[\"target_names\"] = {int(k):v for k,v in target_processing_config_dict[\"target_names\"].items()}\n",
    "\n",
    "    column_info = json.loads(os.environ[\"COLUMNS\"])\n",
    "\n",
    "    X = preprocessor.transform(dataset).astype(np.float32)\n",
    "    y_prob = np.array(\n",
    "        inference_session.run(\n",
    "            [], {\"input_1\": X}\n",
    "        )).flatten()\n",
    "\n",
    "    dataset[\"Prediction\"] = pd.Series(y_prob).apply(\n",
    "        lambda p: 1 if p > target_processing_config_dict[\"threshold\"] else 0\n",
    "    )\n",
    "    dataset[\"Actual\"] = dataset.loc[:, target].apply(\n",
    "        lambda v: 1 if v == target_processing_config_dict[\"target_names\"][1] else 0\n",
    "    )\n",
    "\n",
    "    column_mapping = ColumnMapping()\n",
    "    column_mapping.target_names = target_processing_config_dict[\"target_names\"]\n",
    "    column_mapping.target = \"Actual\"\n",
    "    column_mapping.prediction = \"Prediction\"\n",
    "    column_mapping.task = \"classification\"\n",
    "    column_mapping.numerical_features = [\n",
    "        c\n",
    "        for c in column_info[\"int_columns\"]\n",
    "        if c in set(preprocessor.feature_names_in_)\n",
    "    ]\n",
    "    column_mapping.categorical_features = [\n",
    "        c\n",
    "        for c in column_info[\"label_columns\"]\n",
    "        if c in set(preprocessor.feature_names_in_)\n",
    "    ]\n",
    "\n",
    "    report = Report(\n",
    "        metrics=[\n",
    "            ClassificationPreset(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    report.run(\n",
    "        reference_data=None,\n",
    "        current_data=dataset,\n",
    "        column_mapping=column_mapping,\n",
    "    )\n",
    "\n",
    "    Path(output_report).parent.mkdir(parents=True, exist_ok=True)\n",
    "    report.save_html(output_report)\n",
    "    html_content = open(output_report, \"r\").read()\n",
    "    metadata = {\n",
    "        \"outputs\": [\n",
    "            {\n",
    "                \"type\": \"web-app\",\n",
    "                \"storage\": \"inline\",\n",
    "                \"source\": html_content,\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    with open(mlpipeline_ui_metadata_path, \"w\") as f:\n",
    "        json.dump(metadata, f)\n",
    "\n",
    "    report_dict = report.as_dict()\n",
    "    print(report_dict)\n",
    "    classification_metrics = next(\n",
    "        filter(lambda m: m[\"metric\"]==\"ClassificationQualityMetric\",\n",
    "               report_dict[\"metrics\"])\n",
    "    )\n",
    "    \n",
    "    \n",
    "    metrics = {\n",
    "        \"metrics\": [\n",
    "            {\"name\": \"acc\", \n",
    "             \"numberValue\": classification_metrics[\"result\"][\"current\"][\"accuracy\"],\n",
    "             \"format\": \"RAW\"},\n",
    "            {\"name\": \"F1\", \n",
    "             \"numberValue\": classification_metrics[\"result\"][\"current\"][\"f1\"],\n",
    "             \"format\": \"RAW\"},\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    out_tuple = namedtuple(\"EvaluationOutput\", [\"mlpipeline_metrics\"])\n",
    "    return out_tuple(json.dumps(metrics))\n",
    "    \n",
    "eval_comp = kfp.components.create_component_from_func(\n",
    "    func=evaluate, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602c9610-a5f1-4b90-89ff-8e6e36e9c727",
   "metadata": {},
   "source": [
    "## Define a component to convert the model to ONNX\n",
    "\n",
    "We use an ONNX model for inference. ONNX is an open standard, and offers both a common way to represent TensorFlow and PyTorch models, and also obtain some performance improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "788bf836-5679-415a-96d9-1a766c82cd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_model_to_onnx(tf_model: InputPath(str),\n",
    "                          onnx_model: OutputPath(str)):\n",
    "    import tf2onnx\n",
    "    import tensorflow as tf\n",
    "    import onnx\n",
    "    \n",
    "    keras_model = tf.keras.models.load_model(tf_model)    \n",
    "    converted_model, _ = tf2onnx.convert.from_keras(keras_model)\n",
    "    onnx.save_model(converted_model, onnx_model)\n",
    "\n",
    "convert_model_to_onnx_comp = kfp.components.create_component_from_func(\n",
    "    func=convert_model_to_onnx, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00e258c-e14c-4d3a-b9e1-19ca63d1e6bd",
   "metadata": {},
   "source": [
    "## Define a component to train the explainer\n",
    "\n",
    "This example uses [Alibi Anchor explainers](https://docs.seldon.io/projects/alibi/en/latest/overview/high_level.html). The explainer is a blackbox explainer; it has no visibility to the model's interworkings. When asked to explain an example, it performs a search over similar examples while invoking the predictor. Using the predicted results, it learns which feature values are most important for the example's prediction. It then can return an answer that resembles a set of rules for why the prediction was made.\n",
    "\n",
    "In order to preform the search, the explainer needs to be fit with the distributions of the input variables. It also needs to be informed of the one-hot encoding scheme.\n",
    "\n",
    "This component fits the explainer model. The ONNX model and onnx-runtime is used directly as the predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5983463-5638-46e2-8ba8-d3a890557c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_explainer(train_df: InputPath(str),\n",
    "                    preprocessor: InputPath(str),\n",
    "                    onnx_model: InputPath(str),\n",
    "                    target_processing_config: InputPath(str),\n",
    "                    explainer_dll: OutputPath(str)):\n",
    "    from alibi.explainers.anchors import anchor_tabular\n",
    "    from alibi.utils import gen_category_map\n",
    "    import joblib\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    import re\n",
    "    import numpy as np\n",
    "    import dill\n",
    "    from functools import partial\n",
    "    from typing import Tuple, List, Dict\n",
    "    import json\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import onnxruntime as ort\n",
    "    import logging\n",
    "    \n",
    "    logging.basicConfig()\n",
    "    \n",
    "    def generate_category_map(preprocessor_pipeline: Pipeline) -> Tuple[List[str], Dict[int, List[str]]]:\n",
    "        features = []\n",
    "        seen_features = set()\n",
    "        category_map = dict()\n",
    "\n",
    "        for out_col_name in preprocessor_pipeline.get_feature_names_out():\n",
    "            parts = re.search(\"(.*)__([^_]+)(_([A-Za-z0-9_-]+))?\", out_col_name)\n",
    "            if parts:\n",
    "                if parts.group(2) not in seen_features:\n",
    "                    features.append(parts.group(2))\n",
    "                    seen_features.add(parts.group(2))\n",
    "\n",
    "                if parts.group(1) == \"categorical\" or parts.group(1).startswith(\"ohe_\"):\n",
    "                    levels = category_map.get(len(features) - 1, [])\n",
    "                    levels.append(parts.group(4))\n",
    "                    category_map[len(features) - 1] = levels\n",
    "            else:\n",
    "                raise ValueError(\"Could not parse column \" + out_col_name)\n",
    "\n",
    "        return features, category_map\n",
    "\n",
    "\n",
    "    with open(target_processing_config, \"r\") as f:\n",
    "        target_processing_config_dict = json.load(f)\n",
    "    column_info = json.loads(os.environ[\"COLUMNS\"])\n",
    "\n",
    "    inference_session = ort.InferenceSession(\n",
    "            onnx_model, providers=[\"CPUExecutionProvider\"]\n",
    "        )\n",
    "    threshold:float = float(target_processing_config_dict[\"threshold\"])\n",
    "    def predict(X: np.ndarray) -> np.ndarray:\n",
    "        scores= np.array(inference_session.run(\n",
    "            [], {\"input_1\": X}\n",
    "        )).flatten()\n",
    "        predictions = pd.Series(scores).apply(lambda p: 1 if p > threshold else 0).to_numpy()\n",
    "        return predictions\n",
    "\n",
    "    preprocessor_pipeline = joblib.load(preprocessor)\n",
    "    features, category_map = generate_category_map(preprocessor_pipeline)\n",
    "\n",
    "    dataset = pd.read_pickle(train_df)\n",
    "    X = preprocessor_pipeline.transform(dataset).astype(np.float32)\n",
    "\n",
    "    logging.info(f\"Training explainer: features={features} categories={category_map} X.shape = {X.shape}\")\n",
    "    explainer = anchor_tabular.AnchorTabular(\n",
    "        predictor=predict, feature_names=features, categorical_names=category_map, ohe=True, seed=42\n",
    "    )\n",
    "    explainer.fit(X, disc_prec=[10,25,33,50,66,75,90])\n",
    "\n",
    "    explainer.reset_predictor(None)   # Clear explainer predict_fn as its a lambda and will be reset when loaded\n",
    "    with open(explainer_dll, \"wb\") as f:\n",
    "        dill.dump(explainer, f)\n",
    "\n",
    "train_explainer_comp = kfp.components.create_component_from_func(\n",
    "    func=train_explainer, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628c3373-48b9-4c92-8cff-cf6fc12da10f",
   "metadata": {},
   "source": [
    "## Define a component to upload artifacts\n",
    "\n",
    "There are 3 components that are trained/fitted that need to be uploaded to S3 storage.\n",
    "* Data preprocessor\n",
    "* ONNX model\n",
    "* Explainer\n",
    "\n",
    "This will store the components in a tar archive and upload to S3 storage (MinIO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f9fa77a-32e0-4ea2-a93c-fa0197f10265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_artifacts(\n",
    "    onnx_model: InputPath(str),\n",
    "    preprocessor: InputPath(str),\n",
    "    explainer: InputPath(str),\n",
    "    archive_name: str,\n",
    "    minio_url: str = \"minio-service.kubeflow:9000\",\n",
    "    version: str = \"1\"\n",
    ") -> NamedTuple(\"UploadOutput\", [(\"s3_address\", str)]):\n",
    "    \"\"\"Uploads a model file to MinIO artifact store.\"\"\"\n",
    "\n",
    "    from collections import namedtuple\n",
    "    import logging\n",
    "    from minio import Minio\n",
    "    import sys\n",
    "    import tarfile\n",
    "    import os\n",
    "\n",
    "    logging.basicConfig(\n",
    "        stream=sys.stdout,\n",
    "        level=logging.INFO,\n",
    "        format=\"%(levelname)s %(asctime)s: %(message)s\",\n",
    "    )\n",
    "    logger = logging.getLogger()\n",
    "\n",
    "    ARCHIVE_FILE = f\"/tmp/{archive_name}\"\n",
    "    with tarfile.open(ARCHIVE_FILE, \"w\") as f:\n",
    "        f.add(onnx_model, arcname=\"model.onnx\")\n",
    "        f.add(preprocessor, arcname=\"preprocessor.joblib\")\n",
    "        f.add(explainer, arcname=\"explainer.dll\")\n",
    "\n",
    "    minio_client = Minio(\n",
    "            minio_url, \n",
    "            access_key=os.environ[\"MINIO_ID\"], \n",
    "            secret_key=os.environ[\"MINIO_PWD\"], secure=False\n",
    "        )\n",
    "\n",
    "    # Create export bucket if it does not yet exist\n",
    "    export_bucket=\"{{workflow.namespace}}\"\n",
    "    existing_bucket = next(filter(lambda bucket: bucket.name == export_bucket, minio_client.list_buckets()), None)\n",
    "\n",
    "    if not existing_bucket:\n",
    "        logger.info(f\"Creating bucket '{export_bucket}'...\")\n",
    "        minio_client.make_bucket(bucket_name=export_bucket)\n",
    "\n",
    "    path = f\"tar/{version}/{archive_name}\"\n",
    "    s3_address = f\"s3://{export_bucket}/tar\"\n",
    "\n",
    "    logger.info(f\"Saving onnx file to MinIO (s3 address: {s3_address})...\")\n",
    "    minio_client.fput_object(\n",
    "        bucket_name=export_bucket,  # bucket name in Minio\n",
    "        object_name=path,  # file name in bucket of Minio \n",
    "        file_path=ARCHIVE_FILE,  # file path / name in local system\n",
    "    )\n",
    "\n",
    "    logger.info(\"Finished.\")\n",
    "    out_tuple = namedtuple(\"UploadOutput\", [\"s3_address\"])\n",
    "    return out_tuple(s3_address)\n",
    "\n",
    "\n",
    "upload_artifacts_comp = kfp.components.create_component_from_func(\n",
    "    func=upload_artifacts, base_image=BASE_IMAGE, packages_to_install=[\"minio==7.1.13\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b63e1d-5ab1-4318-93f3-548346cd0a0b",
   "metadata": {},
   "source": [
    "## Define a component to deploy the inference service.\n",
    "\n",
    "The inference service has three custom built components:\n",
    "* Transformer\n",
    "* Predictor\n",
    "* Explainer\n",
    "\n",
    "The transformer and predictor use GRPC to communicate with the predictor, which improves the data transfer performance.\n",
    "\n",
    "The annotation `sidecar.istio.io/inject\": \"false\"` is added to the service because we don't want istio to enforce the same access restrictions that are used for Kubeflow services.\n",
    "\n",
    "The deployed service will pull and expand the model file from S3 in each pod. (The transformer, predictor, and explainer run as separate pods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b53b614-684b-48c0-bc30-b03b3b2a3bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_inference_service(name:str,\n",
    "                             target_processing_config: InputPath(str),\n",
    "                             model_archive_s3: str,\n",
    "                             transformer_image: str,\n",
    "                             predictor_image: str,\n",
    "                             explainer_image: str,\n",
    "                             predictor_max_replicas: int = 1,\n",
    "                             predictor_min_replicas: int = 1,\n",
    "                             predictor_concurrency_target: int = None,\n",
    "                             transformer_max_replicas: int = 1,\n",
    "                             transformer_min_replicas: int = 1,\n",
    "                             transformer_concurrency_target: int = None,\n",
    "                             explainer_max_replicas: int = 1,\n",
    "                             explainer_min_replicas: int = 1,\n",
    "                             explainer_concurrency_target: int = None\n",
    "                            ):\n",
    "    import kserve\n",
    "    from kubernetes import client, config\n",
    "    from kubernetes.client import (V1ServiceAccount, \n",
    "                                   V1Container, \n",
    "                                   V1EnvVar, \n",
    "                                   V1ObjectMeta, \n",
    "                                   V1ContainerPort, \n",
    "                                   V1ObjectReference,\n",
    "                                   V1ResourceRequirements\n",
    "                                  )\n",
    "    from kserve import KServeClient\n",
    "    from kserve import constants\n",
    "    from kserve import V1beta1PredictorSpec\n",
    "    from kserve import V1beta1ExplainerSpec\n",
    "    from kserve import V1beta1TransformerSpec\n",
    "    from kserve import V1beta1InferenceServiceSpec\n",
    "    from kserve import V1beta1InferenceService\n",
    "    import json\n",
    "    from http import HTTPStatus\n",
    "    import logging\n",
    "    import yaml\n",
    "    from time import sleep\n",
    "\n",
    "    with open(target_processing_config, \"r\") as f:\n",
    "        target_processing = json.load(f)\n",
    "\n",
    "    prediction_threshold = target_processing[\"threshold\"]\n",
    "    target_names = json.dumps(\n",
    "        [target_processing[\"target_names\"].get(str(idx),\"?\")\n",
    "         for idx in range(len(target_processing[\"target_names\"]))]\n",
    "    )\n",
    "\n",
    "    config.load_incluster_config()\n",
    "    \n",
    "    # Setup the service account for the inference service to run under\n",
    "    # The service account must have access to the secret with the\n",
    "    # MinIO credentials\n",
    "    SERVICE_ACCOUNT = \"credit-risk-inference-sa\"\n",
    "\n",
    "    sa = V1ServiceAccount(\n",
    "        api_version=\"v1\",\n",
    "        kind=\"ServiceAccount\",\n",
    "        metadata=V1ObjectMeta(name=SERVICE_ACCOUNT, \n",
    "                              namespace=\"{{workflow.namespace}}\"),\n",
    "        secrets=[V1ObjectReference(name=\"minio-credentials\")]\n",
    "    )\n",
    "    corev1 = client.CoreV1Api()\n",
    "    try:\n",
    "        corev1.create_namespaced_service_account(namespace=\"{{workflow.namespace}}\",\n",
    "                                                 body=sa)\n",
    "    except client.exceptions.ApiException as e:\n",
    "        if e.status==HTTPStatus.CONFLICT:\n",
    "            corev1.patch_namespaced_service_account(name=SERVICE_ACCOUNT,\n",
    "                                                    namespace=\"{{workflow.namespace}}\",\n",
    "                                                    body=sa)\n",
    "        else:\n",
    "            raise\n",
    "    \n",
    "    ### Build the InferenceService spec components\n",
    "    \n",
    "    # Predictor component\n",
    "    # We use a custom predictor in this example, and so it is\n",
    "    # necessary to be more complete with the spec than we would\n",
    "    # need for 'built-in' predictors such as Triton.\n",
    "    predictor_spec = V1beta1PredictorSpec(\n",
    "        max_replicas=predictor_max_replicas,\n",
    "        min_replicas=predictor_min_replicas,\n",
    "        scale_target=predictor_concurrency_target,\n",
    "        scale_metric=\"concurrency\",\n",
    "        containers=[\n",
    "            V1Container(\n",
    "                name=\"kserve-container\",\n",
    "                image=predictor_image,\n",
    "                args=[\"--grpc_port=8081\", f\"--model_name={name}\"],\n",
    "                ports=[V1ContainerPort(\n",
    "                    container_port=8081,\n",
    "                    name=\"h2c\",\n",
    "                    protocol=\"TCP\"\n",
    "                )],\n",
    "                resources=V1ResourceRequirements(\n",
    "                    limits={\"memory\": \"10Gi\"},\n",
    "                    requests={\"memory\": \"2Gi\"},\n",
    "                ),\n",
    "                env=[\n",
    "                 V1EnvVar(\n",
    "                     name=\"STORAGE_URI\", value=model_archive_s3\n",
    "                 ),\n",
    "                 V1EnvVar(\n",
    "                     name=\"THRESHOLD\",\n",
    "                     value=str(prediction_threshold)\n",
    "                 )\n",
    "                ],\n",
    "            )\n",
    "        ],\n",
    "        service_account_name=SERVICE_ACCOUNT\n",
    "    )\n",
    "\n",
    "    # Transformer spec\n",
    "    # Uses a custom image, explicitly specifies that GRPC should be used\n",
    "    # to communicate with the predictor\n",
    "    transformer_spec = V1beta1TransformerSpec(\n",
    "        max_replicas=transformer_max_replicas,\n",
    "        min_replicas=transformer_min_replicas,\n",
    "        scale_target=transformer_concurrency_target,\n",
    "        scale_metric=\"concurrency\",\n",
    "        containers=[\n",
    "            V1Container(\n",
    "                name=\"kserve-container\",\n",
    "                image=transformer_image,\n",
    "                args=[\"--protocol=grpc-v2\", f\"--model_name={name}\"],\n",
    "                resources=V1ResourceRequirements(\n",
    "                    limits={\"memory\": \"10Gi\"},\n",
    "                    requests={\"memory\": \"2Gi\"},\n",
    "                ),\n",
    "                env=[\n",
    "                 V1EnvVar(\n",
    "                     name=\"STORAGE_URI\", value=model_archive_s3\n",
    "                 ),\n",
    "                V1EnvVar(\n",
    "                    name=\"TARGET_NAMES\", value=target_names\n",
    "                )\n",
    "                ],\n",
    "            )\n",
    "        ],\n",
    "        service_account_name=SERVICE_ACCOUNT\n",
    "    )\n",
    "\n",
    "    # Explainer spec\n",
    "    # A custom image is used\n",
    "    # GRPC is explicitly specified as the protocol to communicate with the\n",
    "    # predictor.\n",
    "    explainer_spec=V1beta1ExplainerSpec(\n",
    "        max_replicas=explainer_max_replicas,\n",
    "        min_replicas=explainer_min_replicas,\n",
    "        scale_target=explainer_concurrency_target,\n",
    "        scale_metric=\"concurrency\",\n",
    "        containers=[\n",
    "            V1Container(\n",
    "                name=\"kserve-container\",\n",
    "                image=explainer_image,\n",
    "                args=[\"--protocol=grpc-v2\", f\"--model_name={name}\"],\n",
    "                resources=V1ResourceRequirements(\n",
    "                    limits={\"memory\": \"10Gi\"},\n",
    "                    requests={\"memory\": \"4Gi\"},\n",
    "                ),\n",
    "                env=[\n",
    "                 V1EnvVar(\n",
    "                     name=\"STORAGE_URI\", value=model_archive_s3\n",
    "                 ),\n",
    "                 V1EnvVar(\n",
    "                     name=\"EXPLAIN_MIN_SAMPLES_START\", value=\"15000\"\n",
    "                 )\n",
    "                ],\n",
    "            )\n",
    "        ],\n",
    "        service_account_name=SERVICE_ACCOUNT\n",
    "    )\n",
    "\n",
    "\n",
    "    # Build the inference service\n",
    "    # enable-prometheus-scraping causes the service to export metrics.\n",
    "    inference_service=V1beta1InferenceService(\n",
    "        api_version=constants.KSERVE_V1BETA1,\n",
    "        kind=constants.KSERVE_KIND,\n",
    "        metadata=V1ObjectMeta(name=name, \n",
    "                              namespace=\"{{workflow.namespace}}\",\n",
    "                              annotations={\"sidecar.istio.io/inject\": \"false\",\n",
    "                                           \"serving.kserve.io/enable-prometheus-scraping\" : \"true\"}),\n",
    "        spec=V1beta1InferenceServiceSpec(predictor=predictor_spec,\n",
    "                                         transformer=transformer_spec,\n",
    "                                         explainer=explainer_spec)\n",
    "    )\n",
    "    \n",
    "    # Dump the created inference service spec to the log for easier debug\n",
    "    logging.info(\n",
    "        yaml.dump(\n",
    "            client.ApiClient().sanitize_for_serialization(inference_service)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Create the service, handling a conflict error for existing service\n",
    "    api_instance = client.CustomObjectsApi()\n",
    "    while True:\n",
    "        try:\n",
    "            api_instance.create_namespaced_custom_object(\n",
    "                    group=constants.KSERVE_GROUP,\n",
    "                    version=inference_service.api_version.split(\"/\")[1],\n",
    "                    namespace=\"{{workflow.namespace}}\",\n",
    "                    plural=constants.KSERVE_PLURAL,\n",
    "                    body=inference_service)\n",
    "            break\n",
    "        except client.exceptions.ApiException as api_exception:\n",
    "            if api_exception.status==HTTPStatus.CONFLICT:\n",
    "                try:\n",
    "                    api_instance.delete_namespaced_custom_object(\n",
    "                        group=constants.KSERVE_GROUP,\n",
    "                        version=inference_service.api_version.split(\"/\")[1],\n",
    "                        namespace=\"{{workflow.namespace}}\",\n",
    "                        plural=constants.KSERVE_PLURAL,\n",
    "                        name=name)\n",
    "                    sleep(15)\n",
    "                except client.exceptions.ApiException as api_exception2:\n",
    "                    if api_exception2.status in {HTTPStatus.NOT_FOUND, HTTPStatus.GONE}:\n",
    "                        pass\n",
    "                    else:\n",
    "                        raise\n",
    "\n",
    "            else:\n",
    "                raise\n",
    "            \n",
    "    # Wait for inference service to become ready\n",
    "    kclient = KServeClient()\n",
    "    kclient.wait_isvc_ready(name=name, namespace=\"{{workflow.namespace}}\")\n",
    "    \n",
    "    if not kclient.is_isvc_ready(name=name, namespace=\"{{workflow.namespace}}\"):\n",
    "        raise RuntimeError(f\"The inference service {name} is not ready!\")\n",
    "\n",
    "deploy_inference_service_comp = kfp.components.create_component_from_func(\n",
    "    func=deploy_inference_service, base_image=KSERVE_IMAGE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ba2c33-f321-42b7-a060-ed495a83f161",
   "metadata": {},
   "source": [
    "## Define the pipeline\n",
    "\n",
    "The load_training_data_task and load_test_data_task needs to use the correct function (DB2 or PostgreSQL) to load the training data. Connection credentials are passed via the environment by referencing secrets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f544cb1f-1200-4355-a42e-9c09003c5cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_NAME = \"Build-Credit-Risk-Model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25df93f6-5134-4178-bcef-3458ff25ca3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubernetes.client import ( V1PersistentVolumeClaimVolumeSource, V1Volume, V1VolumeMount)\n",
    "@dsl.pipeline(\n",
    "    name=PIPELINE_NAME,\n",
    "    description=\"An example pipeline that builds and deploys a credit risk model\",\n",
    ")\n",
    "def credit_model_pipeline():\n",
    "    def env_var_from_secret(env_var_name: str, secret_name: str, secret_key: str) -> V1EnvVar:\n",
    "        return V1EnvVar(name=env_var_name,\n",
    "                                     value_from=V1EnvVarSource(\n",
    "                                         secret_key_ref=V1SecretKeySelector(\n",
    "                                             name=secret_name,\n",
    "                                             key=secret_key\n",
    "                                         )\n",
    "                                     )\n",
    "                                    )\n",
    "    \n",
    "    def add_db2_connection_secrets(pipeline_task) -> None:\n",
    "        pipeline_task.container.add_env_variable(env_var_from_secret(\"db2_host\", \"db2-credentials\", \"host\"))\n",
    "        pipeline_task.container.add_env_variable(env_var_from_secret(\"db2_user\", \"db2-credentials\", \"username\"))\n",
    "        pipeline_task.container.add_env_variable(env_var_from_secret(\"db2_pwd\", \"db2-credentials\", \"password\"))\n",
    "        pipeline_task.container.add_env_variable(env_var_from_secret(\"db2_port\", \"db2-credentials\", \"port\"))\n",
    "\n",
    "    \n",
    "    def add_pg_connection_secrets(pipeline_task) -> None:\n",
    "        pipeline_task.container.add_env_variable(V1EnvVar(name=\"PG_HOST\", value=\"postgresql.{{workflow.namespace}}.svc\"))\n",
    "        pipeline_task.container.add_env_variable(env_var_from_secret(\"PG_DB_NAME\", \"postgresql\", \"database-name\"))\n",
    "        pipeline_task.container.add_env_variable(env_var_from_secret(\"PG_USER\", \"postgresql\", \"database-user\"))\n",
    "        pipeline_task.container.add_env_variable(env_var_from_secret(\"PG_PWD\", \"postgresql\", \"database-password\"))\n",
    "        pipeline_task.container.add_env_variable(V1EnvVar(name=\"PG_PORT\", value=\"5432\"))\n",
    "\n",
    "    feature_columns = [\n",
    "            \"CheckingStatus\",\n",
    "            \"LoanDuration\",\n",
    "            \"CreditHistory\",\n",
    "            \"LoanPurpose\",\n",
    "            \"LoanAmount\",\n",
    "            \"ExistingSavings\",\n",
    "            \"EmploymentDuration\",\n",
    "            \"InstallmentPercent\",\n",
    "            \"Sex\",\n",
    "            \"OthersOnLoan\",\n",
    "            \"CurrentResidenceDuration\",\n",
    "            \"OwnsProperty\",\n",
    "            \"Age\",\n",
    "            \"InstallmentPlans\",\n",
    "            \"Housing\",\n",
    "            \"ExistingCreditsCount\",\n",
    "            \"Job\",\n",
    "            \"Dependents\",\n",
    "            \"Telephone\",\n",
    "            \"ForeignWorker\",\n",
    "        ]\n",
    "    \n",
    "    load_training_data_task = load_df_from_postgresql_comp(table_name=\"TRAIN\")\n",
    "    load_training_data_task.set_display_name(\"Load_Training_Data\")\n",
    "    load_training_data_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    add_pg_connection_secrets(load_training_data_task)\n",
    "\n",
    "    data_quality_report_comp(df=load_training_data_task.outputs[\"data_frame_pkl\"],\n",
    "                             features=feature_columns)\n",
    "    \n",
    "    load_test_data_task = load_df_from_postgresql_comp(table_name=\"TEST\")\n",
    "    load_test_data_task.set_display_name(\"Load_Test_Data\")\n",
    "    load_test_data_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    add_pg_connection_secrets(load_test_data_task)\n",
    "\n",
    "    fit_preprocessor_task = fit_preprocessor_comp(\n",
    "        training_df=load_training_data_task.outputs[\"data_frame_pkl\"],\n",
    "        features=feature_columns\n",
    "    )\n",
    "\n",
    "    create_tensorboard_volume = dsl.VolumeOp(\n",
    "        name=f\"Create PVC for tensorboard\",\n",
    "        resource_name=\"tensorboard\",\n",
    "        modes=dsl.VOLUME_MODE_RWM,\n",
    "        size=\"4G\",\n",
    "        set_owner_reference=True,\n",
    "    )\n",
    "    create_tensorboard_volume.add_pod_annotation(\n",
    "            name=\"pipelines.kubeflow.org/max_cache_staleness\", value=\"P0D\"\n",
    "        )\n",
    "        \n",
    "    configure_tensorboard_task = configure_tensorboard_comp(\n",
    "        pipeline_name=PIPELINE_NAME,\n",
    "        pvc_name=create_tensorboard_volume.volume.persistent_volume_claim.claim_name\n",
    "    )\n",
    "    \n",
    "    train_model_task = train_comp(\n",
    "        training_df=load_training_data_task.outputs[\"data_frame_pkl\"],\n",
    "        preprocessor=fit_preprocessor_task.outputs[\"preprocessor_pkl\"],\n",
    "        target=\"Risk\",\n",
    "        tensorboard_dir=\"/tensorboard\",\n",
    "    )\n",
    "    train_model_task.after(configure_tensorboard_task)\n",
    "    train_model_task.add_pvolumes({\"/tensorboard\": create_tensorboard_volume.volume})\n",
    "\n",
    "    \n",
    "    convert_model_to_onnx_task = convert_model_to_onnx_comp(tf_model=train_model_task.outputs[\"model\"])\n",
    "    \n",
    "    train_explainer_task = train_explainer_comp(train_df=load_training_data_task.outputs[\"data_frame_pkl\"],\n",
    "                                                preprocessor=fit_preprocessor_task.outputs[\"preprocessor_pkl\"],\n",
    "                                                onnx_model=convert_model_to_onnx_task.outputs[\"onnx_model\"],\n",
    "                                                target_processing_config=train_model_task.outputs[\"target_processing_config\"])\n",
    "    \n",
    "    evaluate_model_task = eval_comp(\n",
    "        load_test_data_task.outputs[\"data_frame_pkl\"],\n",
    "        preprocessor=fit_preprocessor_task.outputs[\"preprocessor_pkl\"],\n",
    "        onnx_model=convert_model_to_onnx_task.outputs[\"onnx_model\"],\n",
    "        target_processing_config=train_model_task.outputs[\"target_processing_config\"],\n",
    "        target=\"Risk\"\n",
    "    )\n",
    "    \n",
    "    upload_artifacts_task = upload_artifacts_comp(\n",
    "        onnx_model=convert_model_to_onnx_task.outputs[\"onnx_model\"],\n",
    "        preprocessor=fit_preprocessor_task.outputs[\"preprocessor_pkl\"],\n",
    "        explainer=train_explainer_task.outputs[\"explainer_dll\"],\n",
    "        archive_name=\"credit-risk.tar\"\n",
    "    )\n",
    "    upload_artifacts_task.container.add_env_variable(env_var_from_secret(\"MINIO_ID\", \"mlpipeline-minio-artifact\", \"accesskey\"))\n",
    "    upload_artifacts_task.container.add_env_variable(env_var_from_secret(\"MINIO_PWD\", \"mlpipeline-minio-artifact\", \"secretkey\"))\n",
    "    upload_artifacts_task.after(evaluate_model_task)\n",
    "    \n",
    "    deploy_inference_service_task=deploy_inference_service_comp(name=\"credit-risk\",\n",
    "                                                                target_processing_config=train_model_task.outputs[\"target_processing_config\"],\n",
    "                                                                model_archive_s3=upload_artifacts_task.output,\n",
    "                                                                transformer_image=TRANSFORMER_IMAGE,\n",
    "                                                                predictor_image=PREDICTOR_IMAGE,\n",
    "                                                                explainer_image=EXPLAINER_IMAGE,\n",
    "                                                                predictor_max_replicas=4,\n",
    "                                                                predictor_concurrency_target=1,\n",
    "                                                                transformer_max_replicas=4,\n",
    "                                                                transformer_concurrency_target=1\n",
    "                                                               )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627766ee-4aa7-48a5-a90f-b7d8b5740dff",
   "metadata": {},
   "source": [
    "## Compile the pipeline\n",
    "\n",
    "Compiling the pipeline creates a yaml file for the pipeline. We can then create a pipeline using the YAML. A run for the pipeline can be started using the UI, without the need to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f1d0cfa-be30-42bc-a1b6-eebc0e98a226",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_conf = kfp.dsl.PipelineConf()\n",
    "\n",
    "def provide_column_info_transformer(op: dsl.ContainerOp):\n",
    "    \n",
    "    if isinstance(op, dsl.ContainerOp):\n",
    "        op.container.add_env_variable(\n",
    "            V1EnvVar(name=\"COLUMNS\",\n",
    "                    value_from=V1EnvVarSource(\n",
    "                                         config_map_key_ref=V1ConfigMapKeySelector(\n",
    "                                             name=\"credit-risk-columns\",\n",
    "                                             key=\"columns\"\n",
    "                                         )\n",
    "                                     )\n",
    "                    )\n",
    "        )\n",
    "\n",
    "pipeline_conf.add_op_transformer(provide_column_info_transformer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07a2f50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "       \n",
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=credit_model_pipeline,\n",
    "    package_path=f\"{PIPELINE_NAME}.yaml\",\n",
    "    pipeline_conf=pipeline_conf,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6ce2b5-a642-44ee-88a5-4cfc8dbbcccd",
   "metadata": {},
   "source": [
    "## Upload the pipeline\n",
    "\n",
    "The compiled pipeline is uloaded here. We could also do this manually using the UI.\n",
    "\n",
    "We can only have one pipeline with a specific name; in case we ran this script multiple times we will delete the pipeline first if it exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76074abf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def delete_pipeline(pipeline_name: str):\n",
    "    \"\"\"Delete's a pipeline with the specified name\"\"\"\n",
    "\n",
    "    client = kfp.Client()\n",
    "    existing_pipelines = client.list_pipelines(page_size=999).pipelines\n",
    "    matches = (\n",
    "        [ep.id for ep in existing_pipelines if ep.name == pipeline_name]\n",
    "        if existing_pipelines\n",
    "        else []\n",
    "    )\n",
    "    for id in matches:\n",
    "        client.delete_pipeline(id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9694d998-e55a-4ef8-b950-915463ebbb21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=/pipeline/#/pipelines/details/b6c36ffb-84bb-4839-b3b1-831513ae6ce0>Pipeline details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pipeline names need to be unique, so before we upload,\n",
    "# check for and delete any pipeline with the same name\n",
    "delete_pipeline(PIPELINE_NAME)\n",
    "\n",
    "client = kfp.Client()\n",
    "uploaded_pipeline = client.upload_pipeline(f\"{PIPELINE_NAME}.yaml\", PIPELINE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81b8c5b-990c-44d1-973e-a5a0590522d7",
   "metadata": {},
   "source": [
    "## Create a pipeline run\n",
    "\n",
    "This creates a run of the pipeline. As with uploading, we could do this manually using the UI.\n",
    "\n",
    "Runnning a pipeline requires a KFP experiment. This code will create the experiment if it does not already exist.\n",
    "\n",
    "Grouping runs into an experiment is advantageous, since it allows us to compare metrics between pipeline runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c6a023f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_id(experiment_name: str) -> str:\n",
    "    \"\"\"Returns the id for the experiment, creating the experiment if needed\"\"\"\n",
    "    client = kfp.Client()\n",
    "    existing_experiments = client.list_experiments(page_size=999).experiments\n",
    "    matches = (\n",
    "        [ex.id for ex in existing_experiments if ex.name == experiment_name]\n",
    "        if existing_experiments\n",
    "        else []\n",
    "    )\n",
    "\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "\n",
    "    exp = client.create_experiment(experiment_name)\n",
    "    return exp.id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3139f4d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/833c603a-a253-4e30-966f-3b04caf615b3\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = client.run_pipeline(\n",
    "    experiment_id=get_experiment_id(\"credit-risk\"),\n",
    "    job_name=\"credit-risk\",\n",
    "    pipeline_id=uploaded_pipeline.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032c370d-fd3a-43be-82fd-5a8a1b943c54",
   "metadata": {},
   "source": [
    "## Wait for pipeline completion\n",
    "\n",
    "This shows how to wait for the pipeline to complete and check for success. The metrics from the evaluate component are also included in the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6dce21c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'Succeeded',\n",
       " 'error': None,\n",
       " 'time': '0:06:52',\n",
       " 'metrics': [{'format': 'RAW',\n",
       "   'name': 'acc',\n",
       "   'node_id': 'build-credit-risk-model-v5d2g-3574078477',\n",
       "   'number_value': 0.786},\n",
       "  {'format': 'RAW',\n",
       "   'name': 'F1',\n",
       "   'node_id': 'build-credit-risk-model-v5d2g-3574078477',\n",
       "   'number_value': 0.6786786786786787}]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TWENTY_MIN = 20 * 60\n",
    "result = client.wait_for_run_completion(run.id, timeout=TWENTY_MIN)\n",
    "{\n",
    "    \"status\": result.run.status,\n",
    "    \"error\": result.run.error,\n",
    "    \"time\": str(result.run.finished_at - result.run.created_at),\n",
    "    \"metrics\": result.run.metrics,\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
