{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dca6eebb-74b5-4d39-b3dd-e47656bbd2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp.components import InputPath, OutputPath\n",
    "from kfp import dsl\n",
    "from typing import List, Tuple\n",
    "from kfp.dsl import ContainerOp\n",
    "from kubernetes.client.models import V1EnvVar,V1EnvVarSource, V1SecretKeySelector,V1ConfigMapKeySelector\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69a64e6a-4b89-4a45-a950-9ef464ff4259",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_IMAGE = \"quay.io/ntlawrence/demo-workflow@sha256:e0b071e361a147d1cc957b96a19ae6144d792ff994ac8daf0ba887a5bd3652f5\"\n",
    "KSERVE_IMAGE = \"quay.io/ntlawrence/demo-kserve@sha256:6803755ecffe42bfb38a703e1d4e453c088bae64096fc8857baa663b526d8978\"\n",
    "TRANSFORMER_IMAGE = \"quay.io/ntlawrence/demo-transformer@sha256:54b5c5a6e379044c474deab4edcf92ec3e2fc22a588aaa7547184bf03199832b\"\n",
    "PREDICTOR_IMAGE = \"quay.io/ntlawrence/demo-predictor@sha256:fca5f3a1de13005ea5033bc3dd3ba5319ae1d317f3bc02ee8a4f12f35ed7318f\"\n",
    "EXPLAINER_IMAGE = \"quay.io/ntlawrence/demo-explainer:0.1.0@sha256:57f4d1e94d3aea40563a801323d627dfecbbcb2f891477a510b81d805232b296\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab52be0e-c807-4d59-bc67-cccbfd695927",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88901cfa-91b5-41a3-9fd6-6fba280f587d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_tensorboard(\n",
    "    mlpipeline_ui_metadata_path: OutputPath(),\n",
    "    pvc_name: str,\n",
    "    pvc_path: str = \"\",\n",
    "    tensorboard_name: str = \"\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Monitors a training job based on Tensorboard logs. \n",
    "    Logs are expected to be written to the specified subpath of the pvc\n",
    "    \"\"\"\n",
    "    from collections import namedtuple\n",
    "    import json\n",
    "    from kubernetes import client, config, watch\n",
    "    import logging\n",
    "    import sys\n",
    "    import os\n",
    "    import yaml\n",
    "    import textwrap\n",
    "    import json\n",
    "    import http\n",
    "\n",
    "    logging.basicConfig(\n",
    "        stream=sys.stdout,\n",
    "        level=logging.INFO,\n",
    "        format=\"%(levelname)s %(asctime)s: %(message)s\",\n",
    "    )\n",
    "    logger = logging.getLogger()\n",
    "\n",
    "    if not tensorboard_name:\n",
    "        tensorboard_name=\"{{workflow.name}}\"\n",
    "        \n",
    "    namespace=\"{{workflow.namespace}}\"\n",
    "\n",
    "    config.load_incluster_config()\n",
    "    api_client = client.ApiClient()\n",
    "    apps_api = client.AppsV1Api(api_client)\n",
    "    custom_object_api = client.CustomObjectsApi(api_client)\n",
    "\n",
    "    # Delete possible existing tensorboard\n",
    "    try:\n",
    "        custom_object_api.delete_namespaced_custom_object(\n",
    "            group=\"tensorboard.kubeflow.org\",\n",
    "            version=\"v1alpha1\",\n",
    "            plural=\"tensorboards\",\n",
    "            namespace=namespace,\n",
    "            name=tensorboard_name,\n",
    "            body=client.V1DeleteOptions()\n",
    "        )\n",
    "    except client.exceptions.ApiException as e:\n",
    "        if e.status != http.HTTPStatus.NOT_FOUND:\n",
    "            raise\n",
    "    \n",
    "    tensorboard_spec = textwrap.dedent(f'''\\\n",
    "            apiVersion: tensorboard.kubeflow.org/v1alpha1\n",
    "            kind: Tensorboard\n",
    "            metadata:\n",
    "              name: \"{tensorboard_name}\"\n",
    "              namespace: \"{namespace}\"\n",
    "              ownerReferences:\n",
    "                - apiVersion: v1\n",
    "                  kind: Workflow\n",
    "                  name: \"{{workflow.name}}\"\n",
    "                  uid: \"{{workflow.uid}}\"\n",
    "            spec:\n",
    "              logspath: \"pvc://{pvc_name}/{pvc_path}\"\n",
    "            '''\n",
    "    )\n",
    "    \n",
    "    logger.info(tensorboard_spec)\n",
    "\n",
    "    custom_object_api.create_namespaced_custom_object(\n",
    "        group=\"tensorboard.kubeflow.org\",\n",
    "        version=\"v1alpha1\",\n",
    "        plural=\"tensorboards\",\n",
    "        namespace=namespace,\n",
    "        body=yaml.safe_load(tensorboard_spec),\n",
    "        pretty=True)\n",
    "\n",
    "    tensorboard_watch = watch.Watch()\n",
    "    try:\n",
    "        for tensorboard_event in tensorboard_watch.stream(\n",
    "            custom_object_api.list_namespaced_custom_object,\n",
    "            group=\"tensorboard.kubeflow.org\",\n",
    "            version=\"v1alpha1\",\n",
    "            plural=\"tensorboards\",\n",
    "            namespace=namespace,\n",
    "            field_selector=f\"metadata.name={tensorboard_name}\",\n",
    "            timeout_seconds=0,\n",
    "        ):\n",
    "\n",
    "            logger.info(f\"tensorboard_event: {json.dumps(tensorboard_event, indent=2)}\")\n",
    "\n",
    "            if tensorboard_event[\"type\"]==\"DELETED\":\n",
    "                raise RuntimeError(\"The tensorboard was deleted!\")\n",
    "\n",
    "            tensorboard = tensorboard_event[\"object\"]\n",
    "\n",
    "            if \"status\" not in tensorboard:\n",
    "                continue\n",
    "\n",
    "            deployment_state = \"Progressing\"\n",
    "            if \"conditions\" in tensorboard[\"status\"]:\n",
    "                deployment_state = tensorboard[\"status\"][\"conditions\"][-1][\n",
    "                    \"deploymentState\"\n",
    "                ]\n",
    "\n",
    "            if deployment_state == \"Progressing\":\n",
    "                logger.info(\"Tensorboard deployment is progressing...\")\n",
    "            elif deployment_state == \"Available\":\n",
    "                logger.info(\"Tensorboard deployment is Available.\")\n",
    "                break\n",
    "            elif deployment_state == \"ReplicaFailure\":\n",
    "                raise RuntimeError(\"Tensorboard deployment failed with a ReplicaFailure!\")\n",
    "            else:\n",
    "                raise RuntimeError(f\"Unknown deployment state: {deployment_state}\")\n",
    "    finally:\n",
    "        tensorboard_watch.stop()\n",
    "\n",
    "    button_style =(\n",
    "        \"align-items: center; \"\n",
    "        \"appearance: none; \"\n",
    "        \"background-color: rgb(26, 115, 232); \"\n",
    "        \"border: 0px none rgb(255, 255, 255); \"\n",
    "        \"border-radius: 3px; \"\n",
    "        \"box-sizing: border-box; \"\n",
    "        \"color: rgb(255, 255, 255); \"\n",
    "        \"cursor: pointer; \"\n",
    "        \"display: inline-flex; \" \n",
    "        \"font-family: 'Google Sans', 'Helvetica Neue', sans-serif; \"\n",
    "        \"font-size: 14px; \"\n",
    "        \"font-stretch: 100%; \"\n",
    "        \"font-style: normal; font-weight: 700; \"\n",
    "        \"justify-content: center; \"\n",
    "        \"letter-spacing: normal; \"\n",
    "        \"line-height: 24.5px; \"\n",
    "        \"margin: 0px 10px 2px 0px; \"\n",
    "        \"min-height: 25px; \"\n",
    "        \"min-width: 64px; \"\n",
    "        \"padding: 2px 6px 2px 6px; \"\n",
    "        \"position: relative; \"\n",
    "        \"tab-size: 4; \"\n",
    "        \"text-align: center; \"\n",
    "        \"text-indent: 0px; \"\n",
    "        \"text-rendering: auto; \"\n",
    "        \"text-shadow: none; \"\n",
    "        \"text-size-adjust: 100%; \"\n",
    "        \"text-transform: none; \"\n",
    "        \"user-select: none; \"\n",
    "        \"vertical-align: middle; \"\n",
    "        \"word-spacing: 0px; \"\n",
    "        \"writing-mode: horizontal-tb;\"\n",
    "    )\n",
    "\n",
    "    # See: https://github.com/kubeflow/kubeflow/blob/master/components/crud-web-apps/tensorboards/frontend/src/app/pages/index/index.component.ts\n",
    "    # window.open(`/tensorboard/${tensorboard.namespace}/${tensorboard.name}/`);\n",
    "    ui_address = f\"/tensorboard/{namespace}/{tensorboard_name}/#scalars\"\n",
    "\n",
    "    markdown = textwrap.dedent(\n",
    "        f'''\\\n",
    "        # Tensorboard\n",
    "        - <a href=\"{ui_address}\" style=\"{button_style}\" target=\"_blank\">Connect</a>\n",
    "        - <a href=\"/_/tensorboards/\" style=\"{button_style}\" target=\"_blank\">Manage all</a>\n",
    "        '''\n",
    "    )\n",
    "\n",
    "    markdown_output = {\n",
    "        \"type\": \"markdown\",\n",
    "        \"storage\": \"inline\",\n",
    "        \"source\": markdown,\n",
    "    }\n",
    "\n",
    "    ui_metadata = {\"outputs\": [markdown_output]}\n",
    "    with open(mlpipeline_ui_metadata_path, \"w\") as metadata_file:\n",
    "        json.dump(ui_metadata, metadata_file)\n",
    "   \n",
    "    logging.info(\"Finished.\")\n",
    "\n",
    "\n",
    "configure_tensorboard_comp = kfp.components.create_component_from_func(\n",
    "    func=configure_tensorboard, base_image=BASE_IMAGE, packages_to_install=[\"kubernetes\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdbbef71-a79f-4472-b9d6-3969b2c3bb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df_from_db2(table_name: str,\n",
    "                     data_frame_pkl: OutputPath(str)):\n",
    "    import warnings\n",
    "    import ibm_db\n",
    "    import ibm_db_dbi\n",
    "    import os\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    from typing import Dict, Any\n",
    "    \n",
    "    def assign_categories_to_df(df: pd.DataFrame, column_info: Dict[str, any]) -> None:\n",
    "        for col_name, levels in column_info[\"label_columns\"].items():\n",
    "            if col_name in df.columns:\n",
    "                ctype = pd.CategoricalDtype(categories=levels, ordered=False)\n",
    "                df[col_name] = df[col_name].astype(ctype)\n",
    "\n",
    "    def df_from_sql(\n",
    "        name: str,\n",
    "        conn: ibm_db.IBM_DBConnection,\n",
    "        column_info: Dict[str, Any],\n",
    "    ) -> pd.DataFrame:\n",
    "        sql_safe_name = name.replace('\"', \"\")\n",
    "\n",
    "        rStmtColsSql = \",\".join([f'\"{col}\"' for col in column_info[\"columns\"]])\n",
    "        rSql = f'SELECT {rStmtColsSql} FROM \"{sql_safe_name}\"'\n",
    "\n",
    "        read_conn = ibm_db_dbi.Connection(conn)\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", message=\"pandas only support SQLAlchemy\")\n",
    "            df = pd.read_sql(rSql, read_conn)\n",
    "\n",
    "        assign_categories_to_df(df, column_info)\n",
    "        return df\n",
    "    \n",
    "    conn_str = (\n",
    "    \"DRIVER={IBM DB2 ODBC DRIVER};\"\n",
    "    f\"DATABASE=BLUDB;HOSTNAME={os.environ['db2_host']};PORT={os.environ['db2_port']};PROTOCOL=TCPIP;UID={os.environ['db2_user']};Pwd={os.environ['db2_pwd']};SECURITY=SSL;\"\n",
    "    )\n",
    "        \n",
    "    conn = ibm_db.connect(conn_str, \"\", \"\")\n",
    "\n",
    "    column_info = json.loads(os.environ[\"COLUMNS\"])\n",
    "    df = df_from_sql(table_name, conn, column_info)\n",
    "    df.to_pickle(data_frame_pkl)\n",
    "\n",
    "\n",
    "load_df_from_db2_comp = kfp.components.create_component_from_func(\n",
    "    func=load_df_from_db2, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ccacd54-7107-4165-9c06-497bd8612b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df_from_postgresql(table_name: str,\n",
    "                           data_frame_pkl: OutputPath(str)):\n",
    "    import os\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    from typing import Dict, Any\n",
    "    import psycopg\n",
    "    from psycopg import sql\n",
    "    import yaml\n",
    "    \n",
    "    def get_pg_conn() -> psycopg.Connection:\n",
    "        host, dbname, username, password, port = (\n",
    "            os.environ.get('PG_HOST'),\n",
    "            os.environ.get('PG_DB_NAME'),\n",
    "            os.environ.get('PG_USER'),\n",
    "            os.environ.get('PG_PWD'),\n",
    "            int(os.environ.get('PG_PORT'))\n",
    "        )\n",
    "\n",
    "        conn_str = f\"postgresql://{username}:{password}@{host}:{port}/{dbname}?connect_timeout=10&application_name=mlpipeline\"\n",
    "        print(conn_str)\n",
    "        conn = psycopg.connect(conn_str)\n",
    "\n",
    "        return conn\n",
    "    \n",
    "    def assign_categories_to_df(df: pd.DataFrame, column_info: Dict[str, any]) -> None:\n",
    "        for col_name, levels in column_info[\"label_columns\"].items():\n",
    "            if col_name in df.columns:\n",
    "                ctype = pd.CategoricalDtype(categories=levels, ordered=False)\n",
    "                df[col_name] = df[col_name].astype(ctype)\n",
    "\n",
    "    def df_from_sql(\n",
    "        name: str,\n",
    "        db: psycopg.Connection,\n",
    "        column_info: Dict[str, Any],\n",
    "    ) -> pd.DataFrame:\n",
    "        with db.cursor() as cur:\n",
    "            cur.execute(sql.SQL('SELECT * FROM {}').format(sql.Identifier(table_name)))\n",
    "            df = pd.DataFrame(cur.fetchall(), columns=[desc[0] for desc in cur.description])\n",
    "        assign_categories_to_df(df, column_info)\n",
    "\n",
    "        return df\n",
    "    \n",
    "   \n",
    "        \n",
    "    conn = get_pg_conn()\n",
    "    column_info = json.loads(os.environ[\"COLUMNS\"])\n",
    "    df = df_from_sql(table_name, conn, column_info)\n",
    "    df.to_pickle(data_frame_pkl)\n",
    "\n",
    "\n",
    "load_df_from_postgresql_comp = kfp.components.create_component_from_func(\n",
    "    func=load_df_from_postgresql, base_image=BASE_IMAGE, packages_to_install=[\"psycopg[binary,pool]\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "033b8788-1df5-4675-a303-504f7a71924c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_quality_report(df: InputPath(str),\n",
    "                        features: List[str],\n",
    "                        mlpipeline_ui_metadata_path: OutputPath(str),\n",
    "                        output_report: OutputPath(str),\n",
    "                        target: str = 'Risk'):\n",
    "    from evidently.metric_preset import DataQualityPreset\n",
    "    from evidently.report import Report\n",
    "    from evidently import ColumnMapping\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    import json\n",
    "\n",
    "    \n",
    "    dataset = pd.read_pickle(df)\n",
    "    column_info = json.loads(os.environ[\"COLUMNS\"])\n",
    "\n",
    "    column_mapping = ColumnMapping()\n",
    "    column_mapping.target = target\n",
    "    column_mapping.task = \"classification\"\n",
    "    feature_set = set(features)\n",
    "    column_mapping.numerical_features = [\n",
    "        c\n",
    "        for c in column_info[\"int_columns\"]\n",
    "        if c in feature_set\n",
    "    ]\n",
    "    column_mapping.categorical_features = [\n",
    "        c\n",
    "        for c in column_info[\"label_columns\"]\n",
    "        if c in feature_set\n",
    "    ]\n",
    "\n",
    "    report = Report(\n",
    "        metrics=[\n",
    "            DataQualityPreset(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    report.run(\n",
    "        reference_data=None,\n",
    "        current_data=dataset,\n",
    "        column_mapping=column_mapping,\n",
    "    )\n",
    "\n",
    "    Path(output_report).parent.mkdir(parents=True, exist_ok=True)\n",
    "    report.save_html(output_report)\n",
    "    html_content = open(output_report, \"r\").read()\n",
    "    metadata = {\n",
    "        \"outputs\": [\n",
    "            {\n",
    "                \"type\": \"web-app\",\n",
    "                \"storage\": \"inline\",\n",
    "                \"source\": html_content,\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    with open(mlpipeline_ui_metadata_path, \"w\") as f:\n",
    "        json.dump(metadata, f)\n",
    "        \n",
    "data_quality_report_comp = kfp.components.create_component_from_func(\n",
    "    func=data_quality_report, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bd8a481-de19-4837-94cf-bcf805617245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_preprocessor(\n",
    "    training_df: InputPath(str),\n",
    "    preprocessor_pkl: OutputPath(str),\n",
    "    features: List[str],\n",
    "):\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import joblib\n",
    "    import os\n",
    "\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    from sklearn.pipeline import Pipeline\n",
    "\n",
    "    feature_set = set(features)\n",
    "    column_info = json.loads(os.environ[\"COLUMNS\"])\n",
    "\n",
    "    ohe_labels = [\n",
    "        (\n",
    "            \"ohe_\" + label,\n",
    "            OneHotEncoder(\n",
    "                handle_unknown=\"ignore\", sparse_output=False, categories=[levels]\n",
    "            ),\n",
    "            [label],\n",
    "        )\n",
    "        for label, levels in column_info[\"label_columns\"].items()\n",
    "        if label in feature_set\n",
    "    ]\n",
    "\n",
    "    int_cols = [\n",
    "        (\n",
    "            \"passthrough\",\n",
    "            \"passthrough\",\n",
    "            [col for col in column_info[\"int_columns\"] if col in feature_set],\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"Preprocess\", ColumnTransformer(ohe_labels + int_cols, remainder=\"drop\")),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(pipe)\n",
    "    train = pd.read_pickle(training_df)\n",
    "    print(train.dtypes)\n",
    "    pipe.fit(train)\n",
    "    joblib.dump(pipe, preprocessor_pkl)\n",
    "\n",
    "\n",
    "fit_preprocessor_comp = kfp.components.create_component_from_func(\n",
    "    func=fit_preprocessor, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2af502ad-b08b-40e6-a8d2-e75fd3374826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    training_df: InputPath(str),\n",
    "    preprocessor: InputPath(str),\n",
    "    model: OutputPath(str),\n",
    "    target_processing_config: OutputPath(str),\n",
    "    mlpipeline_ui_metadata_path: OutputPath(str),\n",
    "    target: str = \"Risk\",\n",
    "    tensorboard_dir: str = None,\n",
    "):\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import joblib\n",
    "    import tensorflow as tf\n",
    "    from keras import Sequential\n",
    "    from keras.layers import Dense, Dropout, BatchNormalization, Input\n",
    "    from keras.callbacks import EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    import numpy as np\n",
    "    import os\n",
    "    from sklearn.metrics import PrecisionRecallDisplay\n",
    "    import base64\n",
    "\n",
    "    tf.keras.utils.set_random_seed(42)\n",
    "    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.DEBUG)\n",
    "    target_processing_config_dict = {\n",
    "        \"threshold\" : 0.5,\n",
    "        \"target_names\" : {0: \"No Risk\", 1: \"Risk\"}\n",
    "    }\n",
    "\n",
    "    def get_tf_model(num_features: int) -> Tuple[tf.keras.Model, List[tf.keras.callbacks.Callback]]:\n",
    "\n",
    "        tf_model = Sequential(\n",
    "            [\n",
    "                Input(shape=(num_features,)),\n",
    "                BatchNormalization(),\n",
    "                Dense(35, activation=\"sigmoid\", name=\"layer1\"),\n",
    "                BatchNormalization(),\n",
    "                Dropout(0.3, name=\"dropout1\"),\n",
    "                Dense(35, activation=\"sigmoid\", name=\"layer2\"),\n",
    "                BatchNormalization(),\n",
    "                Dropout(0.3, name=\"dropout2\"),\n",
    "                Dense(35, activation=\"sigmoid\", name=\"layer3\"),\n",
    "                Dropout(0.3, name=\"dropout3\"),\n",
    "                Dense(\n",
    "                    1,\n",
    "                    activation=\"sigmoid\",\n",
    "                    name=\"output\",\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        tf_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "\n",
    "        callbacks = [\n",
    "            EarlyStopping(\n",
    "                monitor=\"val_loss\",\n",
    "                patience=20,\n",
    "                verbose=0,\n",
    "                mode=\"min\",\n",
    "                restore_best_weights=True,\n",
    "            ),\n",
    "            ReduceLROnPlateau(\n",
    "                monitor=\"val_loss\",\n",
    "                factor=0.1,\n",
    "                patience=10,\n",
    "                verbose=1,\n",
    "                min_delta=0.0001,\n",
    "                mode=\"min\",\n",
    "            )\n",
    "        ]\n",
    "        print(f\"constructing tensorborad with log dir {tensorboard_dir}...\")\n",
    "        if tensorboard_dir:\n",
    "            callbacks.append(TensorBoard(\n",
    "                log_dir=tensorboard_dir\n",
    "            ))\n",
    "\n",
    "        return tf_model, callbacks\n",
    "\n",
    "    print(\"loading training data...\")\n",
    "    train = pd.read_pickle(training_df)\n",
    "    print(\"loading preprocessor...\")\n",
    "    preprocessor = joblib.load(preprocessor)\n",
    "\n",
    "    X = tf.convert_to_tensor(preprocessor.transform(train))\n",
    "    y = tf.convert_to_tensor(\n",
    "        train.loc[:, target].apply(lambda v: 1 if v == target_processing_config_dict[\"target_names\"][1] else 0)\n",
    "    )\n",
    "    print(\"obtaining model....\")\n",
    "    tf_model, callbacks = get_tf_model(num_features=X.shape[1])\n",
    "    print(\"Training...\")\n",
    "    tf_model.fit(\n",
    "        X,\n",
    "        y,\n",
    "        validation_split=0.2,\n",
    "        epochs=500,\n",
    "        callbacks=callbacks,\n",
    "        class_weight={0: 1, 1: 2},\n",
    "    )\n",
    "    \n",
    "    # calculate best threshold of highest f1 score\n",
    "    predictions = tf_model.predict(X)\n",
    "    precision, recall, thresholds = precision_recall_curve(\n",
    "        y_true=y.numpy(), probas_pred=predictions.flatten()\n",
    "    )\n",
    "    f1s = 2 * (precision * recall) / (precision + recall)\n",
    "    threshold_index = np.argmax(f1s)\n",
    "    target_processing_config_dict[\"threshold\"] = float(thresholds[threshold_index])\n",
    "    \n",
    "    # Save model and threshold config\n",
    "    tf_model.save(model, save_format=\"h5\")\n",
    "    with open(target_processing_config, \"w\") as f:\n",
    "        json.dump(target_processing_config_dict, f)\n",
    "        \n",
    "    \n",
    "    plt = PrecisionRecallDisplay.from_predictions(\n",
    "        y_true=y.numpy(), y_pred=predictions.flatten(),\n",
    "    )\n",
    "    plt.ax_.plot(recall[threshold_index], precision[threshold_index], \n",
    "                     marker=\"o\", markersize=10, markeredgecolor=\"black\", markerfacecolor=\"red\")\n",
    "    plt.ax_.text(recall[threshold_index] + .03, precision[threshold_index] + .03,\n",
    "                  (f\"({recall[threshold_index]:.3f},{precision[threshold_index]:.3f})\\n\" +\n",
    "                   f\"threshold={thresholds[threshold_index]:.3f}\\n\" +\n",
    "                   f\"f1={f1s[threshold_index]:.3f}\")\n",
    "                )\n",
    "    plt.ax_.set_title(\"Precision Recall - Risk (Training)\")\n",
    "\n",
    "    plt.figure_.savefig(\"pr.jpg\")\n",
    "    with open(\"pr.jpg\", \"rb\") as f:\n",
    "        jpg = base64.b64encode(f.read())\n",
    "    html = f'<img src=\"data:image/jpg;base64,{jpg.decode(\"utf-8\")}\"/>'\n",
    "    metadata = {\"outputs\": [{\"type\": \"markdown\", \"storage\": \"inline\", \"source\": html}]}\n",
    "    with open(mlpipeline_ui_metadata_path, \"w\") as metadata_file:\n",
    "        json.dump(metadata, metadata_file)\n",
    "\n",
    "train_comp = kfp.components.create_component_from_func(\n",
    "    func=train, base_image=BASE_IMAGE, packages_to_install=[\"minio\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8937ccc-1de2-42cf-a4c2-5bed40cfb022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    df: InputPath(str),\n",
    "    preprocessor: InputPath(str),\n",
    "    onnx_model: InputPath(str),\n",
    "    target_processing_config: InputPath(str),\n",
    "    output_report: OutputPath(str),\n",
    "    mlpipeline_ui_metadata_path: OutputPath(str),\n",
    "    target=\"Risk\",\n",
    "):\n",
    "    import pandas as pd\n",
    "    import joblib\n",
    "    import json\n",
    "    from evidently.metric_preset import ClassificationPreset\n",
    "    from evidently.report import Report\n",
    "    from evidently import ColumnMapping\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    import onnxruntime as ort\n",
    "    import numpy as np\n",
    "\n",
    "    dataset = pd.read_pickle(df)\n",
    "    preprocessor = joblib.load(preprocessor)\n",
    "    \n",
    "    inference_session = ort.InferenceSession(\n",
    "            onnx_model, providers=[\"CPUExecutionProvider\"]\n",
    "    )\n",
    "    \n",
    "    with open(target_processing_config, \"r\") as f:\n",
    "        target_processing_config_dict = json.load(f)\n",
    "        target_processing_config_dict[\"target_names\"] = {int(k):v for k,v in target_processing_config_dict[\"target_names\"].items()}\n",
    "\n",
    "    column_info = json.loads(os.environ[\"COLUMNS\"])\n",
    "\n",
    "    X = preprocessor.transform(dataset).astype(np.float32)\n",
    "    y_prob = np.array(\n",
    "        inference_session.run(\n",
    "            [], {\"input_1\": X}\n",
    "        )).flatten()\n",
    "\n",
    "    dataset[\"Prediction\"] = pd.Series(y_prob).apply(\n",
    "        lambda p: 1 if p > target_processing_config_dict[\"threshold\"] else 0\n",
    "    )\n",
    "    dataset[\"Actual\"] = dataset.loc[:, target].apply(\n",
    "        lambda v: 1 if v == target_processing_config_dict[\"target_names\"][1] else 0\n",
    "    )\n",
    "\n",
    "    column_mapping = ColumnMapping()\n",
    "    column_mapping.target_names = target_processing_config_dict[\"target_names\"]\n",
    "    column_mapping.target = \"Actual\"\n",
    "    column_mapping.prediction = \"Prediction\"\n",
    "    column_mapping.task = \"classification\"\n",
    "    column_mapping.numerical_features = [\n",
    "        c\n",
    "        for c in column_info[\"int_columns\"]\n",
    "        if c in set(preprocessor.feature_names_in_)\n",
    "    ]\n",
    "    column_mapping.categorical_features = [\n",
    "        c\n",
    "        for c in column_info[\"label_columns\"]\n",
    "        if c in set(preprocessor.feature_names_in_)\n",
    "    ]\n",
    "\n",
    "    report = Report(\n",
    "        metrics=[\n",
    "            ClassificationPreset(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    report.run(\n",
    "        reference_data=None,\n",
    "        current_data=dataset,\n",
    "        column_mapping=column_mapping,\n",
    "    )\n",
    "\n",
    "    Path(output_report).parent.mkdir(parents=True, exist_ok=True)\n",
    "    report.save_html(output_report)\n",
    "    html_content = open(output_report, \"r\").read()\n",
    "    metadata = {\n",
    "        \"outputs\": [\n",
    "            {\n",
    "                \"type\": \"web-app\",\n",
    "                \"storage\": \"inline\",\n",
    "                \"source\": html_content,\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    with open(mlpipeline_ui_metadata_path, \"w\") as f:\n",
    "        json.dump(metadata, f)\n",
    "\n",
    "eval_comp = kfp.components.create_component_from_func(\n",
    "    func=evaluate, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5983463-5638-46e2-8ba8-d3a890557c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_explainer(train_df: InputPath(str),\n",
    "                    preprocessor: InputPath(str),\n",
    "                    onnx_model: InputPath(str),\n",
    "                    target_processing_config: InputPath(str),\n",
    "                    explainer_dll: OutputPath(str)):\n",
    "    from alibi.explainers.anchors import anchor_tabular\n",
    "    from alibi.utils import gen_category_map\n",
    "    import joblib\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    import re\n",
    "    import numpy as np\n",
    "    import dill\n",
    "    from functools import partial\n",
    "    from typing import Tuple, List, Dict\n",
    "    import json\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import onnxruntime as ort\n",
    "    import logging\n",
    "    \n",
    "    logging.basicConfig()\n",
    "    \n",
    "    def generate_category_map(preprocessor_pipeline: Pipeline) -> Tuple[List[str], Dict[int, List[str]]]:\n",
    "        features = []\n",
    "        seen_features = set()\n",
    "        category_map = dict()\n",
    "\n",
    "        for out_col_name in preprocessor_pipeline.get_feature_names_out():\n",
    "            parts = re.search(\"(.*)__([^_]+)(_([A-Za-z0-9_-]+))?\", out_col_name)\n",
    "            if parts:\n",
    "                if parts.group(2) not in seen_features:\n",
    "                    features.append(parts.group(2))\n",
    "                    seen_features.add(parts.group(2))\n",
    "\n",
    "                if parts.group(1) == \"categorical\" or parts.group(1).startswith(\"ohe_\"):\n",
    "                    levels = category_map.get(len(features) - 1, [])\n",
    "                    levels.append(parts.group(4))\n",
    "                    category_map[len(features) - 1] = levels\n",
    "            else:\n",
    "                raise ValueError(\"Could not parse column \" + out_col_name)\n",
    "\n",
    "        return features, category_map\n",
    "\n",
    "\n",
    "    with open(target_processing_config, \"r\") as f:\n",
    "        target_processing_config_dict = json.load(f)\n",
    "    column_info = json.loads(os.environ[\"COLUMNS\"])\n",
    "\n",
    "    inference_session = ort.InferenceSession(\n",
    "            onnx_model, providers=[\"CPUExecutionProvider\"]\n",
    "        )\n",
    "    threshold:float = float(target_processing_config_dict[\"threshold\"])\n",
    "    def predict(X: np.ndarray) -> np.ndarray:\n",
    "        scores= np.array(inference_session.run(\n",
    "            [], {\"input_1\": X}\n",
    "        )).flatten()\n",
    "        predictions = pd.Series(scores).apply(lambda p: 1 if p > threshold else 0).to_numpy()\n",
    "        return predictions\n",
    "\n",
    "    preprocessor_pipeline = joblib.load(preprocessor)\n",
    "    features, category_map = generate_category_map(preprocessor_pipeline)\n",
    "\n",
    "    dataset = pd.read_pickle(train_df)\n",
    "    X = preprocessor_pipeline.transform(dataset).astype(np.float32)\n",
    "\n",
    "    logging.info(f\"Training explainer: features={features} categories={category_map} X.shape = {X.shape}\")\n",
    "    explainer = anchor_tabular.AnchorTabular(\n",
    "        predictor=predict, feature_names=features, categorical_names=category_map, ohe=True, seed=42\n",
    "    )\n",
    "    explainer.fit(X, disc_prec=[10,25,33,50,66,75,90])\n",
    "\n",
    "    explainer.reset_predictor(None)   # Clear explainer predict_fn as its a lambda and will be reset when loaded\n",
    "    with open(explainer_dll, \"wb\") as f:\n",
    "        dill.dump(explainer, f)\n",
    "\n",
    "train_explainer_comp = kfp.components.create_component_from_func(\n",
    "    func=train_explainer, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "788bf836-5679-415a-96d9-1a766c82cd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_model_to_onnx(tf_model: InputPath(str),\n",
    "                          onnx_model: OutputPath(str)):\n",
    "    import tf2onnx\n",
    "    import tensorflow as tf\n",
    "    import onnx\n",
    "    \n",
    "    keras_model = tf.keras.models.load_model(tf_model)    \n",
    "    converted_model, _ = tf2onnx.convert.from_keras(keras_model)\n",
    "    onnx.save_model(converted_model, onnx_model)\n",
    "\n",
    "convert_model_to_onnx_comp = kfp.components.create_component_from_func(\n",
    "    func=convert_model_to_onnx, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f9fa77a-32e0-4ea2-a93c-fa0197f10265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_artifacts(\n",
    "    onnx_model: InputPath(str),\n",
    "    preprocessor: InputPath(str),\n",
    "    explainer: InputPath(str),\n",
    "    archive_name: str,\n",
    "    minio_url: str = \"minio-service.kubeflow:9000\",\n",
    "    version: str = \"1\"\n",
    ") -> NamedTuple(\"UploadOutput\", [(\"s3_address\", str)]):\n",
    "    \"\"\"Uploads a model file to MinIO artifact store.\"\"\"\n",
    "\n",
    "    from collections import namedtuple\n",
    "    import logging\n",
    "    from minio import Minio\n",
    "    import sys\n",
    "    import tarfile\n",
    "    import os\n",
    "\n",
    "    logging.basicConfig(\n",
    "        stream=sys.stdout,\n",
    "        level=logging.INFO,\n",
    "        format=\"%(levelname)s %(asctime)s: %(message)s\",\n",
    "    )\n",
    "    logger = logging.getLogger()\n",
    "\n",
    "    ARCHIVE_FILE = f\"/tmp/{archive_name}\"\n",
    "    with tarfile.open(ARCHIVE_FILE, \"w\") as f:\n",
    "        f.add(onnx_model, arcname=\"model.onnx\")\n",
    "        f.add(preprocessor, arcname=\"preprocessor.joblib\")\n",
    "        f.add(explainer, arcname=\"explainer.dll\")\n",
    "\n",
    "    minio_client = Minio(\n",
    "            minio_url, \n",
    "            access_key=os.environ[\"MINIO_ID\"], \n",
    "            secret_key=os.environ[\"MINIO_PWD\"], secure=False\n",
    "        )\n",
    "\n",
    "    # Create export bucket if it does not yet exist\n",
    "    export_bucket=\"{{workflow.namespace}}\"\n",
    "    existing_bucket = next(filter(lambda bucket: bucket.name == export_bucket, minio_client.list_buckets()), None)\n",
    "\n",
    "    if not existing_bucket:\n",
    "        logger.info(f\"Creating bucket '{export_bucket}'...\")\n",
    "        minio_client.make_bucket(bucket_name=export_bucket)\n",
    "\n",
    "    path = f\"tar/{version}/{archive_name}\"\n",
    "    s3_address = f\"s3://{export_bucket}/tar\"\n",
    "\n",
    "    logger.info(f\"Saving onnx file to MinIO (s3 address: {s3_address})...\")\n",
    "    minio_client.fput_object(\n",
    "        bucket_name=export_bucket,  # bucket name in Minio\n",
    "        object_name=path,  # file name in bucket of Minio \n",
    "        file_path=ARCHIVE_FILE,  # file path / name in local system\n",
    "    )\n",
    "\n",
    "    logger.info(\"Finished.\")\n",
    "    out_tuple = namedtuple(\"UploadOutput\", [\"s3_address\"])\n",
    "    return out_tuple(s3_address)\n",
    "\n",
    "\n",
    "upload_artifacts_comp = kfp.components.create_component_from_func(\n",
    "    func=upload_artifacts, base_image=BASE_IMAGE, packages_to_install=[\"minio==7.1.13\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b53b614-684b-48c0-bc30-b03b3b2a3bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_inference_service(name:str,\n",
    "                             target_processing_config: InputPath(str),\n",
    "                             model_archive_s3: str,\n",
    "                             transformer_image: str,\n",
    "                             predictor_image: str,\n",
    "                             explainer_image: str,\n",
    "                             predictor_max_replicas: int = 1,\n",
    "                             predictor_min_replicas: int = 1,\n",
    "                             predictor_concurrency_target: int = None,\n",
    "                             transformer_max_replicas: int = 1,\n",
    "                             transformer_min_replicas: int = 1,\n",
    "                             transformer_concurrency_target: int = None,\n",
    "                             explainer_max_replicas: int = 1,\n",
    "                             explainer_min_replicas: int = 1,\n",
    "                             explainer_concurrency_target: int = None\n",
    "                            ):\n",
    "    import kserve\n",
    "    from kubernetes import client, config\n",
    "    from kubernetes.client import (V1ServiceAccount, \n",
    "                                   V1Container, \n",
    "                                   V1EnvVar, \n",
    "                                   V1ObjectMeta, \n",
    "                                   V1ContainerPort, \n",
    "                                   V1ObjectReference,\n",
    "                                   V1ResourceRequirements\n",
    "                                  )\n",
    "    from kserve import KServeClient\n",
    "    from kserve import constants\n",
    "    from kserve import V1beta1PredictorSpec\n",
    "    from kserve import V1beta1ExplainerSpec\n",
    "    from kserve import V1beta1TransformerSpec\n",
    "    from kserve import V1beta1InferenceServiceSpec\n",
    "    from kserve import V1beta1InferenceService\n",
    "    import json\n",
    "    from http import HTTPStatus\n",
    "    import logging\n",
    "    import yaml\n",
    "    from time import sleep\n",
    "\n",
    "    with open(target_processing_config, \"r\") as f:\n",
    "        target_processing = json.load(f)\n",
    "\n",
    "    prediction_threshold = target_processing[\"threshold\"]\n",
    "    target_names = json.dumps(\n",
    "        [target_processing[\"target_names\"].get(str(idx),\"?\")\n",
    "         for idx in range(len(target_processing[\"target_names\"]))]\n",
    "    )\n",
    "\n",
    "    config.load_incluster_config()\n",
    "    SERVICE_ACCOUNT = \"credit-risk-inference-sa\"\n",
    "\n",
    "    sa = V1ServiceAccount(\n",
    "        api_version=\"v1\",\n",
    "        kind=\"ServiceAccount\",\n",
    "        metadata=V1ObjectMeta(name=SERVICE_ACCOUNT, \n",
    "                              namespace=\"{{workflow.namespace}}\"),\n",
    "        secrets=[V1ObjectReference(name=\"minio-credentials\")]\n",
    "    )\n",
    "    corev1 = client.CoreV1Api()\n",
    "    try:\n",
    "        corev1.create_namespaced_service_account(namespace=\"{{workflow.namespace}}\",\n",
    "                                                 body=sa)\n",
    "    except client.exceptions.ApiException as e:\n",
    "        if e.status==HTTPStatus.CONFLICT:\n",
    "            corev1.patch_namespaced_service_account(name=SERVICE_ACCOUNT,\n",
    "                                                    namespace=\"{{workflow.namespace}}\",\n",
    "                                                    body=sa)\n",
    "        else:\n",
    "            raise\n",
    "    \n",
    "    predictor_spec = V1beta1PredictorSpec(\n",
    "        max_replicas=predictor_max_replicas,\n",
    "        min_replicas=predictor_min_replicas,\n",
    "        scale_target=predictor_concurrency_target,\n",
    "        scale_metric=\"concurrency\",\n",
    "        containers=[\n",
    "            V1Container(\n",
    "                name=\"kserve-container\",\n",
    "                image=predictor_image,\n",
    "                args=[\"--grpc_port=8081\", f\"--model_name={name}\"],\n",
    "                ports=[V1ContainerPort(\n",
    "                    container_port=8081,\n",
    "                    name=\"h2c\",\n",
    "                    protocol=\"TCP\"\n",
    "                )],\n",
    "                resources=V1ResourceRequirements(\n",
    "                    limits={\"memory\": \"10Gi\"},\n",
    "                    requests={\"memory\": \"2Gi\"},\n",
    "                ),\n",
    "                env=[\n",
    "                 V1EnvVar(\n",
    "                     name=\"STORAGE_URI\", value=model_archive_s3\n",
    "                 ),\n",
    "                 V1EnvVar(\n",
    "                     name=\"THRESHOLD\",\n",
    "                     value=str(prediction_threshold)\n",
    "                 )\n",
    "                ],\n",
    "            )\n",
    "        ],\n",
    "        service_account_name=SERVICE_ACCOUNT\n",
    "    )\n",
    "\n",
    "    transformer_spec = V1beta1TransformerSpec(\n",
    "        max_replicas=transformer_max_replicas,\n",
    "        min_replicas=transformer_min_replicas,\n",
    "        scale_target=transformer_concurrency_target,\n",
    "        scale_metric=\"concurrency\",\n",
    "        containers=[\n",
    "            V1Container(\n",
    "                name=\"kserve-container\",\n",
    "                image=transformer_image,\n",
    "                args=[\"--protocol=grpc-v2\", f\"--model_name={name}\"],\n",
    "                resources=V1ResourceRequirements(\n",
    "                    limits={\"memory\": \"10Gi\"},\n",
    "                    requests={\"memory\": \"2Gi\"},\n",
    "                ),\n",
    "                #ports=[V1ContainerPort(\n",
    "                #    container_port=8080,\n",
    "                #    name=\"h2c\",\n",
    "                #    protocol=\"TCP\"\n",
    "                #)],\n",
    "                env=[\n",
    "                 V1EnvVar(\n",
    "                     name=\"STORAGE_URI\", value=model_archive_s3\n",
    "                 ),\n",
    "                V1EnvVar(\n",
    "                    name=\"TARGET_NAMES\", value=target_names\n",
    "                )\n",
    "                ],\n",
    "            )\n",
    "        ],\n",
    "        service_account_name=SERVICE_ACCOUNT\n",
    "    )\n",
    "\n",
    "    explainer_spec=V1beta1ExplainerSpec(\n",
    "        max_replicas=explainer_max_replicas,\n",
    "        min_replicas=explainer_min_replicas,\n",
    "        scale_target=explainer_concurrency_target,\n",
    "        scale_metric=\"concurrency\",\n",
    "        containers=[\n",
    "            V1Container(\n",
    "                name=\"kserve-container\",\n",
    "                image=explainer_image,\n",
    "                args=[\"--protocol=grpc-v2\", f\"--model_name={name}\"],\n",
    "                resources=V1ResourceRequirements(\n",
    "                    limits={\"memory\": \"10Gi\"},\n",
    "                    requests={\"memory\": \"4Gi\"},\n",
    "                ),\n",
    "                #ports=[V1ContainerPort(\n",
    "                #    container_port=8080,\n",
    "                #    name=\"h2c\",\n",
    "                #    protocol=\"TCP\"\n",
    "                #)],\n",
    "                env=[\n",
    "                 V1EnvVar(\n",
    "                     name=\"STORAGE_URI\", value=model_archive_s3\n",
    "                 )\n",
    "                ],\n",
    "            )\n",
    "        ],\n",
    "        service_account_name=SERVICE_ACCOUNT\n",
    "    )\n",
    "\n",
    "\n",
    "    inference_service=V1beta1InferenceService(\n",
    "        api_version=constants.KSERVE_V1BETA1,\n",
    "        kind=constants.KSERVE_KIND,\n",
    "        metadata=V1ObjectMeta(name=name, \n",
    "                              namespace=\"{{workflow.namespace}}\",\n",
    "                              annotations={\"sidecar.istio.io/inject\": \"false\",\n",
    "                                           \"serving.kserve.io/enable-prometheus-scraping\" : \"true\"}),\n",
    "        spec=V1beta1InferenceServiceSpec(predictor=predictor_spec,\n",
    "                                         transformer=transformer_spec,\n",
    "                                         explainer=explainer_spec)\n",
    "    )\n",
    "    # serving.kserve.io/inferenceservice: credit-risk\n",
    "    logging.info(\n",
    "        yaml.dump(\n",
    "            client.ApiClient().sanitize_for_serialization(inference_service)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # KServeClient doesn't throw ApiException for CONFLICT\n",
    "    # Using the k8s API directly for the create\n",
    "    api_instance = client.CustomObjectsApi()\n",
    "    while True:\n",
    "        try:\n",
    "            api_instance.create_namespaced_custom_object(\n",
    "                    group=constants.KSERVE_GROUP,\n",
    "                    version=inference_service.api_version.split(\"/\")[1],\n",
    "                    namespace=\"{{workflow.namespace}}\",\n",
    "                    plural=constants.KSERVE_PLURAL,\n",
    "                    body=inference_service)\n",
    "            break\n",
    "        except client.exceptions.ApiException as api_exception:\n",
    "            if api_exception.status==HTTPStatus.CONFLICT:\n",
    "                try:\n",
    "                    api_instance.delete_namespaced_custom_object(\n",
    "                        group=constants.KSERVE_GROUP,\n",
    "                        version=inference_service.api_version.split(\"/\")[1],\n",
    "                        namespace=\"{{workflow.namespace}}\",\n",
    "                        plural=constants.KSERVE_PLURAL,\n",
    "                        name=name)\n",
    "                    sleep(15)\n",
    "                except client.exceptions.ApiException as api_exception2:\n",
    "                    if api_exception2.status in {HTTPStatus.NOT_FOUND, HTTPStatus.GONE}:\n",
    "                        pass\n",
    "                    else:\n",
    "                        raise\n",
    "\n",
    "            else:\n",
    "                raise\n",
    "            \n",
    "    kclient = KServeClient()\n",
    "    kclient.wait_isvc_ready(name=name, namespace=\"{{workflow.namespace}}\")\n",
    "    \n",
    "    if not kclient.is_isvc_ready(name=name, namespace=\"{{workflow.namespace}}\"):\n",
    "        raise RuntimeError(f\"The inference service {name} is not ready!\")\n",
    "\n",
    "deploy_inference_service_comp = kfp.components.create_component_from_func(\n",
    "    func=deploy_inference_service, base_image=KSERVE_IMAGE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25df93f6-5134-4178-bcef-3458ff25ca3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubernetes.client import ( V1PersistentVolumeClaimVolumeSource, V1Volume, V1VolumeMount)\n",
    "@dsl.pipeline(\n",
    "    name=\"Credit Risk\",\n",
    "    description=\"An example pipeline that builds and deploys a credit risk model\",\n",
    ")\n",
    "def credit_model_pipeline():\n",
    "    def env_var_from_secret(env_var_name: str, secret_name: str, secret_key: str) -> V1EnvVar:\n",
    "        return V1EnvVar(name=env_var_name,\n",
    "                                     value_from=V1EnvVarSource(\n",
    "                                         secret_key_ref=V1SecretKeySelector(\n",
    "                                             name=secret_name,\n",
    "                                             key=secret_key\n",
    "                                         )\n",
    "                                     )\n",
    "                                    )\n",
    "    \n",
    "    def add_db2_connection_secrets(pipeline_task) -> None:\n",
    "        pipeline_task.container.add_env_variable(env_var_from_secret(\"db2_host\", \"db2-credentials\", \"host\"))\n",
    "        pipeline_task.container.add_env_variable(env_var_from_secret(\"db2_user\", \"db2-credentials\", \"username\"))\n",
    "        pipeline_task.container.add_env_variable(env_var_from_secret(\"db2_pwd\", \"db2-credentials\", \"password\"))\n",
    "        pipeline_task.container.add_env_variable(env_var_from_secret(\"db2_port\", \"db2-credentials\", \"port\"))\n",
    "\n",
    "    \n",
    "    def add_pg_connection_secrets(pipeline_task) -> None:\n",
    "        pipeline_task.container.add_env_variable(V1EnvVar(name=\"PG_HOST\", value=\"postgresql.{{workflow.namespace}}.svc\"))\n",
    "        pipeline_task.container.add_env_variable(env_var_from_secret(\"PG_DB_NAME\", \"postgresql\", \"database-name\"))\n",
    "        pipeline_task.container.add_env_variable(env_var_from_secret(\"PG_USER\", \"postgresql\", \"database-user\"))\n",
    "        pipeline_task.container.add_env_variable(env_var_from_secret(\"PG_PWD\", \"postgresql\", \"database-password\"))\n",
    "        pipeline_task.container.add_env_variable(V1EnvVar(name=\"PG_PORT\", value=\"5432\"))\n",
    "\n",
    "    feature_columns = [\n",
    "            \"CheckingStatus\",\n",
    "            \"LoanDuration\",\n",
    "            \"CreditHistory\",\n",
    "            \"LoanPurpose\",\n",
    "            \"LoanAmount\",\n",
    "            \"ExistingSavings\",\n",
    "            \"EmploymentDuration\",\n",
    "            \"InstallmentPercent\",\n",
    "            \"Sex\",\n",
    "            \"OthersOnLoan\",\n",
    "            \"CurrentResidenceDuration\",\n",
    "            \"OwnsProperty\",\n",
    "            \"Age\",\n",
    "            \"InstallmentPlans\",\n",
    "            \"Housing\",\n",
    "            \"ExistingCreditsCount\",\n",
    "            \"Job\",\n",
    "            \"Dependents\",\n",
    "            \"Telephone\",\n",
    "            \"ForeignWorker\",\n",
    "        ]\n",
    "    \n",
    "    load_training_data_task = load_df_from_postgresql_comp(table_name=\"TRAIN\")\n",
    "    load_training_data_task.set_display_name(\"Load_Training_Data_From_PostgreSQL\")\n",
    "    load_training_data_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    add_pg_connection_secrets(load_training_data_task)\n",
    "\n",
    "    data_quality_report_comp(df=load_training_data_task.outputs[\"data_frame_pkl\"],\n",
    "                             features=feature_columns)\n",
    "    \n",
    "    load_test_data_task = load_df_from_postgresql_comp(table_name=\"TEST\")\n",
    "    load_test_data_task.set_display_name(\"Load_Test_Data_From_PostgreSQL\")\n",
    "    load_test_data_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    add_pg_connection_secrets(load_test_data_task)\n",
    "\n",
    "    fit_preprocessor_task = fit_preprocessor_comp(\n",
    "        training_df=load_training_data_task.outputs[\"data_frame_pkl\"],\n",
    "        features=feature_columns\n",
    "    )\n",
    "\n",
    "    create_tensorboard_volume = dsl.VolumeOp(\n",
    "        name=f\"Create PVC for tensorboard\",\n",
    "        resource_name=\"tensorboard\",\n",
    "        modes=dsl.VOLUME_MODE_RWM,\n",
    "        size=\"4G\",\n",
    "        set_owner_reference=True,\n",
    "    )\n",
    "    create_tensorboard_volume.add_pod_annotation(\n",
    "            name=\"pipelines.kubeflow.org/max_cache_staleness\", value=\"P0D\"\n",
    "        )\n",
    "        \n",
    "    configure_tensorboard_task = configure_tensorboard_comp(\n",
    "        pvc_name=create_tensorboard_volume.volume.persistent_volume_claim.claim_name\n",
    "    )\n",
    "    \n",
    "    train_model_task = train_comp(\n",
    "        training_df=load_training_data_task.outputs[\"data_frame_pkl\"],\n",
    "        preprocessor=fit_preprocessor_task.outputs[\"preprocessor_pkl\"],\n",
    "        target=\"Risk\",\n",
    "        tensorboard_dir=\"/tensorboard\",\n",
    "    )\n",
    "    train_model_task.after(configure_tensorboard_task)\n",
    "    train_model_task.add_pvolumes({\"/tensorboard\": create_tensorboard_volume.volume})\n",
    "\n",
    "    \n",
    "    convert_model_to_onnx_task = convert_model_to_onnx_comp(tf_model=train_model_task.outputs[\"model\"])\n",
    "    \n",
    "    train_explainer_task = train_explainer_comp(train_df=load_training_data_task.outputs[\"data_frame_pkl\"],\n",
    "                                                preprocessor=fit_preprocessor_task.outputs[\"preprocessor_pkl\"],\n",
    "                                                onnx_model=convert_model_to_onnx_task.outputs[\"onnx_model\"],\n",
    "                                                target_processing_config=train_model_task.outputs[\"target_processing_config\"])\n",
    "    \n",
    "    evaluate_model_task = eval_comp(\n",
    "        load_test_data_task.outputs[\"data_frame_pkl\"],\n",
    "        preprocessor=fit_preprocessor_task.outputs[\"preprocessor_pkl\"],\n",
    "        onnx_model=convert_model_to_onnx_task.outputs[\"onnx_model\"],\n",
    "        target_processing_config=train_model_task.outputs[\"target_processing_config\"],\n",
    "        target=\"Risk\"\n",
    "    )\n",
    "    \n",
    "    upload_artifacts_task = upload_artifacts_comp(\n",
    "        onnx_model=convert_model_to_onnx_task.outputs[\"onnx_model\"],\n",
    "        preprocessor=fit_preprocessor_task.outputs[\"preprocessor_pkl\"],\n",
    "        explainer=train_explainer_task.outputs[\"explainer_dll\"],\n",
    "        archive_name=\"credit-risk.tar\"\n",
    "    )\n",
    "    upload_artifacts_task.container.add_env_variable(env_var_from_secret(\"MINIO_ID\", \"mlpipeline-minio-artifact\", \"accesskey\"))\n",
    "    upload_artifacts_task.container.add_env_variable(env_var_from_secret(\"MINIO_PWD\", \"mlpipeline-minio-artifact\", \"secretkey\"))\n",
    "    upload_artifacts_task.after(evaluate_model_task)\n",
    "    \n",
    "    deploy_inference_service_task=deploy_inference_service_comp(name=\"credit-risk\",\n",
    "                                                                target_processing_config=train_model_task.outputs[\"target_processing_config\"],\n",
    "                                                                model_archive_s3=upload_artifacts_task.output,\n",
    "                                                                transformer_image=TRANSFORMER_IMAGE,\n",
    "                                                                predictor_image=PREDICTOR_IMAGE,\n",
    "                                                                explainer_image=EXPLAINER_IMAGE,\n",
    "                                                                predictor_max_replicas=4,\n",
    "                                                                predictor_concurrency_target=1,\n",
    "                                                                transformer_max_replicas=4,\n",
    "                                                                transformer_concurrency_target=1\n",
    "                                                               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76074abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pipeline(pipeline_name: str):\n",
    "    \"\"\"Delete's a pipeline with the specified name\"\"\"\n",
    "\n",
    "    client = kfp.Client()\n",
    "    existing_pipelines = client.list_pipelines(page_size=999).pipelines\n",
    "    matches = (\n",
    "        [ep.id for ep in existing_pipelines if ep.name == pipeline_name]\n",
    "        if existing_pipelines\n",
    "        else []\n",
    "    )\n",
    "    for id in matches:\n",
    "        client.delete_pipeline(id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c6a023f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_id(experiment_name: str) -> str:\n",
    "    \"\"\"Returns the id for the experiment, creating the experiment if needed\"\"\"\n",
    "    client = kfp.Client()\n",
    "    existing_experiments = client.list_experiments(page_size=999).experiments\n",
    "    matches = (\n",
    "        [ex.id for ex in existing_experiments if ex.name == experiment_name]\n",
    "        if existing_experiments\n",
    "        else []\n",
    "    )\n",
    "\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "\n",
    "    exp = client.create_experiment(experiment_name)\n",
    "    return exp.id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f1d0cfa-be30-42bc-a1b6-eebc0e98a226",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_conf = kfp.dsl.PipelineConf()\n",
    "\n",
    "def provide_column_info_transformer(op: dsl.ContainerOp):\n",
    "    \n",
    "    if isinstance(op, dsl.ContainerOp):\n",
    "        op.container.add_env_variable(\n",
    "            V1EnvVar(name=\"COLUMNS\",\n",
    "                    value_from=V1EnvVarSource(\n",
    "                                         config_map_key_ref=V1ConfigMapKeySelector(\n",
    "                                             name=\"credit-risk-columns\",\n",
    "                                             key=\"columns\"\n",
    "                                         )\n",
    "                                     )\n",
    "                    )\n",
    "        )\n",
    "                            \n",
    "\n",
    "pipeline_conf.add_op_transformer(provide_column_info_transformer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07a2f50b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=/pipeline/#/pipelines/details/bef8aa85-d0f7-4c2c-838f-2d20e09b3f07>Pipeline details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PIPELINE_NAME = \"Build_Credit_Risk_Model\"\n",
    "# Pipeline names need to be unique, so before we upload,\n",
    "# check for and delete any pipeline with the same name\n",
    "delete_pipeline(PIPELINE_NAME)\n",
    "\n",
    "        \n",
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=credit_model_pipeline,\n",
    "    package_path=f\"{PIPELINE_NAME}.yaml\",\n",
    "    pipeline_conf=pipeline_conf,\n",
    ")\n",
    "\n",
    "# upload\n",
    "client = kfp.Client()\n",
    "uploaded_pipeline = client.upload_pipeline(f\"{PIPELINE_NAME}.yaml\", PIPELINE_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3139f4d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/30c1e4e6-e8c4-4567-a114-7b4e31a86c27\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = client.run_pipeline(\n",
    "    experiment_id=get_experiment_id(\"credit-risk\"),\n",
    "    job_name=\"credit-risk\",\n",
    "    pipeline_id=uploaded_pipeline.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6dce21c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'Succeeded', 'error': None, 'time': '0:06:33', 'metrics': None}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TWENTY_MIN = 20 * 60\n",
    "result = client.wait_for_run_completion(run.id, timeout=TWENTY_MIN)\n",
    "{\n",
    "    \"status\": result.run.status,\n",
    "    \"error\": result.run.error,\n",
    "    \"time\": str(result.run.finished_at - result.run.created_at),\n",
    "    \"metrics\": result.run.metrics,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423d1e2d-865c-49b5-a4ee-abca2149303b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
