{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39cca785-1fe7-45ac-b567-7670ca9fd996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp.components import InputPath, OutputPath\n",
    "from kfp import dsl\n",
    "from typing import List, Tuple\n",
    "from kfp.dsl import ContainerOp\n",
    "from kubernetes.client.models import V1EnvVar,V1EnvVarSource, V1SecretKeySelector,V1ConfigMapKeySelector\n",
    "from typing import NamedTuple, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e721fe88-4ce3-4193-b568-97656c3b0eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_IMAGE = \"quay.io/ntlawrence/demo-workflow:3.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6504d5da-ad61-4dfc-a221-43d1657b791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df_from_db2(table_name: str,\n",
    "                     data_frame_pkl: OutputPath(str),\n",
    "                     target_column: str = 'Risk',\n",
    "                     predictions_column: str = '',\n",
    "                     time_window_minutes: Optional[int] = None) -> NamedTuple(\"LoadOutput\", [(\"num_rows\", \"int\")]):\n",
    "    import warnings\n",
    "    import ibm_db\n",
    "    import ibm_db_dbi\n",
    "    import os\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    from typing import Dict, Any\n",
    "    from collections import namedtuple\n",
    "    \n",
    "    def assign_categories_to_df(df: pd.DataFrame, column_info: Dict[str, any]) -> None:\n",
    "        for col_name, levels in column_info[\"label_columns\"].items():\n",
    "            if col_name in df.columns:\n",
    "                ctype = pd.CategoricalDtype(categories=levels, ordered=False)\n",
    "                df[col_name] = df[col_name].astype(ctype)\n",
    "\n",
    "    def df_from_sql(\n",
    "        name: str,\n",
    "        conn: ibm_db.IBM_DBConnection,\n",
    "        column_info: Dict[str, Any],\n",
    "        target_col: str = 'Risk',\n",
    "        predictions_col: str = ''\n",
    "    ) -> pd.DataFrame:\n",
    "        sql_safe_name = name.replace('\"', \"\")\n",
    "\n",
    "        column_list = column_info[\"columns\"] + ([] if not predictions_col else [predictions_col])\n",
    "        rStmtColsSql = \",\".join([f'\"{col}\"' for col in column_list])\n",
    "        rSql = f'SELECT {rStmtColsSql} FROM \"{sql_safe_name}\" WHERE \"{target_column}\" IS NOT NULL'\n",
    "        \n",
    "        if time_window_minutes:\n",
    "            rSql += ' AND SYSIBM.TIMESTAMPDIFF(2, CHAR(CURRENT_TIMESTAMP - \"LastChangeTimestamp\")) < ?'\n",
    "\n",
    "        read_conn = ibm_db_dbi.Connection(conn)\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", message=\"pandas only support SQLAlchemy\")\n",
    "            df = pd.read_sql(rSql, read_conn, params=(time_window_minutes,) if time_window_minutes else tuple())\n",
    "\n",
    "        assign_categories_to_df(df, column_info)\n",
    "        if predictions_col:\n",
    "            df[predictions_col] = df[predictions_col].astype(df[target_col].dtype)\n",
    "            \n",
    "        return df\n",
    "    \n",
    "    conn_str = (\n",
    "    \"DRIVER={IBM DB2 ODBC DRIVER};\"\n",
    "    f\"DATABASE=BLUDB;HOSTNAME={os.environ['db2_host']};PORT={os.environ['db2_port']};PROTOCOL=TCPIP;UID={os.environ['db2_user']};Pwd={os.environ['db2_pwd']};SECURITY=SSL;\"\n",
    "    )\n",
    "        \n",
    "    conn = ibm_db.connect(conn_str, \"\", \"\")\n",
    "\n",
    "    column_info = json.loads(os.environ[\"COLUMNS\"])\n",
    "    df = df_from_sql(table_name, conn, column_info, target_column, predictions_column)\n",
    "    df.to_pickle(data_frame_pkl)\n",
    "\n",
    "    out_tuple = namedtuple(\"LoadOutput\", [\"num_rows\"])\n",
    "    return out_tuple(df.shape[0])\n",
    "\n",
    "load_df_from_db2_comp = kfp.components.create_component_from_func(\n",
    "    func=load_df_from_db2, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef399a20-8a04-43dc-b113-3620f6457514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df_from_postgresql(table_name: str,\n",
    "                     data_frame_pkl: OutputPath(str),\n",
    "                     target_column: str = 'Risk',\n",
    "                     predictions_column: str = '',\n",
    "                     time_window_minutes: Optional[int] = None) -> NamedTuple(\"LoadOutput\", [(\"num_rows\", \"int\")]):\n",
    "    import warnings\n",
    "    import psycopg\n",
    "    from psycopg import sql\n",
    "    import os\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    from typing import Dict, Any\n",
    "    from collections import namedtuple\n",
    "    \n",
    "    def assign_categories_to_df(df: pd.DataFrame, column_info: Dict[str, any]) -> None:\n",
    "        for col_name, levels in column_info[\"label_columns\"].items():\n",
    "            if col_name in df.columns:\n",
    "                ctype = pd.CategoricalDtype(categories=levels, ordered=False)\n",
    "                df[col_name] = df[col_name].astype(ctype)\n",
    "\n",
    "    def get_pg_conn() -> psycopg.Connection:\n",
    "        host, dbname, username, password, port = (\n",
    "            os.environ.get('PG_HOST'),\n",
    "            os.environ.get('PG_DB_NAME'),\n",
    "            os.environ.get('PG_USER'),\n",
    "            os.environ.get('PG_PWD'),\n",
    "            int(os.environ.get('PG_PORT'))\n",
    "        )\n",
    "\n",
    "        conn_str = f\"postgresql://{username}:{password}@{host}:{port}/{dbname}?connect_timeout=10&application_name=mlpipeline\"\n",
    "        conn = psycopg.connect(conn_str)\n",
    "\n",
    "        return conn\n",
    "    \n",
    "    def df_from_sql(\n",
    "        name: str,\n",
    "        conn: ibm_db.IBM_DBConnection,\n",
    "        column_info: Dict[str, Any],\n",
    "        target_col: str = 'Risk',\n",
    "        predictions_col: str = '',\n",
    "        time_window_minutes: int = 0\n",
    "    ) -> pd.DataFrame:\n",
    "        \n",
    "        column_list = [sql.Identifier(col) for col in \n",
    "                       (column_info[\"columns\"] + ([] if not predictions_col else [predictions_col]))\n",
    "                      ]\n",
    "        \n",
    "        if time_window_minutes:        \n",
    "            query = sql.SQL(\n",
    "                'SELECT {} FROM {} WHERE {} IS NOT NULL AND EXTRACT(EPOCH FROM (CURRENT_TIMESTAMP - \"LastChangeTimestamp\")) / 60 < {}'\n",
    "            ).format(\n",
    "                sql.SQL(\", \").join(column_list),\n",
    "                sql.Identifier(name),\n",
    "                sql.Identifier(target_column),\n",
    "                sql.Literal(time_window_minutes)\n",
    "            )\n",
    "        else:\n",
    "             query = sql.SQL(\n",
    "                'SELECT {} FROM {} WHERE {} IS NOT NULL'\n",
    "            ).format(\n",
    "                sql.SQL(\", \").join(column_list),\n",
    "                sql.Identifier(name),\n",
    "                sql.Identifier(target_column)\n",
    "            )\n",
    "        \n",
    "        with conn.cursor() as cur:\n",
    "            print(query.as_string(cur))\n",
    "            cur.execute(query)\n",
    "            df = pd.DataFrame(cur.fetchall(), columns=[desc[0] for desc in cur.description])\n",
    "          \n",
    "        assign_categories_to_df(df, column_info)\n",
    "        if predictions_col:\n",
    "            df[predictions_col] = df[predictions_col].astype(df[target_col].dtype)\n",
    "            \n",
    "        return df\n",
    "\n",
    "    column_info = json.loads(os.environ[\"COLUMNS\"])\n",
    "    conn=get_pg_conn()\n",
    "    df = df_from_sql(table_name, conn, column_info, target_column, predictions_column, time_window_minutes)\n",
    "    df.to_pickle(data_frame_pkl)\n",
    "\n",
    "    out_tuple = namedtuple(\"LoadOutput\", [\"num_rows\"])\n",
    "    return out_tuple(df.shape[0])\n",
    "\n",
    "load_df_from_postgresql_comp = kfp.components.create_component_from_func(\n",
    "    func=load_df_from_postgresql, base_image=BASE_IMAGE,  packages_to_install=[\"psycopg[binary,pool]\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25da8a2e-74dd-4377-9609-6a65e4b6702a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evidently_report(reference_df: InputPath(str),\n",
    "                      production_df: InputPath(str),\n",
    "                      mlpipeline_ui_metadata_path: OutputPath(str),\n",
    "                      output_report: OutputPath(str),\n",
    "                      output_json: OutputPath(str),\n",
    "                      target: str = 'Risk',\n",
    "                      report_type: str = 'drift'\n",
    "                     ):\n",
    "    from evidently.metric_preset import (\n",
    "    DataDriftPreset,\n",
    "    TargetDriftPreset,\n",
    "    ClassificationPreset,\n",
    "    )\n",
    "    from evidently.report import Report\n",
    "    from evidently import ColumnMapping\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    import json\n",
    "    \n",
    "    reference_dataset = pd.read_pickle(reference_df)\n",
    "    production_dataset = pd.read_pickle(production_df)\n",
    "    column_info = json.loads(os.environ[\"COLUMNS\"])\n",
    "\n",
    "    column_mapping = ColumnMapping()\n",
    "    column_mapping.target = target\n",
    "    column_mapping.task = \"classification\"\n",
    "   \n",
    "\n",
    "    column_mapping.numerical_features = [\n",
    "        c\n",
    "        for c in column_info[\"int_columns\"]\n",
    "        if c != target\n",
    "    ]\n",
    "    column_mapping.categorical_features = [\n",
    "        c\n",
    "        for c in column_info[\"label_columns\"]\n",
    "        if c != target\n",
    "    ]\n",
    "\n",
    "    if report_type.lower() == \"drift\":\n",
    "        report = Report(\n",
    "            metrics=[\n",
    "                DataDriftPreset(),\n",
    "            ]\n",
    "        )\n",
    "    elif report_type.lower() == \"target_drift\":\n",
    "        report = Report(\n",
    "            metrics=[\n",
    "                TargetDriftPreset(),\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    report.run(\n",
    "        reference_data=reference_dataset,\n",
    "        current_data=production_dataset,\n",
    "        column_mapping=column_mapping,\n",
    "    )\n",
    "\n",
    "    Path(output_report).parent.mkdir(parents=True, exist_ok=True)\n",
    "    report.save_html(output_report)\n",
    "    html_content = open(output_report, \"r\").read()\n",
    "    metadata = {\n",
    "        \"outputs\": [\n",
    "            {\n",
    "                \"type\": \"web-app\",\n",
    "                \"storage\": \"inline\",\n",
    "                \"source\": html_content,\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    with open(mlpipeline_ui_metadata_path, \"w\") as f:\n",
    "        json.dump(metadata, f)\n",
    "        \n",
    "    with open(output_json, \"w\") as json_f:\n",
    "        json_f.write(report.json())\n",
    "        \n",
    "evidently_report_comp = kfp.components.create_component_from_func(\n",
    "    func=evidently_report, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b91ebc7-0f40-482f-b4d2-0d002fc086dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evidently_classification_report(production_df: InputPath(str),\n",
    "                      mlpipeline_ui_metadata_path: OutputPath(str),\n",
    "                      output_report: OutputPath(str),\n",
    "                      output_json: OutputPath(str),\n",
    "                      target: str = 'Risk',\n",
    "                      predictions: str = 'PredictedRisk',\n",
    "                      pos_class: str = 'Risk'\n",
    "                     ):\n",
    "    from evidently.metric_preset import (\n",
    "    ClassificationPreset,\n",
    "    )\n",
    "    from evidently.report import Report\n",
    "    from evidently import ColumnMapping\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    import json\n",
    "\n",
    "    production_dataset = pd.read_pickle(production_df)\n",
    "  \n",
    "    production_dataset.dropna(subset=[target, predictions], inplace=True)\n",
    "    column_info = json.loads(os.environ[\"COLUMNS\"])\n",
    "\n",
    "    column_mapping = ColumnMapping()\n",
    "    #column_mapping.target_names = ['No Risk', 'Risk']\n",
    "    column_mapping.target = 'Risk' #'Actual_Int'\n",
    "    column_mapping.prediction = 'PredictedRisk' #'Predicted_Int'\n",
    "    column_mapping.pos_label = 'Risk'\n",
    "    column_mapping.task = \"classification\"\n",
    "\n",
    "   \n",
    "\n",
    "    column_mapping.numerical_features = [\n",
    "        c\n",
    "        for c in column_info[\"int_columns\"]\n",
    "        if c != target\n",
    "    ]\n",
    "    column_mapping.categorical_features = [\n",
    "        c\n",
    "        for c in column_info[\"label_columns\"]\n",
    "        if c != target\n",
    "    ]\n",
    "  \n",
    "    report = Report(\n",
    "        metrics=[\n",
    "            ClassificationPreset()\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    #production_dataset['Actual_Int'] = production_dataset[target].apply(lambda v: 1 if v == pos_class else 0)\n",
    "    #production_dataset['Predicted_Int'] = production_dataset[predictions].apply(lambda v: 1 if v == pos_class else 0)\n",
    "\n",
    "    report.run(\n",
    "        reference_data=None,\n",
    "        current_data=production_dataset,\n",
    "        column_mapping=column_mapping,\n",
    "    )\n",
    "\n",
    "    Path(output_report).parent.mkdir(parents=True, exist_ok=True)\n",
    "    report.save_html(output_report)\n",
    "    html_content = open(output_report, \"r\").read()\n",
    "    metadata = {\n",
    "        \"outputs\": [\n",
    "            {\n",
    "                \"type\": \"web-app\",\n",
    "                \"storage\": \"inline\",\n",
    "                \"source\": html_content,\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    with open(mlpipeline_ui_metadata_path, \"w\") as f:\n",
    "        json.dump(metadata, f)\n",
    "        \n",
    "    with open(output_json, \"w\") as json_f:\n",
    "        json_f.write(report.json())\n",
    "        \n",
    "evidently_classification_report_comp = kfp.components.create_component_from_func(\n",
    "    func=evidently_classification_report, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5da929a4-5137-4704-855d-60cc02b2220a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "def check_metrics(classification_report: InputPath(str),\n",
    "                  data_drift_report: InputPath(str),\n",
    "                  target_drift_report: InputPath(str)) -> NamedTuple(\"EvaluationOutput\", [(\"mlpipeline_metrics\", \"Metrics\")]):\n",
    "    import json\n",
    "    from collections import namedtuple\n",
    "\n",
    "    with open(classification_report) as class_f:\n",
    "         classification = json.load(class_f)\n",
    "\n",
    "    ClassificationQualityMetric = next(filter(lambda m: m[\"metric\"] == \"ClassificationQualityMetric\", classification[\"metrics\"]))\n",
    "        \n",
    "    \n",
    "    metrics = {\n",
    "        \"metrics\": [\n",
    "            {\"name\": \"f1\", \"numberValue\": ClassificationQualityMetric[\"result\"][\"current\"][\"f1\"], \"format\": \"RAW\"}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    out_tuple = namedtuple(\"EvaluationOutput\", [\"mlpipeline_metrics\"])\n",
    "    return out_tuple(json.dumps(metrics))\n",
    "\n",
    "check_metrics_comp = kfp.components.create_component_from_func(\n",
    "    func=check_metrics, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2fe96e3-5cfc-4723-bf2b-514348a2fab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubernetes.client import ( V1PersistentVolumeClaimVolumeSource, V1Volume, V1VolumeMount)\n",
    "@dsl.pipeline(\n",
    "    name=\"Monitor Credit Risk AI\",\n",
    "    description=\"An example pipeline that monitors the behavior of the AI model within the application\",\n",
    ")\n",
    "def monitor_credit_model_pipeline(time_window_minutes:int=60*24*7,\n",
    "                                  min_rows:int=5\n",
    "                                 ):\n",
    "    def env_var_from_secret(env_var_name: str, secret_name: str, secret_key: str) -> V1EnvVar:\n",
    "        return V1EnvVar(name=env_var_name,\n",
    "                                     value_from=V1EnvVarSource(\n",
    "                                         secret_key_ref=V1SecretKeySelector(\n",
    "                                             name=secret_name,\n",
    "                                             key=secret_key\n",
    "                                         )\n",
    "                                     )\n",
    "                                    )\n",
    "    \n",
    "    def add_pg_connection_secrets(pipeline_task) -> None:\n",
    "        pipeline_task.container.add_env_variable(V1EnvVar(name=\"PG_HOST\", value=\"postgresql.{{workflow.namespace}}.svc\"))\n",
    "        pipeline_task.container.add_env_variable(env_var_from_secret(\"PG_DB_NAME\", \"postgresql\", \"database-name\"))\n",
    "        pipeline_task.container.add_env_variable(env_var_from_secret(\"PG_USER\", \"postgresql\", \"database-user\"))\n",
    "        pipeline_task.container.add_env_variable(env_var_from_secret(\"PG_PWD\", \"postgresql\", \"database-password\"))\n",
    "        pipeline_task.container.add_env_variable(V1EnvVar(name=\"PG_PORT\", value=\"5432\"))\n",
    "\n",
    "    def add_db2_connection_secrets(pipeline_task) -> None:\n",
    "        pipeline_task.container.add_env_variable(env_var_from_secret(\"db2_host\", \"db2-credentials\", \"host\"))\n",
    "        pipeline_task.container.add_env_variable(env_var_from_secret(\"db2_user\", \"db2-credentials\", \"username\"))\n",
    "        pipeline_task.container.add_env_variable(env_var_from_secret(\"db2_pwd\", \"db2-credentials\", \"password\"))\n",
    "        pipeline_task.container.add_env_variable(env_var_from_secret(\"db2_port\", \"db2-credentials\", \"port\"))\n",
    "\n",
    "    load_reference_data_task = load_df_from_postgresql_comp(table_name=\"TRAIN\",time_window_minutes=None)\n",
    "    load_reference_data_task.set_display_name(\"Load_Reference_Data\")\n",
    "    load_reference_data_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    add_pg_connection_secrets(load_reference_data_task)\n",
    "\n",
    "    load_production_data_task = load_df_from_postgresql_comp(table_name=\"CLIENT_DATA\", \n",
    "                                                      predictions_column='PredictedRisk',\n",
    "                                                      time_window_minutes=time_window_minutes)\n",
    "    load_production_data_task.set_display_name(\"Load_Production_Data\")\n",
    "    load_production_data_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    add_pg_connection_secrets(load_production_data_task)\n",
    "\n",
    "    \n",
    "    with dsl.Condition(load_production_data_task.outputs['num_rows'] > min_rows):\n",
    "        drift_report_task = evidently_report_comp(\n",
    "                                reference_df = load_reference_data_task.outputs[\"data_frame_pkl\"],\n",
    "                                production_df = load_production_data_task.outputs[\"data_frame_pkl\"],\n",
    "                                report_type=\"drift\"\n",
    "        )\n",
    "        drift_report_task.set_display_name(\"Produce Data Drift Report\")\n",
    "\n",
    "        target_drift_report_task = evidently_report_comp(\n",
    "                                reference_df = load_reference_data_task.outputs[\"data_frame_pkl\"],\n",
    "                                production_df = load_production_data_task.outputs[\"data_frame_pkl\"],\n",
    "                                report_type=\"target_drift\"\n",
    "        )\n",
    "        target_drift_report_task.set_display_name(\"Produce Target Drift Report\")\n",
    "\n",
    "        classification_report_task = evidently_classification_report_comp(\n",
    "                                production_df = load_production_data_task.outputs[\"data_frame_pkl\"],\n",
    "        )\n",
    "        classification_report_task.set_display_name(\"Produce classification Report\")\n",
    "\n",
    "\n",
    "        check_metrics_task = check_metrics_comp(\n",
    "                  classification_report=classification_report_task.outputs[\"output_json\"],\n",
    "                      data_drift_report=drift_report_task.outputs[\"output_json\"],\n",
    "                      target_drift_report=target_drift_report_task.outputs[\"output_json\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf3eae2e-ec33-46b5-bbbf-4104409212f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pipeline(pipeline_name: str):\n",
    "    \"\"\"Delete's a pipeline with the specified name\"\"\"\n",
    "\n",
    "    client = kfp.Client()\n",
    "    existing_pipelines = client.list_pipelines(page_size=999).pipelines\n",
    "    matches = (\n",
    "        [ep.id for ep in existing_pipelines if ep.name == pipeline_name]\n",
    "        if existing_pipelines\n",
    "        else []\n",
    "    )\n",
    "    for id in matches:\n",
    "        client.delete_pipeline(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3fba760-5a07-428c-8e58-38ffba93fae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_id(experiment_name: str) -> str:\n",
    "    \"\"\"Returns the id for the experiment, creating the experiment if needed\"\"\"\n",
    "    client = kfp.Client()\n",
    "    existing_experiments = client.list_experiments(page_size=999).experiments\n",
    "    matches = (\n",
    "        [ex.id for ex in existing_experiments if ex.name == experiment_name]\n",
    "        if existing_experiments\n",
    "        else []\n",
    "    )\n",
    "\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "\n",
    "    exp = client.create_experiment(experiment_name)\n",
    "    return exp.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "621931e2-bfa9-42ba-b3b5-0ab466c4b02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_conf = kfp.dsl.PipelineConf()\n",
    "\n",
    "def provide_column_info_transformer(op: dsl.ContainerOp):\n",
    "    \n",
    "    if isinstance(op, dsl.ContainerOp):\n",
    "        op.container.add_env_variable(\n",
    "            V1EnvVar(name=\"COLUMNS\",\n",
    "                    value_from=V1EnvVarSource(\n",
    "                                         config_map_key_ref=V1ConfigMapKeySelector(\n",
    "                                             name=\"credit-risk-columns\",\n",
    "                                             key=\"columns\"\n",
    "                                         )\n",
    "                                     )\n",
    "                    )\n",
    "        )\n",
    "                            \n",
    "\n",
    "pipeline_conf.add_op_transformer(provide_column_info_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28f9c95a-8496-4d3f-8509-fe27271b1cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=/pipeline/#/pipelines/details/2d20a6d6-8163-4279-a659-2a543962e75c>Pipeline details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PIPELINE_NAME = \"Monitor_Credit_Risk_AI\"\n",
    "# Pipeline names need to be unique, so before we upload,\n",
    "# check for and delete any pipeline with the same name\n",
    "delete_pipeline(PIPELINE_NAME)\n",
    "\n",
    "        \n",
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=monitor_credit_model_pipeline,\n",
    "    package_path=f\"{PIPELINE_NAME}.yaml\",\n",
    "    pipeline_conf=pipeline_conf,\n",
    ")\n",
    "\n",
    "# upload\n",
    "client = kfp.Client()\n",
    "uploaded_pipeline = client.upload_pipeline(f\"{PIPELINE_NAME}.yaml\", PIPELINE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "665d4723-bf14-4a7f-bc15-38bc0b712e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'created_at': datetime.datetime(2023, 12, 5, 19, 31, 27, tzinfo=tzlocal()),\n",
      " 'default_version': {'code_source_url': None,\n",
      "                     'created_at': datetime.datetime(2023, 12, 5, 19, 31, 27, tzinfo=tzlocal()),\n",
      "                     'description': None,\n",
      "                     'id': '2d20a6d6-8163-4279-a659-2a543962e75c',\n",
      "                     'name': 'Monitor_Credit_Risk_AI',\n",
      "                     'package_url': None,\n",
      "                     'parameters': [{'name': 'time_window_minutes',\n",
      "                                     'value': '10080'},\n",
      "                                    {'name': 'min_rows', 'value': '5'}],\n",
      "                     'resource_references': [{'key': {'id': '2d20a6d6-8163-4279-a659-2a543962e75c',\n",
      "                                                      'type': 'PIPELINE'},\n",
      "                                              'name': None,\n",
      "                                              'relationship': 'OWNER'}]},\n",
      " 'description': None,\n",
      " 'error': None,\n",
      " 'id': '2d20a6d6-8163-4279-a659-2a543962e75c',\n",
      " 'name': 'Monitor_Credit_Risk_AI',\n",
      " 'parameters': [{'name': 'time_window_minutes', 'value': '10080'},\n",
      "                {'name': 'min_rows', 'value': '5'}],\n",
      " 'resource_references': None,\n",
      " 'url': None}\n"
     ]
    }
   ],
   "source": [
    "print(uploaded_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17616f00-8eff-41a0-b6c5-c9204a65155b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://github.com/kubeflow/pipelines/blob/1.7.1/sdk/python/kfp/_client.py\n",
    "# https://pkg.go.dev/github.com/robfig/cron#hdr-CRON_Expression_Format\n",
    "_ = client.create_recurring_run(experiment_id=get_experiment_id(\"monitor-production-credit\"),\n",
    "                                job_name=\"monitor_credit_risk_api_performance\",\n",
    "                                description=\"Tests for data drift and f1 performance\",\n",
    "                                cron_expression=\"0 0 0-23 ? JAN-DEC MON-FRI\",\n",
    "                                pipeline_id=uploaded_pipeline.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "829b7962-9443-4b0b-a215-cc74e44897d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                             AGE\n",
      "monitorcreditriskapiperfob5n56   21h\n",
      "monitorcreditriskapiperfokl6hz   1s\n"
     ]
    }
   ],
   "source": [
    "!oc get scheduledworkflows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7bbb19-5329-47b5-a35e-7b86332bb0b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
