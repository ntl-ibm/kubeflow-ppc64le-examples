{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76508eec-bd22-4931-b5d3-393ca6e9cf3e",
   "metadata": {},
   "source": [
    "# Using PVCs in Kubeflow pipeline components\n",
    "\n",
    "These example pipelines describe how to use PVCs and MinIO within kubeflow pipelines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab47228f-66f8-4c3f-8d95-73568c92af0b",
   "metadata": {},
   "source": [
    "## Setup & common includes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c6aecd3-136f-4697-b8e5-201c328fc11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "from kubernetes.client.models import (\n",
    "    V1Volume,\n",
    "    V1VolumeMount,\n",
    "    V1PersistentVolumeClaimVolumeSource,\n",
    ")\n",
    "import ruamel.yaml\n",
    "import os\n",
    "from kfp.dsl import data_passing_methods\n",
    "import subprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e0fa0f7-df80-4b1a-bb8e-27b5fb3833a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_IMAGE = \"quay.io/ibm/kubeflow-notebook-image-ppc64le:elyra3.14.1-py3.9-tf2.9.2-pt1.12.1-v0\"\n",
    "COMPONENT_CATALOG_FOLDER = (\n",
    "    f\"{os.getenv('HOME')}/components\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b481d44-4564-45c7-a7ef-da632ec9111e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pipeline(pipeline_name: str):\n",
    "    \"\"\"Delete's a pipeline with the specified name\"\"\"\n",
    "\n",
    "    client = kfp.Client()\n",
    "    existing_pipelines = client.list_pipelines().pipelines\n",
    "    matches = (\n",
    "        [ep.id for ep in existing_pipelines if ep.name == pipeline_name]\n",
    "        if existing_pipelines\n",
    "        else []\n",
    "    )\n",
    "    for id in matches:\n",
    "        client.delete_pipeline(id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcb44798-266a-476f-a112-dc6724f74320",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68ea6b2-2190-4e02-a5ef-020c783348d6",
   "metadata": {},
   "source": [
    "## Pipeline #1: Store a model with MinIO\n",
    "\n",
    "This example shows how to upload a model to MinIO. Uploading to MinIO is the most common way to store a model, and our examples are setup that way.\n",
    "\n",
    "The example pipeline simplifies the pre-processing and training steps by downloading an ONNX model from the web. The model is then uploaded to MinIO using a reusable component that was built by IBM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0334844-f3c4-444f-9e3d-cf558822741a",
   "metadata": {},
   "source": [
    "### Comonent to download a model\n",
    "\n",
    "This gives us an output parameter that is a model. In the real world, this model would be something that has been trained by the pipeline. This component just downloads a model to keep things simpler. The interesting part of the example is the upload component, not the creation of the model that is defined by this component.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca263544-c74d-4e61-a305-b102b9bab6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_model(model_name: str, model: kfp.components.OutputPath(str)):\n",
    "    import urllib.request\n",
    "    from pathlib import Path\n",
    "    import os\n",
    "\n",
    "    MODEL_URL = (\n",
    "        \"https://github.com/onnx/models/raw/main/vision/classification/resnet/model\"\n",
    "    )\n",
    "\n",
    "    Path(os.path.dirname(model)).mkdir(parents=True, exist_ok=True)\n",
    "    urllib.request.urlretrieve(f\"{MODEL_URL}/{model_name}.onnx\", f\"{model}\")\n",
    "\n",
    "\n",
    "download_model_comp = kfp.components.create_component_from_func(\n",
    "    func=download_model, base_image=BASE_IMAGE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8d1e71-339b-4197-aca1-45330ada756e",
   "metadata": {},
   "source": [
    "### Component to upload the model to MinIO\n",
    "\n",
    "The example component provided by IBM provides the logic to both upload a model to MinIO (using the S3 protocol), and to store the model so that it can be served by an InferenceService.\n",
    "This example pipeline does not actually serve the model, but in a complete pipeline deploying the inference service is common.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4152c967-da84-4195-8f3e-7ad14c1a5672",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "UPLOAD_MODEL_COMPONENT = (\n",
    "    f\"{COMPONENT_CATALOG_FOLDER}/model-building/upload-model/component.yaml\"\n",
    ")\n",
    "\n",
    "upload_model_comp = kfp.components.load_component_from_file(UPLOAD_MODEL_COMPONENT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c8f826-5439-4523-b74e-434bbe47d588",
   "metadata": {},
   "source": [
    "### Define the pipeline\n",
    "\n",
    "There are two components in the pipeline:\n",
    "\n",
    "- download the model\n",
    "- upload the model to MinIO.\n",
    "\n",
    "MinIO is also used by Kubeflow to pass data between components. For example the output \"model\" parameter of download_model_comp (an OutputPath) is stored in MinIO.\n",
    "\n",
    "The special value `{{worflow.namespace}}` can be used to instruct the pipeline to use the namespace as the export bucket.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75b262f1-44cf-432c-8e2b-9248ef03f5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"upload-model-pipeline\")\n",
    "def upload_model_pipeline(model_name=\"resnet101-v2-7\"):\n",
    "    download_model_task = download_model_comp(model_name=model_name)\n",
    "\n",
    "    upload_model_task = upload_model_comp(\n",
    "        download_model_task.outputs[\"model\"],\n",
    "        minio_url=\"minio-service.kubeflow:9000\",\n",
    "        export_bucket=\"{{workflow.namespace}}\",\n",
    "        model_format=\"onnx\",\n",
    "        model_name=model_name,\n",
    "        model_version=1,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b45343-22a0-449e-957b-ed9637098da8",
   "metadata": {},
   "source": [
    "### Upload the pipeline\n",
    "\n",
    "Compile the pipeline, with no configuration options, and it upload to Kubeflow.\n",
    "\n",
    "If you want to run the pipeline, you can do that using the link that appears in the output of the cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "751b798d-7f10-4682-a09d-08e91516ca9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=/pipeline/#/pipelines/details/d43e787b-e484-4405-8b7a-af2bfe00b14d>Pipeline details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PIPELINE_NAME = \"upload-model-pipeline\"\n",
    "\n",
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=upload_model_pipeline, package_path=f\"{PIPELINE_NAME}.yaml\"\n",
    ")\n",
    "\n",
    "\n",
    "# Pipeline names need to be unique, so before we upload,\n",
    "# check for and delete any pipeline with the same name\n",
    "delete_pipeline(PIPELINE_NAME)\n",
    "\n",
    "# upload\n",
    "uploaed_pipeline = client.upload_pipeline(f\"{PIPELINE_NAME}.yaml\", PIPELINE_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d436b298-5f10-4be1-afdc-4dc04a9d900a",
   "metadata": {},
   "source": [
    "## Pipeline #2: Use a PVC to pass parameters\n",
    "\n",
    "Using the default MinIO storage to pass parameters between components is convenient and often recommended, however it is not as efficient when compared to using a PVC to pass data between components.\n",
    "\n",
    "This pipeline modifies the previous pipeine and configuration so that Kubeflow uses a PVC when exchanging data between components. The model is uploaded to MinIO by the pipeline.\n",
    "\n",
    "The term \"blackboard\" is used in the examples to describe the PVC that is used for information sharing between pipeline tasks. The term comes from the blackboard design pattern and is not Kubeflow specific terminology.\n",
    "\n",
    "In this pattern, the pipeline creates the PVC used for passing information, and the PVC is destroyed when the pipeline is destoryed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd240cf4-957b-4f7e-91dc-044a7b42ba9a",
   "metadata": {},
   "source": [
    "### Pipeline definition\n",
    "\n",
    "The pipeline creates the PVC used for information exchange within the pipeline.\n",
    "\n",
    "- A `VolumeOp` is a pipeline component that creates the PVC that is used for passing data.\n",
    "- The VolumeOp has `set_owner_reference=True` so that the PVC is destoryed when the pipeline is destroyed.\n",
    "- The initial tasks in the pipeline are created with an `after` method call to ensure the PVC is created before the tasks, so that the PVC can be (implicitly) used to store the input and output parameters of the task.\n",
    "\n",
    "Creating the PVC does not instruct Kubeflow to use it for passing data. A pipeline configuration will be created in the following cells that contains those instructions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18abcef0-591e-4f8e-bb7b-40d9a82f4c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"upload-model-with-blackboard\")\n",
    "def upload_model_with_blackboard(\n",
    "    blackboard: str = \"mlpipeline-artefacts\", model_name: str = \"resnet101-v2-7\"\n",
    "):\n",
    "\n",
    "    create_blackboard = dsl.VolumeOp(\n",
    "        name=\"Create Artefacts Blackboard\",\n",
    "        resource_name=blackboard,\n",
    "        modes=dsl.VOLUME_MODE_RWO,\n",
    "        size=\"4Gi\",\n",
    "        set_owner_reference=True,\n",
    "    )\n",
    "\n",
    "    download_model_task = download_model_comp(model_name=model_name)\n",
    "    download_model_task.after(create_blackboard)\n",
    "\n",
    "    upload_model_task = upload_model_comp(\n",
    "        download_model_task.outputs[\"model\"],\n",
    "        minio_url=\"minio-service.kubeflow:9000\",\n",
    "        export_bucket=\"{{workflow.namespace}}\",\n",
    "        model_format=\"onnx\",\n",
    "        model_name=model_name,\n",
    "        model_version=1,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015b7aa5-5042-4ef0-9a3e-c457092be09b",
   "metadata": {},
   "source": [
    "### Pipeline configuration\n",
    "\n",
    "The pipeline needs additional configuration for this pattern to work. The configuration specifies the PVC to use, but does not cause the PVC to be created (the pipeline definition creates the PVC).\n",
    "\n",
    "#### Data Passing Method\n",
    "\n",
    "The `data_passing_method` needs to be set to use the PVC for passing data between pipeline tasks.\n",
    "\n",
    "- `{{workflow.name}}` is a special value that will be replaced with the unique workflow id when the pipeline is executed. This avoids name conflicts on the volume name.\n",
    "- The name of the volume must match the resource name in the `VolumeOp` that used for the blackboard in the pipeline. This is how the pipeline refers to the volume, rather than the physical name.\n",
    "\n",
    "#### Disable Caching\n",
    "\n",
    "The PVC used for passing data between components will be different on each pipeline run. As a result, values cannot be cached between pipeline runs. Caching is disabled by defining the `disable_cache_transformer` function and adding the transformer to pipeline configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0b1a859-dc0f-4e9a-a851-75765493c118",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_conf = kfp.dsl.PipelineConf()\n",
    "\n",
    "# Use PVC for passing data between pipeline tasks\n",
    "pipeline_conf.data_passing_method = kfp.dsl.data_passing_methods.KubernetesVolume(\n",
    "    volume=V1Volume(\n",
    "        name=\"mlpipeline-artefacts\",\n",
    "        persistent_volume_claim=V1PersistentVolumeClaimVolumeSource(\n",
    "            \"{{workflow.name}}-mlpipeline-artefacts\"\n",
    "        ),\n",
    "    ),\n",
    "    path_prefix=\"mlpipeline-artefacts/\",\n",
    ")\n",
    "\n",
    "\n",
    "# Disable Caching\n",
    "def disable_cache_transformer(op: dsl.ContainerOp):\n",
    "    if isinstance(op, dsl.ContainerOp):\n",
    "        op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    else:\n",
    "        op.add_pod_annotation(\n",
    "            name=\"pipelines.kubeflow.org/max_cache_staleness\", value=\"P0D\"\n",
    "        )\n",
    "    return op\n",
    "\n",
    "\n",
    "pipeline_conf.add_op_transformer(disable_cache_transformer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08067d0-98e5-49ba-9075-b6908dee96e8",
   "metadata": {},
   "source": [
    "### Compile and upload the pipeline\n",
    "\n",
    "The difference between this step and prior pipelines is that the pipeline configuration is added to the `compile` method call.\n",
    "\n",
    "If you want to run the pipeline, you can do that using the link that appears in the output of the cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "890b1545-08fb-405f-bcab-9babdc35db0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=/pipeline/#/pipelines/details/727e2849-6529-4cf4-ab89-1e7893b3de8b>Pipeline details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PIPELINE_NAME = \"upload-model-with-blackboard\"\n",
    "\n",
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=upload_model_with_blackboard,\n",
    "    package_path=f\"{PIPELINE_NAME}.yaml\",\n",
    "    pipeline_conf=pipeline_conf,\n",
    ")\n",
    "\n",
    "\n",
    "# Pipeline names need to be unique, so before we upload,\n",
    "# check for and delete any pipeline with the same name\n",
    "delete_pipeline(PIPELINE_NAME)\n",
    "\n",
    "# upload\n",
    "uploaed_pipeline = client.upload_pipeline(f\"{PIPELINE_NAME}.yaml\", PIPELINE_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e09d72e-e4e9-4e84-9a1f-6ff0a4631ed7",
   "metadata": {},
   "source": [
    "### Example pipeline #3: Define pipeline tasks that mount a PVC as a file system\n",
    "\n",
    "The prior approaches are recommended for passing data between pipeline components. Mounting a PVC to a pipeline task makes the pipeline less portable. It also makes it more difficult to reuse existing pipeline components.\n",
    "\n",
    "Although not preferred, there are cases where mounting PVC can be useful, since it allows data to be stored and reused across multiple pipelines.\n",
    "\n",
    "This pipeline is a simple example that shows how to download an image to a PVC. The PVC is assumed to have been created outside of the pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33631ab-c2de-4dd9-b349-f5675f40ae4e",
   "metadata": {},
   "source": [
    "### Create the PVC\n",
    "\n",
    "There are several ways to create a PVC. The simpliest is to select the volumes tab on the Kubeflow Central Dashboard. There will be a \"New Volume\" button that can be used to create a new volume.\n",
    "\n",
    "The name of the volume will be the name of the PVC, and we'll use this to mount the volume when we create the pipeline definition.\n",
    "\n",
    "This figure shows how to create a volume in using the Kubeflow dashboard.\n",
    "\n",
    "<img src=\"./volumes.jpg\" />\n",
    "\n",
    "Since the volume might be used by multiple tasks at the same time, it is best to choose an access mode of \"ReadWriteMany\".\n",
    "\n",
    "<img src=\"./new-vol-prompt.jpg\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc690ee0-2051-4734-a3ca-ec48f121ff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PVC_NAME = \"my-data-vol\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fb1379-d094-4a85-ab7a-08aa288b75b2",
   "metadata": {},
   "source": [
    "### Component to download an Image\n",
    "\n",
    "Because this component is storing data on a PVC, rather than returning an OutputPath, the ouput_path is a string type input value. This difference means that the output_path parameter cannot be passed as an InputPath to another component.\n",
    "\n",
    "That is why it is recommended that the pattern in the previous example pipelines is used for passing data between components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f0e1a30-ff78-4441-8612-1c2b9e6bd770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(image_url: str, output_path: str):\n",
    "    import urllib.request\n",
    "    from pathlib import Path\n",
    "\n",
    "    Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    urllib.request.urlretrieve(image_url, output_path)\n",
    "\n",
    "\n",
    "download_image_comp = kfp.components.create_component_from_func(\n",
    "    func=download_image, base_image=BASE_IMAGE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30095c5c-4fe3-4d70-b0d2-97090bfd98c7",
   "metadata": {},
   "source": [
    "### Pipeline definition\n",
    "\n",
    "The `add_volume` method is called on the task so that the volume will be mounted as file system when the pipeline executes the task. The claim source must match the PVC that you created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d36d2f47-4779-4ee5-8814-b834aa95e29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"download_image_to_pvc\",\n",
    ")\n",
    "def pipeline_to_download_image_to_pvc():\n",
    "    image_task = download_image_comp(\n",
    "        \"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/Youngkitten.JPG/320px-Youngkitten.JPG\",\n",
    "        \"/mnt/image/youngkitten.JPG\",\n",
    "    )\n",
    "    # add_volume defines the volume for the task, but does not define any\n",
    "    # volume mounts\n",
    "    image_task.add_volume(\n",
    "        V1Volume(\n",
    "            name=\"myvolume\",\n",
    "            # Claim volume source is the name of the Volume\n",
    "            persistent_volume_claim=V1PersistentVolumeClaimVolumeSource(PVC_NAME),\n",
    "        )\n",
    "    )\n",
    "    # Add Volume Mount defines the path on the file system that the volume is \n",
    "    # mounted to.\n",
    "    # Name must match the name in the V1Volume\n",
    "    image_task.add_volume_mount(V1VolumeMount(name=\"myvolume\", mount_path=\"/mnt/image\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fc703b-6d92-4afc-a340-69f54faab0d8",
   "metadata": {},
   "source": [
    "### Compile and upload pipeline\n",
    "\n",
    "The pipeline is compiled with the default options. If there were multiple components, MinIO would be used to share information between them.\n",
    "\n",
    "If you want to run the pipeline, you can do that using the link that appears in the output of the cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "438579ae-c319-4411-8fdd-326c29e26819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=/pipeline/#/pipelines/details/e3d4f3a9-5af0-47bd-8c90-d04791d36d44>Pipeline details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PIPELINE_NAME = \"download_image_to_pvc\"\n",
    "\n",
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline_to_download_image_to_pvc,\n",
    "    package_path=f\"{PIPELINE_NAME}.yaml\",\n",
    ")\n",
    "\n",
    "# Pipeline names need to be unique, so before we upload,\n",
    "# check for and delete any pipeline with the same name\n",
    "delete_pipeline(PIPELINE_NAME)\n",
    "\n",
    "# upload\n",
    "uploaed_pipeline = client.upload_pipeline(f\"{PIPELINE_NAME}.yaml\", PIPELINE_NAME)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
