{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6378d27-5071-434b-9b4b-fbcac7fc530b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04421004-ced7-4383-bf36-69e2e8bf032c",
   "metadata": {},
   "source": [
    "# Optimizing Inference for Image Classification\n",
    "\n",
    "This notebook shows how to deploy an inference service on a GPU with auto scaling.\n",
    "The pipeline will train and deploy a model for classifying images into one of 10 species of Monkeys.\n",
    "\n",
    "**References**\n",
    "\n",
    "This notebook is inspired by the prior work of Sebastian Lehrig and Marvin Giessing. \n",
    "https://github.com/lehrig/kubeflow-ppc64le-examples/blob/main/computer-vision/monkey-classification/Monkey%20Classification.ipynb\n",
    "\n",
    "Comments and questions should be addressed to Nick Lawrence (ntl@us.ibm.com)\n",
    "\n",
    "**License**\n",
    "\n",
    "Apache-2.0 License"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddbeb0d9-a768-4908-ae42-0dbb8142e21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import kfp\n",
    "from kfp.components import InputPath, OutputPath\n",
    "import kfp.dsl as dsl\n",
    "from kfp.dsl import PipelineConf, data_passing_methods\n",
    "from kubernetes.client.models import V1Volume, V1PersistentVolumeClaimVolumeSource\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import List, NamedTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a848e6e-f039-44fd-9d3b-4760f3298693",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPONENT_CATALOG_FOLDER = f\"{os.getenv('HOME')}/components\"\n",
    "UPDATED_COMPONENT_CATALOG_FOLDER = (\n",
    "    f\"{os.getenv('HOME')}/kf-inference-example/components\"\n",
    ")\n",
    "# COMPONENT_CATALOG_GIT = \"https://github.com/lehrig/kubeflow-ppc64le-components.git\"\n",
    "# COMPONENT_CATALOG_RELEASE = \"main\"\n",
    "\n",
    "DEPLOY_MODEL_WITH_KSERVE_COMPONENT = f\"{COMPONENT_CATALOG_FOLDER}/model-deployment/deploy-model-with-kserve/component.yaml\"\n",
    "\n",
    "BASE_IMAGE = (\n",
    "    \"quay.io/ibm/kubeflow-notebook-image-ppc64le:elyra3.14.1-py3.9-tf2.9.2-pt1.12.1-v0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b240d542-6ab4-4696-ae36-605072bd2fe8",
   "metadata": {},
   "source": [
    "## Import directories of reusable Kubeflow components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "156a97af-d82e-4c4b-9611-3d0205c727f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not os.path.exists(COMPONENT_CATALOG_FOLDER):\n",
    "#    !git clone --branch $COMPONENT_CATALOG_RELEASE $COMPONENT_CATALOG_GIT $COMPONENT_CATALOG_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b309145-8bb5-472c-aeaf-737983671c0b",
   "metadata": {},
   "source": [
    "## Load and prepare dataset component\n",
    "\n",
    "This component will load the data set from huggingface, preprocess it, and save it as a tensorflow model for training.\n",
    "\n",
    "The preprocessing step coverts the images to tensors, and expands the class labels to one-hot vectors. The training images are all of the same size and can be batched without needing to resize here. Rescaling is also done as a layer of the model itself.\n",
    "\n",
    "We want the model to include resize and rescale layers for two reasons:\n",
    "\n",
    "* Inferencing can reuse these layers later without needing additional code\n",
    "* The model training component will run on the GPU, and performace of these layers will be enhanced.\n",
    "\n",
    "This component also outputs the class labels in a file for later use. These are not needed for training, but the inference service can use them to present a more readable response to the client.\n",
    "\n",
    "This component also splits the data into train/validation/test data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "687f9364-872e-405f-87c3-3d02d86dd41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_train_dataset_for_tf(\n",
    "    train_dataset_dir: OutputPath(str),\n",
    "    validation_dataset_dir: OutputPath(str),\n",
    "    test_dataset_dir: OutputPath(str),\n",
    "    class_names: OutputPath(str),\n",
    "    dataset_url=\"Lehrig/Monkey-Species-Collection\",\n",
    "    dataset_configuration=\"downsized\",\n",
    "):\n",
    "    import os\n",
    "\n",
    "    import datasets\n",
    "    import json\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    import base64\n",
    "\n",
    "    def process(examples):\n",
    "        examples[\"image\"] = [np.array(img) for img in examples[\"image\"]]\n",
    "\n",
    "        examples[\"label\"] = [ONE_HOT_MATRIX[label] for label in examples[\"label\"]]\n",
    "\n",
    "        return examples\n",
    "\n",
    "    def save_as_tf_dataset(dataset, directory):\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        tf.data.experimental.save(\n",
    "            dataset.to_tf_dataset(\n",
    "                batch_size=16, columns=[\"image\"], label_cols=[\"label\"]\n",
    "            ),\n",
    "            directory,\n",
    "        )\n",
    "\n",
    "    dataset = datasets.load_dataset(dataset_url, dataset_configuration)\n",
    "\n",
    "    CLASSES = dataset[\"train\"].features[\"label\"].names\n",
    "    ONE_HOT_MATRIX = np.identity(len(CLASSES))\n",
    "\n",
    "    dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "    # The input dataset is known to have images of shape (224, 224, 3)\n",
    "    # The images are all of the same size, and so they can be batched\n",
    "    # together. The model will have a resizing layer so that inference will\n",
    "    # resize the input images if necessary, but there is an assumption\n",
    "    # that images in the batch are all the same size for both\n",
    "    # training and inferencing. We assume that is true with the\n",
    "    # hugging face data and don't resize here.\n",
    "    dataset = dataset.map(\n",
    "        process,\n",
    "        batched=True,\n",
    "        batch_size=16,\n",
    "        features=datasets.Features(\n",
    "            {\n",
    "                \"image\": datasets.Array3D(dtype=\"uint8\", shape=(224, 224, 3)),\n",
    "                \"label\": datasets.Sequence(\n",
    "                    feature=datasets.Value(dtype=\"int32\"), length=len(CLASSES)\n",
    "                ),\n",
    "            }\n",
    "        ),\n",
    "        num_proc=8,\n",
    "    )\n",
    "\n",
    "    dev_test_dataset = dataset[\"test\"].train_test_split(test_size=0.5, shuffle=False)\n",
    "\n",
    "    save_as_tf_dataset(dataset[\"train\"], train_dataset_dir)\n",
    "    save_as_tf_dataset(dev_test_dataset[\"train\"], validation_dataset_dir)\n",
    "    save_as_tf_dataset(dev_test_dataset[\"test\"], test_dataset_dir)\n",
    "\n",
    "    os.makedirs(os.path.dirname(class_names), exist_ok=True)\n",
    "    with open(class_names, \"w\") as f:\n",
    "        json.dump(CLASSES, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c73da6d3-bc52-4d5e-9a48-23ee62d2a511",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data_comp = kfp.components.create_component_from_func(\n",
    "    load_test_train_dataset_for_tf, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8ac6cc-11d2-445d-b8cd-f40da647b15f",
   "metadata": {},
   "source": [
    "## Model training component\n",
    "\n",
    "Several important optimizations are included here.\n",
    "\n",
    "* Resizing and Rescaling are done here to take advantage of the GPU.\n",
    "* Image augmentation layers are included here for accuracy. These also benefit from the GPU. \n",
    "* The dataset is cached and prefetched, so that the GPU does not have to wait for data during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58e848c5-ad13-445c-9635-a93584e15ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    train_dataset_dir: InputPath(str),\n",
    "    validation_dataset_dir: InputPath(str),\n",
    "    model_dir: OutputPath(str),\n",
    "    epochs: int = 100,\n",
    "    batch_size: int = 32,\n",
    "):\n",
    "    \"\"\"Uses transfer learning on a prepared dataset. Once trained, the model is persisted to `model_dir`.\"\"\"\n",
    "    import os\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import Sequential\n",
    "    from tensorflow.keras.applications import InceptionV3\n",
    "    from tensorflow.keras.layers import (\n",
    "        BatchNormalization,\n",
    "        Dense,\n",
    "        Dropout,\n",
    "        GlobalAveragePooling2D,\n",
    "        Resizing,\n",
    "        Rescaling,\n",
    "        RandomContrast,\n",
    "        RandomBrightness,\n",
    "        RandomRotation,\n",
    "        Input,\n",
    "    )\n",
    "\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "    def build_model():\n",
    "\n",
    "        backbone = InceptionV3(include_top=False, weights=\"imagenet\")\n",
    "\n",
    "        for layer in backbone.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(None, None, 3), name=\"image\", dtype=tf.uint8))\n",
    "        model.add(Resizing(height=224, width=224, interpolation=\"nearest\"))\n",
    "        model.add(Rescaling(scale=1.0 / 255))\n",
    "        model.add(RandomContrast(factor=0.2, seed=42))\n",
    "        model.add(RandomBrightness(factor=0.2, value_range=(0.0, 1.0), seed=42))\n",
    "        model.add(RandomRotation(factor=0.2, seed=42))\n",
    "        model.add(backbone)\n",
    "        model.add(GlobalAveragePooling2D())\n",
    "        model.add(Dense(128, activation=\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(64, activation=\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(10, activation=\"softmax\", name=\"scores\"))\n",
    "\n",
    "        return model\n",
    "\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=20, verbose=0, mode=\"min\"),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor=\"val_loss\",\n",
    "            factor=0.1,\n",
    "            patience=7,\n",
    "            verbose=1,\n",
    "            min_delta=0.0001,\n",
    "            mode=\"min\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # Datasets were saved in batches so no need to batch again\n",
    "    train_dataset = (\n",
    "        tf.data.experimental.load(train_dataset_dir).cache().prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    validation_dataset = (\n",
    "        tf.data.experimental.load(validation_dataset_dir)\n",
    "        .cache()\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    model = build_model()\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"categorical_accuracy\"],\n",
    "    )\n",
    "\n",
    "    hist = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=validation_dataset,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "\n",
    "    print(\"Model train history:\")\n",
    "    print(hist.history)\n",
    "\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    model.save(model_dir)\n",
    "    print(f\"Model saved to: {model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd25cfb4-24a1-4f65-ba56-b9c205e649d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_comp = kfp.components.create_component_from_func(\n",
    "    train_model, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e6a0fb-f655-4cd5-b904-d554bce52457",
   "metadata": {},
   "source": [
    "## Evaluate model component\n",
    "The model is evaluated against the test dataset using a tensorflow evaluate call.\n",
    "\n",
    "The results are output from the component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d302e540-140d-4430-8551-818c54d1dd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline metrics have problems in Kubeflow 1.6.0\n",
    "# so using ui metadata here instead\n",
    "# https://github.com/kubeflow/pipelines/issues/8356\n",
    "def evaluate_model(\n",
    "    test_dataset_dir: InputPath(str),\n",
    "    mlpipeline_ui_metadata_path: OutputPath(str),\n",
    "    model_dir: InputPath(str),\n",
    "    batch_size: int = 20,\n",
    "):\n",
    "    import json\n",
    "    import tensorflow as tf\n",
    "    import os\n",
    "\n",
    "    test_dataset = tf.data.experimental.load(test_dataset_dir)\n",
    "    model = tf.keras.models.load_model(model_dir)\n",
    "    (loss, accuracy) = model.evaluate(test_dataset)\n",
    "\n",
    "    print((loss, accuracy))\n",
    "\n",
    "    metadata = {\n",
    "        \"outputs\": [\n",
    "            {\n",
    "                \"type\": \"table\",\n",
    "                \"storage\": \"inline\",\n",
    "                \"format\": \"csv\",\n",
    "                \"header\": [\"Loss\", \"Accuracy\"],\n",
    "                \"source\": f\"{loss},{accuracy}\",\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    with open(mlpipeline_ui_metadata_path, \"w\") as f:\n",
    "        json.dump(metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ac9fc38-3015-4860-8ac1-d44b793e4efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_comp = kfp.components.create_component_from_func(\n",
    "    func=evaluate_model, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbaf518-7c52-4d30-b4b9-b6f735c82880",
   "metadata": {},
   "source": [
    "## Convert the model to ONNX component\n",
    "Once the model is in ONNX format, we can use common tools without a dependency on TensorFlow technologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fffb231a-ad40-4f29-8f7e-d0fafcef4a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONVERT_MODEL_TO_ONNX_COMPONENT = (\n",
    "    f\"{COMPONENT_CATALOG_FOLDER}/model-building/convert-to-onnx/component.yaml\"\n",
    ")\n",
    "\n",
    "convert_model_to_onnx_comp = kfp.components.load_component_from_file(\n",
    "    CONVERT_MODEL_TO_ONNX_COMPONENT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f9842b-1f1f-4290-b311-8d3e9513ee71",
   "metadata": {},
   "source": [
    "## Upload model to MinIO component\n",
    "\n",
    "S3 buckets are the most common way to store ML models. We use MinIO, which is installed on the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "843028ef-1c7e-4d36-8dc1-8066f205bf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "UPLOAD_MODEL_COMPONENT = (\n",
    "    f\"{COMPONENT_CATALOG_FOLDER}/model-building/upload-model/component.yaml\"\n",
    ")\n",
    "\n",
    "upload_model_comp = kfp.components.load_component_from_file(UPLOAD_MODEL_COMPONENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44358dda-03d3-4eba-8ca7-9cd63ad55999",
   "metadata": {},
   "source": [
    "## Create Configmap with class names\n",
    "\n",
    "The config map is used to set environment variables for the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b474a2e-0f93-4e9b-a28d-61a711b91bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_config_map(name: str, namespace: str, class_names: InputPath(str)):\n",
    "    from kubernetes import client, config\n",
    "\n",
    "    config.load_incluster_config()\n",
    "\n",
    "    with open(class_names, \"r\") as f:\n",
    "        class_names_json = f.read()\n",
    "\n",
    "    metadata = client.V1ObjectMeta(name=name, namespace=namespace)\n",
    "    # https://github.com/kubernetes-client/python/blob/master/kubernetes/docs/V1ConfigMap.md\n",
    "    configmap = client.V1ConfigMap(\n",
    "        api_version=\"v1\",\n",
    "        kind=\"ConfigMap\",\n",
    "        metadata=metadata,\n",
    "        data={\"CLASS_LABELS\": class_names_json},\n",
    "    )\n",
    "\n",
    "    api_instance = client.CoreV1Api(client.ApiClient())\n",
    "    try:\n",
    "        api_instance.delete_namespaced_config_map(name=name, namespace=namespace)\n",
    "    except Exception as ex:\n",
    "        pass\n",
    "\n",
    "    api_instance.create_namespaced_config_map(\n",
    "        namespace=namespace,\n",
    "        body=configmap,\n",
    "    )\n",
    "\n",
    "\n",
    "create_config_map_comp = kfp.components.create_component_from_func(\n",
    "    func=create_config_map, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c81cf7e-62f5-4afa-bd96-924c1664a6d9",
   "metadata": {},
   "source": [
    "## Component to deploy the inference service\n",
    "\n",
    "This component builds the K8S resource templates for our inference service. It has been customized to include a transformer and autoscaing options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e265e289-8f8d-4c47-b4bb-645b87f12577",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPLOY_INFERENCE_SERVICE_COMPONENT = f\"{os.getenv('HOME')}/kubeflow-ppc64le-examples/deploy_triton_inference_service_component/deploy_triton_inference_service_component.yaml\"\n",
    "\n",
    "deploy_inference_service_comp = kfp.components.load_component_from_file(\n",
    "    DEPLOY_INFERENCE_SERVICE_COMPONENT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd8a651-ba5d-4936-96ab-c304e2ab9507",
   "metadata": {},
   "source": [
    "## Define the Kubeflow Pipeline\n",
    "\n",
    "This pipeline uses MinIO for parameter passing. This allows it to cache the results of training the model, which makes testing the deployment faster when the model has not changed since the last run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "637c19fb-2ee3-44dd-9942-68a4c3437f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"End-to-end monkey species classification pipeline\",\n",
    "    description=\"An example pipeline that performs an image classification and determines different monkey species\",\n",
    ")\n",
    "def monkey_pipeline(\n",
    "    model_name: str = \"monkey-classification\",\n",
    "    model_version: int = 1,\n",
    "    minio_url: str = \"minio-service.kubeflow:9000\",\n",
    "):\n",
    "    load_dataset_task = load_data_comp()\n",
    "\n",
    "    train_model_task = train_model_comp(\n",
    "        train_dataset_dir=load_dataset_task.outputs[\"train_dataset_dir\"],\n",
    "        validation_dataset_dir=load_dataset_task.outputs[\"validation_dataset_dir\"],\n",
    "    )\n",
    "    train_model_task.set_gpu_limit(1)\n",
    "    evaluate_model_task = evaluate_model_comp(\n",
    "        load_dataset_task.outputs[\"test_dataset_dir\"],\n",
    "        train_model_task.outputs[\"model_dir\"],\n",
    "    )\n",
    "\n",
    "    convert_model_to_onnx_task = convert_model_to_onnx_comp(\n",
    "        model_dir=train_model_task.outputs[\"model_dir\"]\n",
    "    )\n",
    "\n",
    "    upload_model_task = upload_model_comp(\n",
    "        convert_model_to_onnx_task.outputs[\"onnx_model_dir\"],\n",
    "        minio_url=\"minio-service.kubeflow:9000\",\n",
    "        export_bucket=\"{{workflow.namespace}}-models\",\n",
    "        model_format=\"onnx\",\n",
    "        model_name=model_name,\n",
    "        model_version=model_version,\n",
    "    )\n",
    "\n",
    "    create_config_map_task = create_config_map_comp(\n",
    "        name=f\"{model_name}-transformer-env\",\n",
    "        namespace=\"{{workflow.namespace}}\",\n",
    "        class_names=load_dataset_task.outputs[\"class_names\"],\n",
    "    )\n",
    "\n",
    "    transformer_specification = {\n",
    "        \"image\": \"quay.io/ntlawrence/monkeytransform:v4.0\",\n",
    "        \"env_configmap\": f\"{model_name}-transformer-env\",\n",
    "        \"minReplicas\": 1,\n",
    "        \"maxReplicas\": 4,\n",
    "    }\n",
    "\n",
    "    deploy_model_task = deploy_inference_service_comp(\n",
    "        name=model_name,\n",
    "        rm_existing=True,\n",
    "        storage_uri=\"s3://{{workflow.namespace}}-models/onnx/\",\n",
    "        minio_url=minio_url,\n",
    "        concurrency_target=4,  # soft limit, may be exceeded for short periods of time\n",
    "        predictor_min_replicas=0,  # min_replicas supports scale to 0\n",
    "        predictor_max_replicas=4,  # We don't want to scale till we consume all the available GPUs\n",
    "        predictor_protocol=\"grpc-v2\",\n",
    "        # Uncomment these next two lines to use the GPU runtime and allocate GPUs\n",
    "        #triton_runtime_version=\"21.08-py3-gpu\",\n",
    "        #predictor_gpu_allocation=1,  # gpu per replica\n",
    "        transformer_specification=transformer_specification,\n",
    "    )\n",
    "    # For testing, we might need to force just the depoyement to run, even\n",
    "    # if it would otherwise be cached. This next line will force\n",
    "    # that to happen.\n",
    "    deploy_model_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    deploy_model_task.after(upload_model_task)\n",
    "    deploy_model_task.after(create_config_map_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "99f9ad22-ac7c-4ecd-844a-a9db32d16e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_NAME = \"Monkey Classification Pipeline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b41bc676-075c-44be-a924-ccf030df16cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=monkey_pipeline,\n",
    "    package_path=f\"{PIPELINE_NAME}.yaml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f55ba8-6361-4c8f-8794-4810c64ba10f",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "These two functions make it a little easier to manage pipelines and experiments that have already been created in KubeFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "280707e7-80b6-45c3-be2a-d923bdf4fc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pipeline(pipeline_name: str):\n",
    "    \"\"\"Delete's a pipeline with the specified name\"\"\"\n",
    "\n",
    "    client = kfp.Client()\n",
    "    existing_pipelines = client.list_pipelines(page_size=999).pipelines\n",
    "    matches = (\n",
    "        [ep.id for ep in existing_pipelines if ep.name == pipeline_name]\n",
    "        if existing_pipelines\n",
    "        else []\n",
    "    )\n",
    "    for id in matches:\n",
    "        client.delete_pipeline(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c7db529d-e6a7-496b-a9bb-ac5cbc857fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_id(experiment_name: str) -> str:\n",
    "    \"\"\"Returns the id for the experiment, creating the experiment if needed\"\"\"\n",
    "    client = kfp.Client()\n",
    "    existing_experiments = client.list_experiments(page_size=999).experiments\n",
    "    matches = (\n",
    "        [ex.id for ex in existing_experiments if ex.name == experiment_name]\n",
    "        if existing_experiments\n",
    "        else []\n",
    "    )\n",
    "\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "\n",
    "    exp = client.create_experiment(experiment_name)\n",
    "    return exp.id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19905936-5fae-45cf-a7c0-31db0c4791a5",
   "metadata": {},
   "source": [
    "## Upload the Pipeline\n",
    "This will make the pipeline we just compiled appear in the Pipelines panel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d5ca57bc-7613-4d15-b1b8-5e62c6480f3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=/pipeline/#/pipelines/details/7340bdb5-7ecf-4ea5-a31d-762a2f5510d5>Pipeline details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pipeline names need to be unique, so before we upload,\n",
    "# check for and delete any pipeline with the same name\n",
    "delete_pipeline(PIPELINE_NAME)\n",
    "\n",
    "# upload\n",
    "client = kfp.Client()\n",
    "uploaded_pipeline = client.upload_pipeline(f\"{PIPELINE_NAME}.yaml\", PIPELINE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f94d95-bbf4-460c-8a5e-f1e35256a88f",
   "metadata": {},
   "source": [
    "## Run the pipeline\n",
    "Runs the pipeline that was just uploaded. Clicking on the returned link will show the progress of the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e118e826-1941-4a26-be42-c0e6b4abbec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/6c6b6e69-7456-4992-b77d-589a1d6c238b\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = client.run_pipeline(\n",
    "    experiment_id=get_experiment_id(\"monkey-classification-exp\"),\n",
    "    job_name=\"monkey-classification-pipeline\",\n",
    "    pipeline_id=uploaded_pipeline.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9832f892-f0e8-43f3-b602-7baa479e537c",
   "metadata": {},
   "source": [
    "## Wait for completion\n",
    "\n",
    "Wait until the pipeline finishes, for up to 20 Min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9a2a2a59-8807-43d2-9df3-29512a23708f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'Succeeded', 'error': None, 'time': '0:02:59', 'metrics': None}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TWENTY_MIN = 20 * 60\n",
    "result = client.wait_for_run_completion(run.id, timeout=TWENTY_MIN)\n",
    "{\n",
    "    \"status\": result.run.status,\n",
    "    \"error\": result.run.error,\n",
    "    \"time\": str(result.run.finished_at - result.run.created_at),\n",
    "    \"metrics\": result.run.metrics,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079ec769-7488-40d5-97a2-d3c467881104",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "The inference service is now deployed!\n",
    "\n",
    "You can move on to the Inference notebook to learn how to use it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
