{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_script_from_url(\n",
    "    source: str,\n",
    "    dest: str,\n",
    "):\n",
    "    import os\n",
    "    import shutil\n",
    "    from urllib.request import urlretrieve\n",
    "    from urllib.parse import urlparse\n",
    "\n",
    "    # Make target directories if needed\n",
    "    parent_dirs = os.path.dirname(dest)\n",
    "    if not os.path.exists(parent_dirs):\n",
    "        os.makedirs(parent_dirs)\n",
    "\n",
    "    source_details = urlparse(source)\n",
    "\n",
    "    if source_details.scheme == \"file\":\n",
    "        if os.path.isdir(source_details.path):\n",
    "            shutil.copytree(source_details.path, dest)\n",
    "        else:\n",
    "            shutil.copyfile(source_details.path, dest)\n",
    "    elif source_details.scheme in (\"http\", \"https\", \"ftp\", \"ftps\"):\n",
    "        urlretrieve(source, filename=dest)\n",
    "    else:\n",
    "        raise ValueError(f\"source does not use a supported url scheme\")\n",
    "\n",
    "\n",
    "load_script_from_url_comp = kfp.components.create_component_from_func(\n",
    "    load_script_from_url, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_distributed(\n",
    "    model: OutputPath(str),\n",
    "    results: OutputPath(str),\n",
    "    epochs: int,\n",
    "    training_pvc_name: str,\n",
    "    timeout: int = 60 * 60 * 8,\n",
    "    base_image: str = \"quay.io/ntlawrence/pytorchv1.13:1.0\",\n",
    "    replicas: int = 6,\n",
    "):\n",
    "    import os\n",
    "    import subprocess\n",
    "    import shutil\n",
    "\n",
    "    # Install kubeflow-training, hack/fix other dependents\n",
    "    subprocess.run(\"pip uninstall -y kfp\", shell=True)\n",
    "    subprocess.run(\"pip install kubernetes==26.1.0\", shell=True, check=True)\n",
    "    subprocess.run(\"pip install kubeflow-training==1.6.0\", shell=True, check=True)\n",
    "\n",
    "    from kubernetes.client import (\n",
    "        V1PodTemplateSpec,\n",
    "        V1ObjectMeta,\n",
    "        V1PodSpec,\n",
    "        V1Container,\n",
    "        V1EnvVar,\n",
    "        V1ResourceRequirements,\n",
    "        V1VolumeMount,\n",
    "        V1Volume,\n",
    "        V1PersistentVolumeClaimVolumeSource,\n",
    "        V1EmptyDirVolumeSource,\n",
    "        V1OwnerReference,\n",
    "        V1ObjectFieldSelector,\n",
    "        V1EnvVarSource,\n",
    "    )\n",
    "\n",
    "    from kubeflow.training import (\n",
    "        V1ReplicaSpec,\n",
    "        KubeflowOrgV1PyTorchJob,\n",
    "        KubeflowOrgV1PyTorchJobSpec,\n",
    "        KubeflowOrgV1ElasticPolicy,\n",
    "        V1RunPolicy,\n",
    "        TrainingClient,\n",
    "    )\n",
    "\n",
    "    from kubeflow.training.constants import constants\n",
    "    from kubernetes import client, config, watch\n",
    "\n",
    "    def copyf(source: str, dest: str(str)):\n",
    "        \"\"\"\n",
    "        Copies a file or directory,\n",
    "        creating destination parent dirs as needed\n",
    "        \"\"\"\n",
    "        import os\n",
    "        import shutil\n",
    "\n",
    "        parent_dirs = os.path.dirname(dest)\n",
    "        os.makedirs(parent_dirs, exist_ok=True)\n",
    "\n",
    "        if os.path.isdir(source):\n",
    "            shutil.copytree(source, dest)\n",
    "        else:\n",
    "            shutil.copyfile(source, dest)\n",
    "\n",
    "    def wait_for_pod_ready(name: str, namespace: str = \"{{workflow.namespace}}\"):\n",
    "        \"\"\"Waits for a Pod to become ready.\n",
    "        At that point all containers have been started\n",
    "        \"\"\"\n",
    "        config.load_incluster_config()\n",
    "        w = watch.Watch()\n",
    "        core_v1 = client.CoreV1Api()\n",
    "\n",
    "        # Watching a specific pod is done with a field selector on the name.\n",
    "        # https://github.com/kubernetes-client/python/issues/467\n",
    "        for event in w.stream(\n",
    "            func=core_v1.list_namespaced_pod,\n",
    "            namespace=namespace,\n",
    "            field_selector=f\"metadata.name={name}\",\n",
    "            timeout_seconds=120,\n",
    "        ):\n",
    "            # Phases: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase\n",
    "            if event[\"object\"].status.phase not in {\"Pending\"}:\n",
    "                w.stop()\n",
    "                return\n",
    "            # event.type: ADDED, MODIFIED, DELETED\n",
    "            if event[\"type\"] == \"DELETED\":\n",
    "                print(f\" {name} deleted before it started\")\n",
    "                w.stop()\n",
    "                return\n",
    "\n",
    "    # The \"training pvc is used to share data between the torch jobs and this component\n",
    "    # We need to copy the config and initial weights to it\n",
    "    copyf(model_cfg_path, f\"{training_pvc_mount}/input/config.yaml\")\n",
    "    copyf(initial_weights_path, f\"{training_pvc_mount}/input/weights.pt\")\n",
    "\n",
    "    # Create a directory to store the output of training, we'll\n",
    "    # copy data from here to output paths\n",
    "    os.makedirs(f\"{training_pvc_mount}/output\", exist_ok=True)\n",
    "\n",
    "    ###########################################\n",
    "    # PyTorchJob template for the training job\n",
    "    ###########################################\n",
    "\n",
    "    # Define GPU device list and job name\n",
    "    devices = \",\".join([str(d) for d in range(gpus)])\n",
    "    job_name = \"{{workflow.name}}-training\"\n",
    "    batch_size = batch_size * replicas\n",
    "\n",
    "    # An owner reference for the workflow\n",
    "    # When the workflow is deleted, the torch job is\n",
    "    # garbage collected.\n",
    "    workflow_ownership = V1OwnerReference(\n",
    "        api_version=\"v1\",\n",
    "        kind=\"Workflow\",\n",
    "        name=\"{{workflow.name}}\",\n",
    "        uid=\"{{workflow.uid}}\",\n",
    "    )\n",
    "\n",
    "    # Pod definition for each worker replica\n",
    "    # Defines the container image, command, and volume mounts\n",
    "    # yolov5 handles the distributed training for us, but\n",
    "    # there is an example of how to write your own\n",
    "    # training code here: https://github.com/kubeflow/training-operator/blob/master/examples/pytorch/elastic/imagenet/\n",
    "    # https://github.com/ultralytics/yolov5/issues/475\n",
    "    # https://github.com/pytorch/pytorch/issues/65992#issuecomment-954392973\n",
    "    pod_template = V1PodTemplateSpec(\n",
    "        metadata=V1ObjectMeta(\n",
    "            name=job_name,\n",
    "            namespace=\"{{workflow.namespace}}\",\n",
    "            owner_references=[workflow_ownership],\n",
    "            annotations={\"sidecar.istio.io/inject\": \"false\"},\n",
    "        ),\n",
    "        spec=V1PodSpec(\n",
    "            containers=[\n",
    "                V1Container(\n",
    "                    name=constants.PYTORCHJOB_CONTAINER,\n",
    "                    image=yolov5_base_image,\n",
    "                    image_pull_policy=\"IfNotPresent\",\n",
    "                    working_dir=\"/yolov5\",\n",
    "                    command=[\n",
    "                        \"python\",\n",
    "                        \"-m\",\n",
    "                        \"torch.distributed.run\",\n",
    "                        # \"--nproc_per_node\",\n",
    "                        # f\"{gpus}\",\n",
    "                        # \"--nnodes\",\n",
    "                        # \"$(WORLD_SIZE)\",\n",
    "                        # \"--node_rank\",\n",
    "                        # \"$(RANK)\",\n",
    "                        # \"--rdzv_backend\",\n",
    "                        # \"etcd\",\n",
    "                        # \"--rdzv_endpoint\",\n",
    "                        # f\"etcd-server-ntl:2379\",\n",
    "                        \"train.py\",\n",
    "                        f\"--device={devices}\",\n",
    "                        \"--img=640\",\n",
    "                        f\"--batch-size={batch_size}\",\n",
    "                        \"--noplots\",\n",
    "                        f\"--epochs={epochs}\",\n",
    "                        \"--weights=/input/weights.pt\",\n",
    "                        \"--cache=/home/jovyan/cache\",\n",
    "                        \"--cfg=/input/config.yaml\",\n",
    "                        \"--data=/dataset/data.yaml\",\n",
    "                        \"--optimizer=Adam\",\n",
    "                    ],\n",
    "                    env=[\n",
    "                        V1EnvVar(name=\"LOGLEVEL\", value=\"DEBUG\"),\n",
    "                        V1EnvVar(name=\"NCCL_DEBUG\", value=\"DEBUG\"),\n",
    "                        V1EnvVar(name=\"TORCH_CPP_LOG_LEVEL\", value=\"INFO\"),\n",
    "                        # V1EnvVar(name=\"TORCH_DISTRIBUTED_DEBUG\", value=\"DETAIL\"),\n",
    "                        V1EnvVar(name=\"C10D_DEBUG_MODE\", value=\"DETAIL\"),\n",
    "                        V1EnvVar(name=\"PET_VERBOSE\", value=\"1\"),\n",
    "                        # V1EnvVar(\n",
    "                        #    name=\"RANK\",\n",
    "                        #    value_from=V1EnvVarSource(\n",
    "                        #        field_ref=V1ObjectFieldSelector(\n",
    "                        #            field_path=\"metadata.labels['replica-index']\"\n",
    "                        #        )\n",
    "                        #    ),\n",
    "                        # ),\n",
    "                        # V1EnvVar(name=\"WORLD_SIZE\", value=f\"{replicas}\"),\n",
    "                    ],\n",
    "                    # Allocate GPUs to each pod\n",
    "                    resources=V1ResourceRequirements(\n",
    "                        limits={\"nvidia.com/gpu\": f\"{gpus}\"}\n",
    "                    ),\n",
    "                    volume_mounts=[\n",
    "                        # Mount the input files directory from the training pvc\n",
    "                        V1VolumeMount(\n",
    "                            mount_path=\"/input\", name=\"training\", sub_path=\"input\"\n",
    "                        ),\n",
    "                        # This mounts the output directory of the training pvc\n",
    "                        # to a path where yolov5 stores the model and results.\n",
    "                        # And then we can copy those files back into kubeflow,\n",
    "                        # Because this component has that directory accessible\n",
    "                        # to it.\n",
    "                        V1VolumeMount(\n",
    "                            mount_path=\"/yolov5/runs/\",\n",
    "                            name=\"training\",\n",
    "                            sub_path=\"output\",\n",
    "                        ),\n",
    "                        # PyTorch requires shared memory on each pod\n",
    "                        V1VolumeMount(mount_path=\"/dev/shm\", name=\"dshm\"),\n",
    "                        # Mount the dataset pvc\n",
    "                        V1VolumeMount(\n",
    "                            mount_path=\"/dataset\",\n",
    "                            sub_path=dataset_subpath,\n",
    "                            name=\"dataset\",\n",
    "                        ),\n",
    "                    ],\n",
    "                )\n",
    "            ],\n",
    "            volumes=[\n",
    "                V1Volume(\n",
    "                    name=\"training\",\n",
    "                    persistent_volume_claim=V1PersistentVolumeClaimVolumeSource(\n",
    "                        claim_name=training_pvc_name\n",
    "                    ),\n",
    "                ),\n",
    "                V1Volume(\n",
    "                    name=\"dataset\",\n",
    "                    persistent_volume_claim=V1PersistentVolumeClaimVolumeSource(\n",
    "                        claim_name=dataset_pvc_name\n",
    "                    ),\n",
    "                ),\n",
    "                V1Volume(\n",
    "                    name=\"dshm\", empty_dir=V1EmptyDirVolumeSource(medium=\"Memory\")\n",
    "                ),\n",
    "            ],\n",
    "            service_account_name=\"default-editor\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # The PyTorchJob supports the concept of a master replica, but not\n",
    "    # specified the first worker takes that role. Having only workers\n",
    "    # simplifies the template a bit. This is how the imagenet example is\n",
    "    # setup. https://github.com/kubeflow/training-operator/blob/master/examples/pytorch/elastic/imagenet/\n",
    "    worker_spec = V1ReplicaSpec(\n",
    "        replicas=replicas, restart_policy=\"Never\", template=pod_template\n",
    "    )\n",
    "\n",
    "    # The owner references and replica spec are used to define the torch job\n",
    "    pytorchjob = KubeflowOrgV1PyTorchJob(\n",
    "        api_version=f\"{constants.KUBEFLOW_GROUP}/{constants.OPERATOR_VERSION}\",\n",
    "        kind=constants.PYTORCHJOB_KIND,\n",
    "        metadata=V1ObjectMeta(name=job_name, owner_references=[workflow_ownership]),\n",
    "        spec=KubeflowOrgV1PyTorchJobSpec(\n",
    "            # c10d is the most commonly used because it doesn't require additional\n",
    "            # packages. The primary advantage that elastic solutions offer is the\n",
    "            # ability to use cheap hardware in the cloud that can be taken away\n",
    "            # at any time. That doesn't apply so much for Power servers that are\n",
    "            # running on on-premise. Here we default to a fixed size of the\n",
    "            # number of replicias.\n",
    "            elastic_policy=KubeflowOrgV1ElasticPolicy(\n",
    "                rdzv_backend=\"c10d\",\n",
    "                # rdzv_host=\"etcd-client.kubeflow-ntl.svc.cluster.local\",\n",
    "                # rdzv_port=2379,\n",
    "                rdzv_id=job_name,\n",
    "                n_proc_per_node=gpus,\n",
    "                min_replicas=replicas,\n",
    "                max_replicas=replicas,\n",
    "                max_restarts=0,\n",
    "            ),\n",
    "            run_policy=V1RunPolicy(clean_pod_policy=\"None\"),\n",
    "            pytorch_replica_specs={\"Worker\": worker_spec},\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    ##########################\n",
    "    # Submit training job\n",
    "    ##########################\n",
    "    training_client = TrainingClient()\n",
    "    training_client.create_pytorchjob(pytorchjob)\n",
    "\n",
    "    try:\n",
    "        training_client.wait_for_job_conditions(\n",
    "            job_name,\n",
    "            expected_conditions={\n",
    "                constants.JOB_CONDITION_RUNNING,\n",
    "                constants.JOB_CONDITION_SUCCEEDED,\n",
    "                constants.JOB_CONDITION_FAILED,\n",
    "            },\n",
    "            job_kind=constants.PYTORCHJOB_KIND,\n",
    "            timeout=120,\n",
    "        )\n",
    "    except RuntimeError as e:\n",
    "        # https://github.com/kubeflow/training-operator/issues/1806#issue-1708084586\n",
    "        pass\n",
    "\n",
    "    # Wait for pods to be ready, must do this before reading logs\n",
    "    pod_names = training_client.get_job_pod_names(name=job_name, is_master=None)\n",
    "    for pod in pod_names:\n",
    "        wait_for_pod_ready(pod)\n",
    "\n",
    "    # stream logs for all workers\n",
    "    # (most of the interesting stuff is in worker 0)\n",
    "    training_client.get_job_logs(\n",
    "        name=job_name,\n",
    "        is_master=False,\n",
    "        container=constants.PYTORCHJOB_CONTAINER,\n",
    "        follow=True,\n",
    "    )\n",
    "\n",
    "    # No more logs means workers have finished, wait for the rest of the job\n",
    "    try:\n",
    "        training_client.wait_for_job_conditions(\n",
    "            job_name,\n",
    "            expected_conditions={\n",
    "                constants.JOB_CONDITION_SUCCEEDED,\n",
    "                constants.JOB_CONDITION_FAILED,\n",
    "            },\n",
    "            timeout=timeout,\n",
    "            job_kind=constants.PYTORCHJOB_KIND,\n",
    "        )\n",
    "    except RuntimeError as e:\n",
    "        # https://github.com/kubeflow/training-operator/issues/1806#issue-1708084586\n",
    "        pass\n",
    "\n",
    "    if training_client.is_job_failed(name=job_name, job_kind=constants.PYTORCHJOB_KIND):\n",
    "        raise RuntimeError(f\"Job {job_name} Failed!\")\n",
    "\n",
    "    ########################################################\n",
    "    # Copy trained model and results to output parameters\n",
    "    ########################################################\n",
    "    os.makedirs(os.path.dirname(model), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(results), exist_ok=True)\n",
    "    shutil.copyfile(f\"/{training_pvc_mount}/output/train/exp/weights/best.pt\", model)\n",
    "    shutil.copyfile(f\"/{training_pvc_mount}/output/train/exp/results.csv\", results)\n",
    "\n",
    "\n",
    "train_model_comp = kfp.components.create_component_from_func(\n",
    "    train_model_distributed,\n",
    "    base_image=BASE_IMAGE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
