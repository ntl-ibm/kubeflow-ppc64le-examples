{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "841ff471-215d-4984-a0f5-b88fd13a5b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4038e527-8023-445d-849d-123cae61e46c",
   "metadata": {},
   "source": [
    "# Distributed training with Pytorch example\n",
    "\n",
    "This notebook trains and evaluates a classifier of handwritten digits (Using the MNIST dataset). The training is distributed across multiple GPUs.\n",
    "\n",
    "There are important features and differences in this flow than the MPI/Tensorflow example, and other training examples.\n",
    "\n",
    "## Container Image\n",
    "The components of this pipeline use a custom built container image as their base image. The Dockerfile is included in this directory. The container image does *not* include a notebook server, meaning you cannot use it to launch a notebook. The notebook container images from https://quay.io/repository/ibm/kubeflow-notebook-image-ppc64le have a wide range of packages installed in them, and these can be used to start an interactive notebook server. The base image used in this example has installed a much smaller set of newer packages, including Python 3.10 and PyTorch 1.13 from RocketCE. Also included is pytorch-lightning, which makes it easier to code up models for distributed training.\n",
    "\n",
    "Since your notebook image and pipeline images are not the same you may be able to run code in the pipeline that does not run interactivly in the notebook. And you may be able to run code interactivly that does not run in the pipeline.\n",
    "\n",
    "You can see how the custom image was built and that packages that are included by looking at the Dockerfile.\n",
    "\n",
    "## Model Training Script\n",
    "When using distributed training with GPUs, most PyTorch models are trained within a script that is called from the command line, as opposed to a cell in a interactive notebook.\n",
    "\n",
    "The script to run the model is stored in the GitHub Repo.\n",
    "\n",
    "The Kubeflow component will create a PyTorch job that runs the script across a pre-defined number of worker pods. The component will then monitor the job and wait for the job's completion.\n",
    "\n",
    "## Shared Storage\n",
    "The Kubeflow component needs to make the Python script and training data available to the PytorchJob, and it needs to be able to obtain the trained model after training is completed.\n",
    "\n",
    "The pipeline will create a Volume as the first step in the pipeline.\n",
    "The pipeline will copy the training data and training script onto this storage so that the workers can access it.\n",
    "After training, the pipeline will copy the model from the shared storage to storage that is accessible outside the pipeline. (The notebook's volume in this example).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3deb0460-f169-4bed-8d16-cba226a651bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp.components import InputPath, OutputPath\n",
    "import kfp.dsl as dsl\n",
    "from kfp.dsl import PipelineConf, data_passing_methods\n",
    "from kubernetes.client.models import V1Volume, V1PersistentVolumeClaimVolumeSource\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import List, NamedTuple\n",
    "\n",
    "BASE_IMAGE = \"quay.io/ntlawrence/pytorchv1.13:1.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb5ba09-6f1b-4f3a-96f3-69c2448fb995",
   "metadata": {},
   "source": [
    "# Prepare shared storage\n",
    "\n",
    "This cell downloads 3 things to the shared storage\n",
    "* Training data\n",
    "* Test data\n",
    "* Model training script\n",
    "\n",
    "The assumption in the function is that the shared volume has been mounted to the path \"/workspace\". This mount is done in the pipeline definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b552cf95-1480-4b5e-a3d7-88ce6153b8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_shared_storage():\n",
    "    from torchvision.datasets import MNIST\n",
    "    import urllib\n",
    "\n",
    "    # Download training data\n",
    "    _ = MNIST(\"/workspace\", download=True, train=True)\n",
    "    _ = MNIST(\"/workspace\", download=True, train=False)\n",
    "\n",
    "    # Download python model training script\n",
    "    r = urllib.request.urlretrieve(\n",
    "        \"https://raw.githubusercontent.com/ntl-ibm/kubeflow-ppc64le-examples/multi-gpu-yolov5/pytorch_distributed/mnist/mnist.py\",\n",
    "        \"/workspace/mnist.py\",\n",
    "    )\n",
    "\n",
    "\n",
    "prepare_shared_storage_comp = kfp.components.create_component_from_func(\n",
    "    prepare_shared_storage, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d8c99d-d8ed-400d-ae8c-4587772c9307",
   "metadata": {},
   "source": [
    "# Train and Test the model\n",
    "For this example, both training and evaluating the model is part of the same script.\n",
    "\n",
    "The training data and training script has been downloaded to shared storage (assumed to be mounted to /workspace).\n",
    "\n",
    "The pipeline component needs to create a PyTorchJob resource and monitor the resource. Only the creation of the resource and monitoring happens in this component, the actual training happens in independent worker pods.\n",
    "\n",
    "Creating and monitoring the resource is complex. The component installers a deploy script (actually from this repo) to do that in a single function call.\n",
    "\n",
    "Both this script and the training script could have been included in the container image, which avoid the need to download. However downloading has the advantage of being able to pickup versioned changes from Git, without rebuilding the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87c120ff-ef8a-4b09-b823-ae743bb867c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_model(\n",
    "    shared_pvc_name: str, mlpipeline_ui_metadata_path: OutputPath(str)\n",
    "):\n",
    "    # Install a python package to make deploying and monitoring the pytorch job easier\n",
    "    import subprocess\n",
    "\n",
    "    subprocess.run(\n",
    "        \"pip install 'pytorch_distributed_kf_tools @ \"\n",
    "        \"git+https://github.com/ntl-ibm/kubeflow-ppc64le-examples@multi-gpu-yolov5#\"\n",
    "        \"subdirectory=pytorch_distributed/pytorch_distributed_kf_tools'\",\n",
    "        shell=True,\n",
    "    )\n",
    "\n",
    "    import pytorch_distributed_kf_tools.deploy as deploy\n",
    "    import shutil\n",
    "\n",
    "    ## Start the PyTorch job for distributed training\n",
    "    deploy.run_pytorch_job(\n",
    "        # owning_workflow setups it up so that when the pipeline is deleted, so is the\n",
    "        # training job\n",
    "        owning_workflow=deploy.OwningWorkFlow(\n",
    "            name=\"{{workflow.name}}\", uid=\"{{workflow.uid}}\"\n",
    "        ),\n",
    "        # These place holders for namespace and job name are\n",
    "        # filled in by Kubeflow when the pipeline runs.\n",
    "        namespace=\"{{workflow.namespace}}\",\n",
    "        pytorch_job_name=\"{{workflow.name}}\",\n",
    "        # Shared volumes used by the training script\n",
    "        pvcs=[\n",
    "            deploy.PvcMount(\n",
    "                pvc_name=(shared_pvc_name),\n",
    "                mount_path=\"/workspace\",\n",
    "            )\n",
    "        ],\n",
    "        # The command to run in each worker\n",
    "        # This almost always starts with \"torch.distributed.run\" for DDP\n",
    "        command=[\n",
    "            \"python\",\n",
    "            \"-m\",\n",
    "            \"torch.distributed.run\",\n",
    "            \"/workspace/mnist.py\",\n",
    "            \"--root_dir=/workspace\",\n",
    "            \"--data_dir=/workspace\",\n",
    "            \"--model=/workspace/mnist_model.pt\",\n",
    "            f\"--kubeflow_ui_metadata=/workspace/metadata.json\",\n",
    "            \"--max_epochs=10\",\n",
    "        ],\n",
    "        # Number of workers\n",
    "        num_workers=6,\n",
    "        # Number of GPUs per worker (OK to leave this at 1)\n",
    "        gpus_per_worker=1,\n",
    "        # The base image used for the worker pods\n",
    "        worker_image=\"quay.io/ntlawrence/pytorchv1.13:1.1\",\n",
    "    )\n",
    "\n",
    "    # The example python script outputs a file metadata.json\n",
    "    # This file is already in a format suitable to display metrics on the\n",
    "    # visualizations tab of the component. We just have to copy it to\n",
    "    # the output path provided by Kubeflow for it to show up.\n",
    "    shutil.copyfile(\"/workspace/metadata.json\", mlpipeline_ui_metadata_path)\n",
    "\n",
    "\n",
    "train_and_test_model_comp = kfp.components.create_component_from_func(\n",
    "    train_and_test_model, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3dc156a-34eb-4186-81e5-b42e14d89791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_data(source: str, dest: str):\n",
    "    import os\n",
    "    import shutil\n",
    "\n",
    "    # Make target directories if needed\n",
    "    parent_dirs = os.path.basename(dest)\n",
    "    if not os.path.exists(parent_dirs):\n",
    "        os.makedirs(parent_dirs)\n",
    "\n",
    "    if os.path.isdir(source):\n",
    "        shutil.copytree(source, dest)\n",
    "    else:\n",
    "        shutil.copyfile(source, dest)\n",
    "\n",
    "\n",
    "copy_data_comp = kfp.components.create_component_from_func(\n",
    "    copy_data, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31a8707d-77db-47b6-b029-1a3ba605edf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubernetes.client import V1VolumeMount\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"Handwritten digit classification\",\n",
    "    description=\"An example pipeline that trains using distributed pytorch\",\n",
    ")\n",
    "def mnist_pipeline(notebook_pvc_name: str = \"pytorch-minst-volume\"):\n",
    "\n",
    "    create_shared_volume_volop = dsl.VolumeOp(\n",
    "        name=\"Create shared volume for training\",\n",
    "        resource_name=\"shared-pvc\",\n",
    "        modes=dsl.VOLUME_MODE_RWM,\n",
    "        size=\"4Gi\",\n",
    "        set_owner_reference=True,\n",
    "    )\n",
    "\n",
    "    prepare_shared_storage_task = prepare_shared_storage_comp()\n",
    "    prepare_shared_storage_task.add_pvolumes(\n",
    "        {\"/workspace\": create_shared_volume_volop.volume}\n",
    "    )\n",
    "\n",
    "    train_model_task = train_and_test_model_comp(\n",
    "        create_shared_volume_volop.volume.persistent_volume_claim.claim_name\n",
    "    )\n",
    "    train_model_task.add_pvolumes({\"/workspace\": create_shared_volume_volop.volume})\n",
    "    train_model_task.after(prepare_shared_storage_task)\n",
    "    train_model_task.set_display_name(\"Train and Test Model\")\n",
    "\n",
    "    copy_model_task = copy_data_comp(\n",
    "        \"/workspace/mnist_model.pt\", \"/target/mnist_model.pt\"\n",
    "    )\n",
    "    copy_model_task.add_pvolumes({\"/workspace\": create_shared_volume_volop.volume})\n",
    "    copy_model_task.add_volume(\n",
    "        V1Volume(\n",
    "            name=notebook_pvc_name,\n",
    "            persistent_volume_claim=V1PersistentVolumeClaimVolumeSource(\n",
    "                notebook_pvc_name\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    copy_model_task.add_volume_mount(\n",
    "        V1VolumeMount(name=notebook_pvc_name, mount_path=\"/target\")\n",
    "    )\n",
    "    copy_model_task.set_display_name(f\"Copy Model to target PVC\")\n",
    "    copy_model_task.after(train_model_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb9970a3-f083-49b1-a7ab-5d999215adf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_NAME = \"MNIST Classification Pipeline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a36cd67f-8f7d-4186-85f8-a09935fcd297",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_conf = kfp.dsl.PipelineConf()\n",
    "\n",
    "# Disable Caching\n",
    "def disable_cache_transformer(op: dsl.ContainerOp):\n",
    "    if isinstance(op, dsl.ContainerOp):\n",
    "        op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    else:\n",
    "        op.add_pod_annotation(\n",
    "            name=\"pipelines.kubeflow.org/max_cache_staleness\", value=\"P0D\"\n",
    "        )\n",
    "    return op\n",
    "\n",
    "\n",
    "pipeline_conf.add_op_transformer(disable_cache_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c00906ed-c38f-4159-bac8-ee8dd1b50689",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=mnist_pipeline,\n",
    "    package_path=f\"{PIPELINE_NAME}.yaml\",\n",
    "    pipeline_conf=pipeline_conf,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85b89812-f7c1-412a-a8ef-a499ce5dcef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pipeline(pipeline_name: str):\n",
    "    \"\"\"Delete's a pipeline with the specified name\"\"\"\n",
    "\n",
    "    client = kfp.Client()\n",
    "    existing_pipelines = client.list_pipelines(page_size=999).pipelines\n",
    "    matches = (\n",
    "        [ep.id for ep in existing_pipelines if ep.name == pipeline_name]\n",
    "        if existing_pipelines\n",
    "        else []\n",
    "    )\n",
    "    for id in matches:\n",
    "        client.delete_pipeline(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac39fb69-5612-4adf-8dfc-9a6bb4ee7692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_id(experiment_name: str) -> str:\n",
    "    \"\"\"Returns the id for the experiment, creating the experiment if needed\"\"\"\n",
    "    client = kfp.Client()\n",
    "    existing_experiments = client.list_experiments(page_size=999).experiments\n",
    "    matches = (\n",
    "        [ex.id for ex in existing_experiments if ex.name == experiment_name]\n",
    "        if existing_experiments\n",
    "        else []\n",
    "    )\n",
    "\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "\n",
    "    exp = client.create_experiment(experiment_name)\n",
    "    return exp.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b463702d-0cd1-4147-bbc6-39f67abc3b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=/pipeline/#/pipelines/details/20f26031-19b2-4eea-9cb2-bd97d199bdd1>Pipeline details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pipeline names need to be unique, so before we upload,\n",
    "# check for and delete any pipeline with the same name\n",
    "delete_pipeline(PIPELINE_NAME)\n",
    "\n",
    "# upload\n",
    "client = kfp.Client()\n",
    "uploaded_pipeline = client.upload_pipeline(f\"{PIPELINE_NAME}.yaml\", PIPELINE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52ab3b0b-ed41-4092-ae5b-c5d970518fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/45eaaea0-54b2-4013-a1ab-ffbd18039f52\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = client.run_pipeline(\n",
    "    experiment_id=get_experiment_id(\"mnist\"),\n",
    "    job_name=\"mnist-classification-pipeline\",\n",
    "    pipeline_id=uploaded_pipeline.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c97734f-ba5a-46c4-b275-5e752db4e81b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
