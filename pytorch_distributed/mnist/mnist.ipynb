{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "841ff471-215d-4984-a0f5-b88fd13a5b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3deb0460-f169-4bed-8d16-cba226a651bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp.components import InputPath, OutputPath\n",
    "import kfp.dsl as dsl\n",
    "from kfp.dsl import PipelineConf, data_passing_methods\n",
    "from kubernetes.client.models import V1Volume, V1PersistentVolumeClaimVolumeSource\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import List, NamedTuple\n",
    "\n",
    "BASE_IMAGE = (\n",
    "    \"quay.io/ibm/kubeflow-notebook-image-ppc64le:elyra3.14.1-py3.9-tf2.9.2-pt1.12.1-v0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60e961e2-54af-48b6-8bae-4f49f27678f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_shared_storage():\n",
    "    from torchvision.datasets import MNIST\n",
    "    import urllib\n",
    "\n",
    "    # Download training data\n",
    "    _ = MNIST(\"/workspace\", download=True, train=True)\n",
    "    _ = MNIST(\"/workspace\", download=True, train=False)\n",
    "\n",
    "    # Download python script\n",
    "    r = urllib.request.urlretrieve(\n",
    "        \"https://raw.githubusercontent.com/ntl-ibm/kubeflow-ppc64le-examples/multi-gpu-yolov5/pytorch_distributed/mnist/mnist.py\",\n",
    "        \"/workspace/mnist.py\",\n",
    "    )\n",
    "\n",
    "\n",
    "prepare_shared_storage_comp = kfp.components.create_component_from_func(\n",
    "    prepare_shared_storage, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87c120ff-ef8a-4b09-b823-ae743bb867c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_model(\n",
    "    shared_pvc_name: str, mlpipeline_ui_metadata_path: OutputPath(str)\n",
    "):\n",
    "    import subprocess\n",
    "\n",
    "    subprocess.run(\n",
    "        \"pip install 'pytorch_distributed_kf_tools @ \"\n",
    "        \"git+https://github.com/ntl-ibm/kubeflow-ppc64le-examples@multi-gpu-yolov5#\"\n",
    "        \"subdirectory=pytorch_distributed/pytorch_distributed_kf_tools'\",\n",
    "        shell=True,\n",
    "    )\n",
    "\n",
    "    from pytorch_distributed_kf_tools.deploy import (\n",
    "        run_pytorch_job,\n",
    "        PvcMount,\n",
    "        OwningWorkFlow,\n",
    "    )\n",
    "    import shutil\n",
    "\n",
    "    run_pytorch_job(\n",
    "        owning_workflow=OwningWorkFlow(\n",
    "            name=\"{{workflow.name}}\", uid=\"{{workflow.uid}}\"\n",
    "        ),\n",
    "        namespace=\"{{workflow.namespace}}\",\n",
    "        pytorch_job_name=\"{{workflow.name}}\",\n",
    "        pvcs=[\n",
    "            PvcMount(\n",
    "                pvc_name=(shared_pvc_name),\n",
    "                mount_path=\"/workspace\",\n",
    "            )\n",
    "        ],\n",
    "        command=[\n",
    "            \"python\",\n",
    "            \"-m\",\n",
    "            \"torch.distributed.run\",\n",
    "            \"/workspace/mnist.py\",\n",
    "            \"--root_dir=/workspace\",\n",
    "            \"--data_dir=/workspace\",\n",
    "            \"--model=/workspace/mnist_model.pt\",\n",
    "            f\"--kubeflow_ui_metadata=/workspace/metadata.json\",\n",
    "            \"--max_epochs=10\",\n",
    "        ],\n",
    "        num_workers=6,\n",
    "        gpus_per_worker=1,\n",
    "        worker_image=\"quay.io/ntlawrence/pytorchv1.13:1.1\",\n",
    "    )\n",
    "\n",
    "    shutil.copyfile(\"/workspace/metadata.json\", mlpipeline_ui_metadata_path)\n",
    "\n",
    "\n",
    "train_and_test_model_comp = kfp.components.create_component_from_func(\n",
    "    train_and_test_model, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3dc156a-34eb-4186-81e5-b42e14d89791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_data(source: str, dest: str):\n",
    "    import os\n",
    "    import shutil\n",
    "\n",
    "    # Make target directories if needed\n",
    "    parent_dirs = os.path.basename(dest)\n",
    "    if not os.path.exists(parent_dirs):\n",
    "        os.makedirs(parent_dirs)\n",
    "\n",
    "    if os.path.isdir(source):\n",
    "        shutil.copytree(source, dest)\n",
    "    else:\n",
    "        shutil.copyfile(source, dest)\n",
    "\n",
    "\n",
    "copy_data_comp = kfp.components.create_component_from_func(\n",
    "    copy_data, base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31a8707d-77db-47b6-b029-1a3ba605edf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubernetes.client import V1VolumeMount\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"Handwritten digit classification\",\n",
    "    description=\"An example pipeline that trains using distributed pytorch\",\n",
    ")\n",
    "def mnist_pipeline(notebook_pvc_name: str = \"pytorch-minst-volume\"):\n",
    "\n",
    "    create_shared_volume_volop = dsl.VolumeOp(\n",
    "        name=\"Create shared volume for training\",\n",
    "        resource_name=\"shared-pvc\",\n",
    "        modes=dsl.VOLUME_MODE_RWM,\n",
    "        size=\"4Gi\",\n",
    "        set_owner_reference=True,\n",
    "    )\n",
    "\n",
    "    prepare_shared_storage_task = prepare_shared_storage_comp()\n",
    "    prepare_shared_storage_task.add_pvolumes(\n",
    "        {\"/workspace\": create_shared_volume_volop.volume}\n",
    "    )\n",
    "\n",
    "    train_model_task = train_and_test_model_comp(\n",
    "        create_shared_volume_volop.volume.persistent_volume_claim.claim_name\n",
    "    )\n",
    "    train_model_task.add_pvolumes({\"/workspace\": create_shared_volume_volop.volume})\n",
    "    train_model_task.after(prepare_shared_storage_task)\n",
    "    train_model_task.set_display_name(\"Train and Test Model\")\n",
    "\n",
    "    copy_model_task = copy_data_comp(\n",
    "        \"/workspace/mnist_model.pt\", \"/target/mnist_model.pt\"\n",
    "    )\n",
    "    copy_model_task.add_pvolumes({\"/workspace\": create_shared_volume_volop.volume})\n",
    "    copy_model_task.add_volume(\n",
    "        V1Volume(\n",
    "            name=notebook_pvc_name,\n",
    "            persistent_volume_claim=V1PersistentVolumeClaimVolumeSource(\n",
    "                notebook_pvc_name\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    copy_model_task.add_volume_mount(\n",
    "        V1VolumeMount(name=notebook_pvc_name, mount_path=\"/target\")\n",
    "    )\n",
    "    copy_model_task.set_display_name(f\"Copy Model to target PVC\")\n",
    "    copy_model_task.after(train_model_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb9970a3-f083-49b1-a7ab-5d999215adf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_NAME = \"MNIST Classification Pipeline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a36cd67f-8f7d-4186-85f8-a09935fcd297",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_conf = kfp.dsl.PipelineConf()\n",
    "\n",
    "# Disable Caching\n",
    "def disable_cache_transformer(op: dsl.ContainerOp):\n",
    "    if isinstance(op, dsl.ContainerOp):\n",
    "        op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    else:\n",
    "        op.add_pod_annotation(\n",
    "            name=\"pipelines.kubeflow.org/max_cache_staleness\", value=\"P0D\"\n",
    "        )\n",
    "    return op\n",
    "\n",
    "\n",
    "pipeline_conf.add_op_transformer(disable_cache_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c00906ed-c38f-4159-bac8-ee8dd1b50689",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=mnist_pipeline,\n",
    "    package_path=f\"{PIPELINE_NAME}.yaml\",\n",
    "    pipeline_conf=pipeline_conf,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85b89812-f7c1-412a-a8ef-a499ce5dcef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pipeline(pipeline_name: str):\n",
    "    \"\"\"Delete's a pipeline with the specified name\"\"\"\n",
    "\n",
    "    client = kfp.Client()\n",
    "    existing_pipelines = client.list_pipelines(page_size=999).pipelines\n",
    "    matches = (\n",
    "        [ep.id for ep in existing_pipelines if ep.name == pipeline_name]\n",
    "        if existing_pipelines\n",
    "        else []\n",
    "    )\n",
    "    for id in matches:\n",
    "        client.delete_pipeline(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac39fb69-5612-4adf-8dfc-9a6bb4ee7692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_id(experiment_name: str) -> str:\n",
    "    \"\"\"Returns the id for the experiment, creating the experiment if needed\"\"\"\n",
    "    client = kfp.Client()\n",
    "    existing_experiments = client.list_experiments(page_size=999).experiments\n",
    "    matches = (\n",
    "        [ex.id for ex in existing_experiments if ex.name == experiment_name]\n",
    "        if existing_experiments\n",
    "        else []\n",
    "    )\n",
    "\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "\n",
    "    exp = client.create_experiment(experiment_name)\n",
    "    return exp.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b463702d-0cd1-4147-bbc6-39f67abc3b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=/pipeline/#/pipelines/details/a1373493-c5da-4074-a647-cef5e94ddb20>Pipeline details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pipeline names need to be unique, so before we upload,\n",
    "# check for and delete any pipeline with the same name\n",
    "delete_pipeline(PIPELINE_NAME)\n",
    "\n",
    "# upload\n",
    "client = kfp.Client()\n",
    "uploaded_pipeline = client.upload_pipeline(f\"{PIPELINE_NAME}.yaml\", PIPELINE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52ab3b0b-ed41-4092-ae5b-c5d970518fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/7766643f-dfc1-4e5e-a777-0374cd619242\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = client.run_pipeline(\n",
    "    experiment_id=get_experiment_id(\"mnist\"),\n",
    "    job_name=\"mnist-classification-pipeline\",\n",
    "    pipeline_id=uploaded_pipeline.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f522ddb7-3cfa-43f6-9615-8090401f0b39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
